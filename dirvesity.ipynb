{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61be201-650d-48ab-9986-b98b2c8aa4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training: 4, Validation: 9113, Test: 20872\n",
      "Starting improved training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 â€” Loss: 0.9211 â€” Train: 0.600 â€” Val: 0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 1/3000 [00:15<12:47:11, 15.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–ˆâ–Ž                                                                              | 50/3000 [01:10<57:15,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 â€” Loss: 0.0115 â€” Train: 1.000 â€” Val: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–ˆâ–Ž                                                                            | 51/3000 [01:27<4:50:59,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–‹                                                                            | 100/3000 [02:26<53:37,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 â€” Loss: 0.0106 â€” Train: 1.000 â€” Val: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–Œ                                                                          | 101/3000 [02:40<4:12:21,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–ˆâ–ˆâ–ˆâ–‰                                                                           | 150/3000 [03:36<55:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150 â€” Loss: 0.0102 â€” Train: 1.000 â€” Val: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–ˆâ–ˆâ–ˆâ–‰                                                                         | 151/3000 [03:54<4:55:24,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                       | 201/3000 [05:07<4:32:32,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 â€” Loss: 0.0101 â€” Train: 1.000 â€” Val: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                      | 251/3000 [06:14<4:18:13,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250 â€” Loss: 0.0101 â€” Train: 1.000 â€” Val: 0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                     | 301/3000 [07:23<4:08:29,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300 â€” Loss: 0.0102 â€” Train: 1.000 â€” Val: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                    | 351/3000 [08:20<2:52:14,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 350 â€” Loss: 0.0101 â€” Train: 1.000 â€” Val: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                  | 401/3000 [09:31<4:07:31,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                   | 450/3000 [10:27<49:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 450 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 451/3000 [10:44<4:06:18,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                | 501/3000 [11:54<4:00:46,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 551/3000 [13:03<3:38:43,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 550 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                             | 601/3000 [14:13<3:19:25,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 651/3000 [15:24<3:54:32,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 650 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                           | 701/3000 [16:38<3:52:08,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                         | 751/3000 [17:46<3:35:47,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 750 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                          | 800/3000 [18:38<41:32,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                        | 801/3000 [18:56<3:46:24,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                       | 851/3000 [20:06<3:35:55,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 850 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 901/3000 [21:18<3:21:10,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                    | 951/3000 [22:33<3:25:43,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 950 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                  | 1001/3000 [23:41<3:05:17,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                 | 1051/3000 [24:49<3:09:03,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                | 1101/3000 [25:58<2:55:09,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 1151/3000 [27:10<2:53:04,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 1201/3000 [28:22<2:50:31,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                            | 1251/3000 [29:32<2:38:31,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                           | 1301/3000 [30:38<2:41:00,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 1351/3000 [31:47<2:37:09,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 1401/3000 [32:58<2:27:16,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                       | 1451/3000 [34:10<2:26:53,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1501/3000 [35:20<2:17:50,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 1551/3000 [36:27<2:15:08,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1601/3000 [37:36<2:16:37,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 1651/3000 [38:47<2:00:36,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1700/3000 [39:43<25:49,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 1701/3000 [39:59<2:05:38,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 1751/3000 [41:11<1:51:08,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 1801/3000 [42:19<1:49:38,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 1851/3000 [43:28<1:48:20,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 1901/3000 [44:40<1:42:48,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 1951/3000 [45:50<1:34:20,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 2001/3000 [47:04<1:29:42,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 2051/3000 [48:10<1:24:57,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 2101/3000 [49:19<1:21:17,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2151/3000 [50:31<1:19:41,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 2201/3000 [51:41<1:14:17,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 2251/3000 [52:52<1:05:51,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 2301/3000 [53:58<1:03:40,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2351/3000 [55:05<59:09,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 2401/3000 [56:16<55:36,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 2451/3000 [57:25<50:26,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 2501/3000 [58:38<46:37,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 2551/3000 [59:44<39:06,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 2601/3000 [1:00:49<34:03,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 2651/3000 [1:02:00<32:28,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 2701/3000 [1:03:10<26:13,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 2751/3000 [1:04:22<22:33,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2801/3000 [1:05:29<16:56,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2851/3000 [1:06:35<14:12,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2901/3000 [1:07:45<09:30,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2951/3000 [1:08:57<04:35,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 â€” Loss: 0.0100 â€” Train: 1.000 â€” Val: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [1:09:52<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Best validation accuracy: 0.678\n",
      "Best model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "print(f\"Training: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Enhanced transforms with stronger augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode(data, n_way, k_shot, q_query):\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        samples = class_samples[cls]\n",
    "        if len(samples) < k_shot + q_query:\n",
    "            # Oversample if needed\n",
    "            selected = random.choices(samples, k=k_shot + q_query)\n",
    "        else:\n",
    "            selected = random.sample(samples, k_shot + q_query)\n",
    "        \n",
    "        # Support samples\n",
    "        for i in range(k_shot):\n",
    "            episode.append({\n",
    "                'path': selected[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "        \n",
    "        # Query samples  \n",
    "        for i in range(k_shot, k_shot + q_query):\n",
    "            episode.append({\n",
    "                'path': selected[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last few layers instead of freezing everything\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with dropout and batch norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Temperature parameter for distance scaling\n",
    "        self.temperature = nn.Parameter(torch.tensor(10.0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return F.normalize(self.projection(features), dim=1)  # L2 normalization\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# Model and training setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, total_steps=3000, pct_start=0.3\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "n_way, k_shot, q_query = 4, 1, 15\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 300\n",
    "\n",
    "print(\"Starting improved training...\")\n",
    "\n",
    "for episode in tqdm(range(3000)):\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode(train_data, n_way, k_shot, q_query)\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    # Forward pass\n",
    "    support_embeddings = model(support_images)\n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Compute prototypes with better aggregation\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    # Cosine similarity with temperature scaling\n",
    "    logits = torch.mm(query_embeddings, prototypes.t()) * model.temperature\n",
    "    loss = F.cross_entropy(logits, query_labels)\n",
    "    \n",
    "    # Add prototype regularization\n",
    "    proto_reg = 0.01 * torch.mean(torch.norm(prototypes, dim=1))\n",
    "    loss += proto_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            for _ in range(20):  # More validation episodes\n",
    "                val_episode_data, _ = create_episode(val_data, n_way, k_shot, q_query)\n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_logits = torch.mm(val_query_embeddings, val_prototypes.t()) * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            \n",
    "            print(f\"Episode {episode} â€” Loss: {loss.item():.4f} â€” Train: {train_acc:.3f} â€” Val: {val_acc_avg:.3f}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "model.load_state_dict(torch.load('improved_protonet_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8255a8-93e3-449a-85e8-c99ed3a9cf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ COMPREHENSIVE PROTONET EVALUATION - ALL TEST IMAGES\n",
      "================================================================================\n",
      "ðŸŽ¯ Running Diverse Ensemble (n=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [17:51<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ensemble 1: 0.6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [17:03<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ensemble 2: 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [17:08<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ensemble 3: 0.6599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [17:09<00:00, 20.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ensemble 4: 0.6597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [17:09<00:00, 20.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ensemble 5: 0.6602\n",
      "\n",
      "ðŸ“Š Enhanced Ensemble Results:\n",
      "  Final Accuracy: 0.6610\n",
      "  Precision: 0.7325\n",
      "  Recall: 0.6610\n",
      "  F1-Score: 0.6237\n",
      "  Std Dev: 0.0004\n",
      "ðŸŽ¯ Comprehensive Prediction Analysis for All Test Images...\n",
      "Processing 20872 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing all test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [06:48<00:00, 51.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Overall Accuracy: 0.6489\n",
      "\n",
      "ðŸ† Extracting Top 1000 Most Dissimilar & High Confidence Predictions Per Class:\n",
      "------------------------------------------------------------------------------------------\n",
      "Amphibolis     : 1000 predictions | Avg Dissim: 0.4006 | Avg Conf: 0.9280 | Accuracy: 0.3570\n",
      "Background     : 1000 predictions | Avg Dissim: 0.2551 | Avg Conf: 0.9471 | Accuracy: 0.6200\n",
      "Halophila      : 1000 predictions | Avg Dissim: 0.3311 | Avg Conf: 0.9497 | Accuracy: 0.5960\n",
      "Posidonia      : 1000 predictions | Avg Dissim: 0.2987 | Avg Conf: 0.9146 | Accuracy: 0.9170\n",
      "\n",
      "ðŸ“Š Confidence-based Analysis (5 bins):\n",
      "Bin   Range                Count    Accuracy   Avg Margin   Avg Dissim  \n",
      "---------------------------------------------------------------------------\n",
      "1     1.000-1.000          4174     0.676      1.0000       0.0103      \n",
      "2     1.000-1.000          4174     0.589      1.0000       0.0206      \n",
      "3     1.000-1.000          4174     0.796      1.0000       0.0464      \n",
      "4     1.000-1.000          4174     0.677      0.9998       0.1101      \n",
      "5     0.291-1.000          4176     0.507      0.8566       0.3381      \n",
      "âœ… Confusion matrix saved to confusion_matrix.png\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class        Precision  Recall     F1         Support   \n",
      "-------------------------------------------------------\n",
      "Amphibolis   0.603      0.929      0.731      5375      \n",
      "Background   0.787      0.339      0.474      4706      \n",
      "Halophila    0.608      0.963      0.745      5669      \n",
      "Posidonia    0.947      0.292      0.446      5122      \n",
      "\n",
      "ðŸ† Top 5 Most Dissimilar & High Confidence Predictions Per Class:\n",
      "====================================================================================================\n",
      "\n",
      "Amphibolis (Top 5 out of 1000 total):\n",
      "------------------------------------------------------------------------------------------\n",
      " 1. Halophila_CMER_test_123_patch_20.jp | Halophila    â†’ Amphibolis  \n",
      "     Conf: 0.8797 | Dissim: 0.7331 | Score: 0.7918 âŒ\n",
      "     Closest Support: Amphibolis_DBCA_training_136_patch_4.jpg\n",
      " 2. Halophila_CMER_test_6_patch_21.jpg  | Halophila    â†’ Amphibolis  \n",
      "     Conf: 0.8870 | Dissim: 0.7250 | Score: 0.7898 âŒ\n",
      "     Closest Support: Amphibolis_DBCA_training_136_patch_4.jpg\n",
      " 3. Halophila_CMER_test_30_patch_18.jpg | Halophila    â†’ Amphibolis  \n",
      "     Conf: 0.9382 | Dissim: 0.6811 | Score: 0.7839 âŒ\n",
      "     Closest Support: Amphibolis_DBCA_training_136_patch_4.jpg\n",
      " 4. Halophila_CMER_test_260_patch_19.jp | Halophila    â†’ Amphibolis  \n",
      "     Conf: 0.9214 | Dissim: 0.6910 | Score: 0.7831 âŒ\n",
      "     Closest Support: Amphibolis_DBCA_training_136_patch_4.jpg\n",
      " 5. Halophila_CMER_test_94_patch_14.jpg | Halophila    â†’ Amphibolis  \n",
      "     Conf: 0.8144 | Dissim: 0.7456 | Score: 0.7731 âŒ\n",
      "     Closest Support: Amphibolis_DBCA_training_136_patch_4.jpg\n",
      "\n",
      "Background (Top 5 out of 1000 total):\n",
      "------------------------------------------------------------------------------------------\n",
      " 1. Halophila_CMER_test_259_patch_11.jp | Halophila    â†’ Background  \n",
      "     Conf: 0.8943 | Dissim: 0.7212 | Score: 0.7904 âŒ\n",
      "     Closest Support: Background_DBCA_training_391_patch_9.jpg\n",
      " 2. Halophila_CMER_test_54_patch_7.jpg  | Halophila    â†’ Background  \n",
      "     Conf: 0.9168 | Dissim: 0.7016 | Score: 0.7877 âŒ\n",
      "     Closest Support: Background_DBCA_training_391_patch_9.jpg\n",
      " 3. Posidonia_DBCA_test_152_patch_5.jpg | Posidonia    â†’ Background  \n",
      "     Conf: 0.9184 | Dissim: 0.6891 | Score: 0.7808 âŒ\n",
      "     Closest Support: Background_DBCA_training_391_patch_9.jpg\n",
      " 4. Halophila_CMER_test_7_patch_11.jpg  | Halophila    â†’ Background  \n",
      "     Conf: 0.8518 | Dissim: 0.7243 | Score: 0.7753 âŒ\n",
      "     Closest Support: Background_DBCA_training_391_patch_9.jpg\n",
      " 5. Amphibolis_DBCA_test_233_patch_8.jp | Amphibolis   â†’ Background  \n",
      "     Conf: 0.9364 | Dissim: 0.6571 | Score: 0.7688 âŒ\n",
      "     Closest Support: Background_DBCA_training_391_patch_9.jpg\n",
      "\n",
      "Halophila (Top 5 out of 1000 total):\n",
      "------------------------------------------------------------------------------------------\n",
      " 1. Halophila_CMER_test_14_patch_10.jpg | Halophila    â†’ Halophila   \n",
      "     Conf: 0.7963 | Dissim: 0.7609 | Score: 0.7751 âœ…\n",
      "     Closest Support: Halophila_CMER_training_100_patch_12.jpg\n",
      " 2. Halophila_CMER_test_45_patch_13.jpg | Halophila    â†’ Halophila   \n",
      "     Conf: 0.9515 | Dissim: 0.6565 | Score: 0.7745 âœ…\n",
      "     Closest Support: Halophila_CMER_training_100_patch_12.jpg\n",
      " 3. Halophila_CMER_test_7_patch_22.jpg  | Halophila    â†’ Halophila   \n",
      "     Conf: 0.9521 | Dissim: 0.6495 | Score: 0.7705 âœ…\n",
      "     Closest Support: Halophila_CMER_training_100_patch_12.jpg\n",
      " 4. Posidonia_DBCA_test_15_patch_2.jpg  | Posidonia    â†’ Halophila   \n",
      "     Conf: 0.9528 | Dissim: 0.6446 | Score: 0.7679 âŒ\n",
      "     Closest Support: Halophila_CMER_training_100_patch_12.jpg\n",
      " 5. Amphibolis_DBCA_test_196_patch_3.jp | Amphibolis   â†’ Halophila   \n",
      "     Conf: 0.8257 | Dissim: 0.7217 | Score: 0.7633 âŒ\n",
      "     Closest Support: Halophila_CMER_training_100_patch_12.jpg\n",
      "\n",
      "Posidonia (Top 5 out of 1000 total):\n",
      "------------------------------------------------------------------------------------------\n",
      " 1. Amphibolis_DBCA_test_219_patch_24.j | Amphibolis   â†’ Posidonia   \n",
      "     Conf: 0.7587 | Dissim: 0.7771 | Score: 0.7697 âŒ\n",
      "     Closest Support: Posidonia_DBCA_training_703_patch_22.jpg\n",
      " 2. Halophila_CMER_test_259_patch_18.jp | Halophila    â†’ Posidonia   \n",
      "     Conf: 0.9064 | Dissim: 0.6715 | Score: 0.7655 âŒ\n",
      "     Closest Support: Posidonia_DBCA_training_703_patch_22.jpg\n",
      " 3. Halophila_CMER_test_14_patch_20.jpg | Halophila    â†’ Posidonia   \n",
      "     Conf: 0.7311 | Dissim: 0.7843 | Score: 0.7630 âŒ\n",
      "     Closest Support: Posidonia_DBCA_training_703_patch_22.jpg\n",
      " 4. Halophila_CMER_test_27_patch_14.jpg | Halophila    â†’ Posidonia   \n",
      "     Conf: 0.8851 | Dissim: 0.6583 | Score: 0.7490 âŒ\n",
      "     Closest Support: Posidonia_DBCA_training_703_patch_22.jpg\n",
      " 5. Posidonia_DBCA_test_145_patch_19.jp | Posidonia    â†’ Posidonia   \n",
      "     Conf: 0.8454 | Dissim: 0.6690 | Score: 0.7396 âœ…\n",
      "     Closest Support: Posidonia_DBCA_training_703_patch_22.jpg\n",
      "\n",
      "ðŸ“Š Visual Dissimilarity Analysis:\n",
      "  Average Dissimilarity (Correct): 0.0917\n",
      "  Average Dissimilarity (Incorrect): 0.1298\n",
      "  Dissimilarity Difference: 0.0381\n",
      "\n",
      "ðŸ“Š FINAL COMPREHENSIVE RESULTS:\n",
      "  Enhanced Ensemble: 0.6610 (Â±0.0004)\n",
      "  Comprehensive Analysis: 0.6489\n",
      "  Total Test Images Processed: 20872\n",
      "  Best Single Model: 0.6610\n",
      "  Weighted F1-Score: 0.6237\n",
      "  Total Top Predictions Saved: 4000\n",
      "  Top Predictions Per Class: [1000, 1000, 1000, 1000]\n",
      "\n",
      "âœ… Enhanced results saved to 'enhanced_protonet_results.json'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create idx_to_label mapping (missing from original)\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "# Missing prepare_batch function\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        try:\n",
    "            img = Image.open(sample['path']).convert('RGB')\n",
    "            images.append(transform(img))\n",
    "            labels.append(sample['episode_label'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {sample['path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not images:\n",
    "        return torch.empty(0), torch.empty(0, dtype=torch.long)\n",
    "    \n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# Missing create_fixed_support_set function\n",
    "def create_fixed_support_set(data, n_way, k_shot):\n",
    "    \"\"\"Create a fixed support set for evaluation\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            selected = available[:k_shot]  # Take first k_shot samples\n",
    "        else:\n",
    "            # Repeat samples if not enough available\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def calculate_visual_dissimilarity(query_embedding, support_embeddings, support_labels, predicted_class_idx, support_paths):\n",
    "    \"\"\"Calculate visual dissimilarity between query and support images\"\"\"\n",
    "    # Get support samples for the predicted class\n",
    "    pred_class_indices = torch.where(support_labels == predicted_class_idx)[0]\n",
    "    \n",
    "    if len(pred_class_indices) == 0:\n",
    "        return {\n",
    "            'avg_dissimilarity': 1.0,\n",
    "            'min_dissimilarity': 1.0,\n",
    "            'max_dissimilarity': 1.0,\n",
    "            'closest_support_image': None,\n",
    "            'most_dissimilar_support_image': None\n",
    "        }\n",
    "    \n",
    "    # Calculate dissimilarity (1 - cosine similarity) to each support image of predicted class\n",
    "    pred_class_embeddings = support_embeddings[pred_class_indices]\n",
    "    similarities = F.cosine_similarity(query_embedding.unsqueeze(0), pred_class_embeddings, dim=1)\n",
    "    dissimilarities = 1 - similarities\n",
    "    \n",
    "    # Find indices of most and least similar support images\n",
    "    min_dissim_idx = dissimilarities.argmin().item()\n",
    "    max_dissim_idx = dissimilarities.argmax().item()\n",
    "    \n",
    "    # Get the actual support image paths\n",
    "    closest_support_idx = pred_class_indices[min_dissim_idx].item()\n",
    "    most_dissimilar_idx = pred_class_indices[max_dissim_idx].item()\n",
    "    \n",
    "    return {\n",
    "        'avg_dissimilarity': dissimilarities.mean().item(),\n",
    "        'min_dissimilarity': dissimilarities.min().item(),\n",
    "        'max_dissimilarity': dissimilarities.max().item(),\n",
    "        'closest_support_image': support_paths[closest_support_idx],\n",
    "        'most_dissimilar_support_image': support_paths[most_dissimilar_idx],\n",
    "        'dissimilarity_std': dissimilarities.std().item() if len(dissimilarities) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "def calculate_advanced_metrics(query_embedding, prototypes, support_embeddings, support_labels, predicted_class_idx, support_paths):\n",
    "    \"\"\"Enhanced metrics calculation with better confidence estimation and visual dissimilarity\"\"\"\n",
    "    # Cosine similarity with temperature (matching training)\n",
    "    similarities = F.cosine_similarity(query_embedding.unsqueeze(0), prototypes, dim=1)\n",
    "    confidences = F.softmax(similarities * 10.0, dim=0)  # Use same temperature as training\n",
    "    \n",
    "    # Entropy-based uncertainty\n",
    "    entropy = -torch.sum(confidences * torch.log(confidences + 1e-8))\n",
    "    \n",
    "    # Distance to prototype\n",
    "    pred_prototype = prototypes[predicted_class_idx]\n",
    "    dist_to_prototype = 1 - F.cosine_similarity(query_embedding, pred_prototype, dim=0)\n",
    "    \n",
    "    # Margin (difference between top 2 predictions)\n",
    "    sorted_conf, _ = torch.sort(confidences, descending=True)\n",
    "    margin = (sorted_conf[0] - sorted_conf[1]).item() if len(sorted_conf) > 1 else 1.0\n",
    "    \n",
    "    # Visual dissimilarity to support images\n",
    "    visual_dissim = calculate_visual_dissimilarity(\n",
    "        query_embedding, support_embeddings, support_labels, predicted_class_idx, support_paths\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'confidence': confidences[predicted_class_idx].item(),\n",
    "        'entropy': entropy.item(),\n",
    "        'distance_to_prototype': dist_to_prototype.item(),\n",
    "        'margin': margin,\n",
    "        'max_confidence': confidences.max().item(),\n",
    "        **visual_dissim\n",
    "    }\n",
    "\n",
    "def test_with_tta(model, test_data, prototypes, transform_eval, device, n_tta=5):\n",
    "    \"\"\"Test with Test Time Augmentation for better accuracy\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Create TTA transforms\n",
    "    tta_transforms = [\n",
    "        transform_eval,  # Original\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_data, desc=\"TTA Testing\"):\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                \n",
    "                # Collect predictions from multiple augmentations\n",
    "                tta_logits = []\n",
    "                for transform in tta_transforms[:min(n_tta, len(tta_transforms))]:\n",
    "                    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                    query_embedding = model(img_tensor)\n",
    "                    logits = torch.mm(query_embedding, prototypes.t()) * model.temperature\n",
    "                    tta_logits.append(logits)\n",
    "                \n",
    "                # Average logits\n",
    "                avg_logits = torch.stack(tta_logits).mean(0)\n",
    "                pred_idx = avg_logits.argmax().item()\n",
    "                confidence = F.softmax(avg_logits, dim=1).max().item()\n",
    "                \n",
    "                predictions.append({\n",
    "                    'predicted': pred_idx,\n",
    "                    'true': label_to_idx[sample['label']],\n",
    "                    'confidence': confidence,\n",
    "                    'path': sample['path']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy = sum(1 for p in predictions if p['predicted'] == p['true']) / len(predictions) if predictions else 0.0\n",
    "    return accuracy, predictions\n",
    "\n",
    "def ensemble_with_diversity(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, n_ensembles=5):\n",
    "    \"\"\"Enhanced ensemble with diversity-based support set selection\"\"\"\n",
    "    print(f\"ðŸŽ¯ Running Diverse Ensemble (n={n_ensembles})\")\n",
    "    \n",
    "    ensemble_predictions = []\n",
    "    ensemble_accuracies = []\n",
    "    \n",
    "    for i in range(n_ensembles):\n",
    "        # Create diverse support sets\n",
    "        support_set = create_diverse_support_set(train_data, k_shot=3, diversity_factor=i)\n",
    "        \n",
    "        model.eval()\n",
    "        support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_embeddings = model(support_images)\n",
    "            \n",
    "            # Compute prototypes with L2 normalization\n",
    "            prototypes = []\n",
    "            for cls_idx in range(len(all_labels)):\n",
    "                cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "                if len(cls_indices) > 0:\n",
    "                    proto = support_embeddings[cls_indices].mean(0)\n",
    "                    prototypes.append(F.normalize(proto, dim=0))\n",
    "                else:\n",
    "                    prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "            prototypes = torch.stack(prototypes)\n",
    "            \n",
    "            # Test with TTA\n",
    "            accuracy, predictions = test_with_tta(model, test_data, prototypes, transform_eval, device, n_tta=3)\n",
    "            \n",
    "        ensemble_accuracies.append(accuracy)\n",
    "        ensemble_predictions.append(predictions)\n",
    "        print(f\"  Ensemble {i+1}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Majority voting ensemble\n",
    "    final_predictions = []\n",
    "    for i in range(len(test_data)):\n",
    "        votes = [pred[i]['predicted'] for pred in ensemble_predictions if i < len(pred)]\n",
    "        confidence_scores = [pred[i]['confidence'] for pred in ensemble_predictions if i < len(pred)]\n",
    "        \n",
    "        if not votes:\n",
    "            continue\n",
    "        \n",
    "        # Weighted voting based on confidence\n",
    "        vote_counts = {}\n",
    "        for vote, conf in zip(votes, confidence_scores):\n",
    "            vote_counts[vote] = vote_counts.get(vote, 0) + conf\n",
    "        \n",
    "        final_pred = max(vote_counts.items(), key=lambda x: x[1])[0]\n",
    "        true_label = ensemble_predictions[0][i]['true'] if i < len(ensemble_predictions[0]) else 0\n",
    "        \n",
    "        final_predictions.append({\n",
    "            'predicted': final_pred,\n",
    "            'true': true_label,\n",
    "            'ensemble_votes': votes,\n",
    "            'avg_confidence': np.mean(confidence_scores),\n",
    "            'vote_agreement': votes.count(final_pred) / len(votes)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true = [p['true'] for p in final_predictions]\n",
    "    y_pred = [p['predicted'] for p in final_predictions]\n",
    "    \n",
    "    final_accuracy = accuracy_score(y_true, y_pred) if final_predictions else 0.0\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0) if final_predictions else 0.0\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0) if final_predictions else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0) if final_predictions else 0.0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Enhanced Ensemble Results:\")\n",
    "    print(f\"  Final Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(ensemble_accuracies):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'individual_accuracies': ensemble_accuracies,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': final_predictions,\n",
    "        'std_dev': np.std(ensemble_accuracies)\n",
    "    }\n",
    "\n",
    "def create_diverse_support_set(data, k_shot=3, diversity_factor=0):\n",
    "    \"\"\"Create diverse support sets for better ensemble performance\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    random.seed(diversity_factor * 42)  # Different seed for diversity\n",
    "    \n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            # Sample with diversity (skip some samples based on diversity_factor)\n",
    "            start_idx = (diversity_factor * 2) % max(1, len(available) - k_shot)\n",
    "            selected = available[start_idx:start_idx + k_shot]\n",
    "            if len(selected) < k_shot:\n",
    "                selected.extend(random.sample(available, k_shot - len(selected)))\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def comprehensive_prediction_analysis(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                                    transform_eval, device):\n",
    "    \"\"\"Comprehensive prediction analysis for all test images with top 1000 per class\"\"\"\n",
    "    print(\"ðŸŽ¯ Comprehensive Prediction Analysis for All Test Images...\")\n",
    "    \n",
    "    support_set = create_fixed_support_set(train_data, n_way=4, k_shot=1)\n",
    "    model.eval()\n",
    "    support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "    \n",
    "    # Store support image paths for dissimilarity calculation\n",
    "    support_paths = [item['path'] for item in support_set]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(support_images)\n",
    "        \n",
    "        # Compute normalized prototypes\n",
    "        prototypes = []\n",
    "        for cls_idx in range(len(all_labels)):\n",
    "            cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "            if len(cls_indices) > 0:\n",
    "                proto = support_embeddings[cls_indices].mean(0)\n",
    "                prototypes.append(F.normalize(proto, dim=0))\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        all_predictions = []\n",
    "        print(f\"Processing {len(test_data)} test images...\")\n",
    "        \n",
    "        for sample in tqdm(test_data, desc=\"Analyzing all test images\"):\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                img_tensor = transform_eval(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                query_embedding = F.normalize(model(img_tensor).squeeze(), dim=0)\n",
    "                \n",
    "                # Get prediction\n",
    "                similarities = F.cosine_similarity(query_embedding.unsqueeze(0), prototypes, dim=1)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                \n",
    "                # Calculate advanced metrics including visual dissimilarity\n",
    "                metrics = calculate_advanced_metrics(\n",
    "                    query_embedding, prototypes, support_embeddings, support_labels, pred_idx, support_paths\n",
    "                )\n",
    "                \n",
    "                all_predictions.append({\n",
    "                    'image_path': sample['path'],\n",
    "                    'true_label': sample['label'],\n",
    "                    'predicted_label': idx_to_label[pred_idx],\n",
    "                    'is_correct': pred_idx == label_to_idx[sample['label']],\n",
    "                    'predicted_class_idx': pred_idx,\n",
    "                    **metrics\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    overall_accuracy = sum(1 for p in all_predictions if p['is_correct']) / len(all_predictions) if all_predictions else 0.0\n",
    "    print(f\"ðŸŽ¯ Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    # Group predictions by predicted class and get top 1000 most dissimilar and high confidence for each class\n",
    "    class_predictions = {label: [] for label in all_labels}\n",
    "    \n",
    "    for pred in all_predictions:\n",
    "        predicted_class = pred['predicted_label']\n",
    "        class_predictions[predicted_class].append(pred)\n",
    "    \n",
    "    top_predictions_per_class = {}\n",
    "    \n",
    "    print(f\"\\nðŸ† Extracting Top 1000 Most Dissimilar & High Confidence Predictions Per Class:\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for class_name in all_labels:\n",
    "        class_preds = class_predictions[class_name]\n",
    "        if not class_preds:\n",
    "            print(f\"{class_name}: No predictions found\")\n",
    "            top_predictions_per_class[class_name] = []\n",
    "            continue\n",
    "        \n",
    "        # Sort by dissimilarity (descending) and confidence (descending)\n",
    "        # Combined score: high dissimilarity + high confidence\n",
    "        for pred in class_preds:\n",
    "            # Normalize dissimilarity and confidence to [0, 1] range\n",
    "            dissim_score = pred.get('avg_dissimilarity', 0)\n",
    "            conf_score = pred.get('confidence', 0)\n",
    "            # Weighted combination: 60% dissimilarity, 40% confidence\n",
    "            pred['combined_score'] = 0.6 * dissim_score + 0.4 * conf_score\n",
    "        \n",
    "        # Sort by combined score (higher is better for our criteria)\n",
    "        class_preds.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        # Take top 1000 or all available if less than 1000\n",
    "        top_count = min(1000, len(class_preds))\n",
    "        top_predictions_per_class[class_name] = class_preds[:top_count]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_dissim = np.mean([p.get('avg_dissimilarity', 0) for p in class_preds[:top_count]])\n",
    "        avg_conf = np.mean([p.get('confidence', 0) for p in class_preds[:top_count]])\n",
    "        accuracy_top = sum(1 for p in class_preds[:top_count] if p['is_correct']) / top_count if top_count > 0 else 0\n",
    "        \n",
    "        print(f\"{class_name:<15}: {top_count:>4} predictions | \"\n",
    "              f\"Avg Dissim: {avg_dissim:.4f} | Avg Conf: {avg_conf:.4f} | Accuracy: {accuracy_top:.4f}\")\n",
    "    \n",
    "    return all_predictions, top_predictions_per_class, overall_accuracy\n",
    "\n",
    "def analyze_predictions_by_confidence(predictions, n_bins=5):\n",
    "    \"\"\"Analyze prediction quality across confidence bins\"\"\"\n",
    "    print(f\"\\nðŸ“Š Confidence-based Analysis ({n_bins} bins):\")\n",
    "    \n",
    "    # Sort by confidence and create bins\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x['confidence'], reverse=True)\n",
    "    bin_size = len(sorted_preds) // n_bins\n",
    "    \n",
    "    print(f\"{'Bin':<5} {'Range':<20} {'Count':<8} {'Accuracy':<10} {'Avg Margin':<12} {'Avg Dissim':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    bin_results = []\n",
    "    for i in range(n_bins):\n",
    "        start_idx = i * bin_size\n",
    "        end_idx = (i + 1) * bin_size if i < n_bins - 1 else len(sorted_preds)\n",
    "        bin_preds = sorted_preds[start_idx:end_idx]\n",
    "        \n",
    "        if not bin_preds:\n",
    "            continue\n",
    "            \n",
    "        bin_accuracy = sum(1 for p in bin_preds if p['is_correct']) / len(bin_preds)\n",
    "        avg_confidence = sum(p['confidence'] for p in bin_preds) / len(bin_preds)\n",
    "        avg_margin = sum(p.get('margin', 0) for p in bin_preds) / len(bin_preds)\n",
    "        avg_dissim = sum(p.get('avg_dissimilarity', 0) for p in bin_preds) / len(bin_preds)\n",
    "        \n",
    "        conf_range = f\"{bin_preds[-1]['confidence']:.3f}-{bin_preds[0]['confidence']:.3f}\"\n",
    "        \n",
    "        print(f\"{i+1:<5} {conf_range:<20} {len(bin_preds):<8} {bin_accuracy:<10.3f} {avg_margin:<12.4f} {avg_dissim:<12.4f}\")\n",
    "        \n",
    "        bin_results.append({\n",
    "            'bin': i+1,\n",
    "            'accuracy': bin_accuracy,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'avg_dissimilarity': avg_dissim,\n",
    "            'count': len(bin_preds)\n",
    "        })\n",
    "    \n",
    "    return bin_results\n",
    "\n",
    "def create_confusion_matrix_plot(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Create and save confusion matrix visualization\"\"\"\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Confusion matrix saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating confusion matrix: {e}\")\n",
    "\n",
    "# Main execution\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ COMPREHENSIVE PROTONET EVALUATION - ALL TEST IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Enhanced Ensemble Analysis\n",
    "ensemble_results = ensemble_with_diversity(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, n_ensembles=5\n",
    ")\n",
    "\n",
    "# Comprehensive Analysis of All Test Images\n",
    "all_predictions, top_predictions_per_class, overall_accuracy = comprehensive_prediction_analysis(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, transform_eval, device\n",
    ")\n",
    "\n",
    "# Confidence-based Analysis\n",
    "confidence_analysis = analyze_predictions_by_confidence(all_predictions, n_bins=5)\n",
    "\n",
    "# Create Confusion Matrix\n",
    "y_true = [label_to_idx[p['true_label']] for p in all_predictions]\n",
    "y_pred = [label_to_idx[p['predicted_label']] for p in all_predictions]\n",
    "create_confusion_matrix_plot(y_true, y_pred, all_labels)\n",
    "\n",
    "# Per-class Analysis\n",
    "print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Support':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "class_report = classification_report(y_true, y_pred, target_names=all_labels, output_dict=True, zero_division=0)\n",
    "for class_name in all_labels:\n",
    "    metrics = class_report[class_name]\n",
    "    print(f\"{class_name:<12} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} \"\n",
    "          f\"{metrics['f1-score']:<10.3f} {int(metrics['support']):<10}\")\n",
    "\n",
    "# Top Predictions Analysis with Visual Dissimilarity for each class\n",
    "print(f\"\\nðŸ† Top 5 Most Dissimilar & High Confidence Predictions Per Class:\")\n",
    "print(\"=\" * 100)\n",
    "for class_name in all_labels:\n",
    "    top_class_preds = top_predictions_per_class[class_name]\n",
    "    if not top_class_preds:\n",
    "        print(f\"\\n{class_name}: No predictions available\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{class_name} (Top 5 out of {len(top_class_preds)} total):\")\n",
    "    print(\"-\" * 90)\n",
    "    for i, pred in enumerate(top_class_preds[:5]):\n",
    "        status = \"âœ…\" if pred['is_correct'] else \"âŒ\"\n",
    "        filename = os.path.basename(pred['image_path'])\n",
    "        print(f\"{i+1:2d}. {filename[:35]:<35} | {pred['true_label']:<12} â†’ {pred['predicted_label']:<12}\")\n",
    "        print(f\"     Conf: {pred['confidence']:.4f} | Dissim: {pred.get('avg_dissimilarity', 0):.4f} | \"\n",
    "              f\"Score: {pred.get('combined_score', 0):.4f} {status}\")\n",
    "        if pred.get('closest_support_image'):\n",
    "            closest_file = os.path.basename(pred['closest_support_image'])\n",
    "            print(f\"     Closest Support: {closest_file[:40]}\")\n",
    "\n",
    "# Visual Dissimilarity Analysis\n",
    "print(f\"\\nðŸ“Š Visual Dissimilarity Analysis:\")\n",
    "correct_preds = [p for p in all_predictions if p['is_correct']]\n",
    "incorrect_preds = [p for p in all_predictions if not p['is_correct']]\n",
    "\n",
    "if correct_preds and incorrect_preds:\n",
    "    avg_dissim_correct = np.mean([p.get('avg_dissimilarity', 0) for p in correct_preds])\n",
    "    avg_dissim_incorrect = np.mean([p.get('avg_dissimilarity', 0) for p in incorrect_preds])\n",
    "    print(f\"  Average Dissimilarity (Correct): {avg_dissim_correct:.4f}\")\n",
    "    print(f\"  Average Dissimilarity (Incorrect): {avg_dissim_incorrect:.4f}\")\n",
    "    print(f\"  Dissimilarity Difference: {avg_dissim_incorrect - avg_dissim_correct:.4f}\")\n",
    "\n",
    "# Final Summary\n",
    "print(f\"\\nðŸ“Š FINAL COMPREHENSIVE RESULTS:\")\n",
    "print(f\"  Enhanced Ensemble: {ensemble_results['final_accuracy']:.4f} (Â±{ensemble_results['std_dev']:.4f})\")\n",
    "print(f\"  Comprehensive Analysis: {overall_accuracy:.4f}\")\n",
    "print(f\"  Total Test Images Processed: {len(all_predictions)}\")\n",
    "print(f\"  Best Single Model: {max(ensemble_results['individual_accuracies']):.4f}\")\n",
    "print(f\"  Weighted F1-Score: {ensemble_results['f1']:.4f}\")\n",
    "\n",
    "# Calculate total top predictions saved\n",
    "total_top_predictions = sum(len(preds) for preds in top_predictions_per_class.values())\n",
    "print(f\"  Total Top Predictions Saved: {total_top_predictions}\")\n",
    "print(f\"  Top Predictions Per Class: {[len(top_predictions_per_class[cls]) for cls in all_labels]}\")\n",
    "\n",
    "# Save comprehensive results with top 1000 per class\n",
    "final_results = {\n",
    "    'enhanced_ensemble': ensemble_results,\n",
    "    'comprehensive_analysis': {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'total_predictions': len(all_predictions),\n",
    "        'confidence_analysis': confidence_analysis,\n",
    "        'top_predictions_per_class': top_predictions_per_class,\n",
    "        'top_predictions_summary': {\n",
    "            'total_saved': total_top_predictions,\n",
    "            'per_class_counts': {cls: len(top_predictions_per_class[cls]) for cls in all_labels}\n",
    "        },\n",
    "        'visual_dissimilarity_stats': {\n",
    "            'avg_dissim_correct': np.mean([p.get('avg_dissimilarity', 0) for p in correct_preds]) if correct_preds else 0,\n",
    "            'avg_dissim_incorrect': np.mean([p.get('avg_dissimilarity', 0) for p in incorrect_preds]) if incorrect_preds else 0,\n",
    "            'dissim_difference': (np.mean([p.get('avg_dissimilarity', 0) for p in incorrect_preds]) - \n",
    "                                 np.mean([p.get('avg_dissimilarity', 0) for p in correct_preds])) if (correct_preds and incorrect_preds) else 0\n",
    "        }\n",
    "    },\n",
    "    'all_predictions': all_predictions,  # All predictions with detailed metrics\n",
    "    'class_performance': class_report,\n",
    "    'summary': {\n",
    "        'best_ensemble_accuracy': ensemble_results['final_accuracy'],\n",
    "        'comprehensive_accuracy': overall_accuracy,\n",
    "        'ensemble_std': ensemble_results['std_dev'],\n",
    "        'weighted_f1': ensemble_results['f1'],\n",
    "        'total_images_processed': len(all_predictions)\n",
    "    }\n",
    "}\n",
    "    \n",
    "with open('enhanced_protonet_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nâœ… Enhanced results saved to 'enhanced_protonet_results.json'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09a5388e-73b4-453d-85cf-0d32600e5420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting 1+1 shot training (2-shot total per class)\n",
      "============================================================\n",
      "  - Original shots per class: 1\n",
      "  - Enhanced shots per class: 1\n",
      "  - Query samples per class: 15\n",
      "  - Total training episodes: 4000\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 0.9755 | Train: 0.633 | Val: 0.603 | LR: 2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:17<18:57:00, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:20<1:26:47,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.0105 | Train: 1.000 | Val: 0.662 | LR: 2.21e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:37<6:42:24,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:40<1:22:26,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.0097 | Train: 1.000 | Val: 0.732 | LR: 2.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [02:57<6:29:04,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 150/4000 [04:01<1:25:28,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.0097 | Train: 1.000 | Val: 0.745 | LR: 3.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [04:19<6:43:31,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [05:37<5:59:53,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.0096 | Train: 1.000 | Val: 0.696 | LR: 5.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [06:52<6:02:38,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.0095 | Train: 1.000 | Val: 0.694 | LR: 7.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [08:09<6:01:56,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.0094 | Train: 1.000 | Val: 0.697 | LR: 9.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [09:26<5:56:47,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.0091 | Train: 1.000 | Val: 0.698 | LR: 1.15e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [10:45<5:15:03,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.0087 | Train: 1.000 | Val: 0.687 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [11:59<5:44:29,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.0083 | Train: 1.000 | Val: 0.668 | LR: 1.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [13:12<4:44:33,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.0078 | Train: 1.000 | Val: 0.678 | LR: 1.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [14:24<5:04:22,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.0077 | Train: 1.000 | Val: 0.687 | LR: 2.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [15:44<5:18:31,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.0076 | Train: 1.000 | Val: 0.693 | LR: 2.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [17:03<5:18:49,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.0075 | Train: 1.000 | Val: 0.667 | LR: 2.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [18:15<5:06:38,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.0073 | Train: 1.000 | Val: 0.654 | LR: 3.23e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [19:30<5:14:04,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.0073 | Train: 1.000 | Val: 0.670 | LR: 3.53e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [20:48<5:12:08,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.0072 | Train: 1.000 | Val: 0.683 | LR: 3.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [22:08<5:25:50,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.0072 | Train: 1.000 | Val: 0.667 | LR: 4.07e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [23:24<4:49:41,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.0072 | Train: 1.000 | Val: 0.652 | LR: 4.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [24:39<4:52:56,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.0071 | Train: 1.000 | Val: 0.658 | LR: 4.51e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [25:56<4:54:21,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.0071 | Train: 1.000 | Val: 0.638 | LR: 4.68e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [27:14<4:57:39,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.0071 | Train: 1.000 | Val: 0.665 | LR: 4.82e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [28:35<4:33:33,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.0071 | Train: 1.000 | Val: 0.621 | LR: 4.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [29:48<4:19:46,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.0070 | Train: 1.000 | Val: 0.584 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [31:03<4:23:55,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.0070 | Train: 1.000 | Val: 0.649 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [32:23<4:33:48,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.0070 | Train: 1.000 | Val: 0.645 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [33:42<4:27:53,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.0070 | Train: 1.000 | Val: 0.623 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [35:01<4:09:45,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.0069 | Train: 1.000 | Val: 0.687 | LR: 4.96e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [36:15<4:17:05,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.0069 | Train: 1.000 | Val: 0.608 | LR: 4.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [37:31<3:49:21,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.0069 | Train: 1.000 | Val: 0.633 | LR: 4.90e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [38:51<4:01:08,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.0069 | Train: 1.000 | Val: 0.651 | LR: 4.86e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [40:10<3:58:35,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.0069 | Train: 1.000 | Val: 0.645 | LR: 4.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [41:25<3:42:39,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.0069 | Train: 1.000 | Val: 0.628 | LR: 4.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [42:39<3:32:52,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.0068 | Train: 1.000 | Val: 0.627 | LR: 4.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [43:56<3:41:27,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.0068 | Train: 1.000 | Val: 0.646 | LR: 4.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [45:14<3:34:37,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.0068 | Train: 1.000 | Val: 0.649 | LR: 4.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [46:34<3:36:15,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.0068 | Train: 1.000 | Val: 0.639 | LR: 4.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [47:47<3:23:40,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.0068 | Train: 1.000 | Val: 0.654 | LR: 4.36e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [49:01<3:16:14,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.0068 | Train: 1.000 | Val: 0.614 | LR: 4.26e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [50:19<3:22:42,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.0068 | Train: 1.000 | Val: 0.631 | LR: 4.16e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 2001/4000 [51:36<3:19:56,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.0068 | Train: 1.000 | Val: 0.592 | LR: 4.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 2051/4000 [52:54<2:55:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.0067 | Train: 1.000 | Val: 0.668 | LR: 3.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 2101/4000 [54:07<3:03:49,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.0067 | Train: 1.000 | Val: 0.630 | LR: 3.83e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 2151/4000 [55:23<3:00:01,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.0067 | Train: 1.000 | Val: 0.594 | LR: 3.70e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 2201/4000 [56:42<2:54:42,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.0067 | Train: 1.000 | Val: 0.633 | LR: 3.58e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2251/4000 [58:02<2:53:38,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.0099 | Train: 1.000 | Val: 0.619 | LR: 3.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 2301/4000 [59:18<2:33:09,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.0099 | Train: 1.000 | Val: 0.541 | LR: 3.32e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2351/4000 [1:00:31<2:41:05,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.0093 | Train: 1.000 | Val: 0.520 | LR: 3.19e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2401/4000 [1:01:50<2:37:43,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.0089 | Train: 1.000 | Val: 0.630 | LR: 3.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:03:08<2:28:19,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.0094 | Train: 1.000 | Val: 0.593 | LR: 2.91e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:04:20<1:38:39,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.0087 | Train: 1.000 | Val: 0.566 | LR: 2.77e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:05:24<1:36:11,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.0091 | Train: 1.000 | Val: 0.632 | LR: 2.63e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:06:37<1:59:05,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.0090 | Train: 1.000 | Val: 0.661 | LR: 2.49e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:07:55<2:15:04,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.0090 | Train: 1.000 | Val: 0.573 | LR: 2.35e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:09:13<2:11:10,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.0086 | Train: 1.000 | Val: 0.648 | LR: 2.21e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:10:34<2:07:11,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.0088 | Train: 1.000 | Val: 0.589 | LR: 2.08e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:11:50<1:59:16,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.0087 | Train: 1.000 | Val: 0.595 | LR: 1.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:13:05<1:52:20,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.0086 | Train: 1.000 | Val: 0.550 | LR: 1.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:14:26<1:50:12,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.0085 | Train: 1.000 | Val: 0.584 | LR: 1.67e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:15:45<1:46:42,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.0084 | Train: 1.000 | Val: 0.608 | LR: 1.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:17:05<1:39:48,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.0085 | Train: 1.000 | Val: 0.646 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:18:20<1:35:45,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.0084 | Train: 1.000 | Val: 0.628 | LR: 1.29e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:19:39<1:32:33,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.0086 | Train: 1.000 | Val: 0.573 | LR: 1.17e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:20:57<1:24:13,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.0083 | Train: 1.000 | Val: 0.595 | LR: 1.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:22:19<1:23:52,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.0087 | Train: 1.000 | Val: 0.620 | LR: 9.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:23:34<1:12:51,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.0084 | Train: 1.000 | Val: 0.581 | LR: 8.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 3301/4000 [1:24:49<1:09:34,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.0082 | Train: 1.000 | Val: 0.602 | LR: 7.28e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:26:09<1:04:28,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.0083 | Train: 1.000 | Val: 0.582 | LR: 6.32e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 3401/4000 [1:27:26<1:00:55,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.0082 | Train: 1.000 | Val: 0.569 | LR: 5.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 3451/4000 [1:28:47<51:30,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.0084 | Train: 1.000 | Val: 0.609 | LR: 4.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:30:01<48:56,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.0083 | Train: 1.000 | Val: 0.599 | LR: 3.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:31:19<44:28,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.0082 | Train: 1.000 | Val: 0.577 | LR: 3.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:32:37<37:16,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.0082 | Train: 1.000 | Val: 0.618 | LR: 2.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:33:57<35:13,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.0083 | Train: 1.000 | Val: 0.679 | LR: 1.88e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3701/4000 [1:35:13<26:04,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 | Loss: 0.0085 | Train: 1.000 | Val: 0.576 | LR: 1.38e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3751/4000 [1:36:27<22:57,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3750 | Loss: 0.0082 | Train: 1.000 | Val: 0.587 | LR: 9.62e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3801/4000 [1:37:44<19:19,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 | Loss: 0.0082 | Train: 1.000 | Val: 0.648 | LR: 6.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3851/4000 [1:39:02<14:30,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3850 | Loss: 0.0084 | Train: 1.000 | Val: 0.567 | LR: 3.44e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3901/4000 [1:40:24<09:56,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 | Loss: 0.0083 | Train: 1.000 | Val: 0.620 | LR: 1.51e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3951/4000 [1:41:26<02:39,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3950 | Loss: 0.0083 | Train: 1.000 | Val: 0.581 | LR: 3.64e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [1:42:16<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 0.745\n",
      "============================================================\n",
      "\n",
      "Best model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Enhanced transforms with stronger augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.6 + dissim * 0.4\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last few layers instead of freezing everything\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with dropout and batch norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Temperature parameter for distance scaling\n",
    "        self.temperature = nn.Parameter(torch.tensor(10.0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return F.normalize(self.projection(features), dim=1)\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# Model and training setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, total_steps=4000, pct_start=0.3\n",
    ")\n",
    "\n",
    "# Training loop - 2-shot (1+1)\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "total_k_shot = k_shot_original + k_shot_enhanced\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 400\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting 1+1 shot training (2-shot total per class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Original shots per class: {k_shot_original}\")\n",
    "print(f\"  - Enhanced shots per class: {k_shot_enhanced}\")\n",
    "print(f\"  - Query samples per class: {q_query}\")\n",
    "print(f\"  - Total training episodes: 4000\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    # Forward pass\n",
    "    support_embeddings = model(support_images)\n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Compute prototypes (2 samples per class)\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    # Cosine similarity with temperature scaling\n",
    "    logits = torch.mm(query_embeddings, prototypes.t()) * model.temperature\n",
    "    loss = F.cross_entropy(logits, query_labels)\n",
    "    \n",
    "    # Add prototype regularization\n",
    "    proto_reg = 0.01 * torch.mean(torch.norm(prototypes, dim=1))\n",
    "    loss += proto_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            for _ in range(20):\n",
    "                # Create standard 1-shot validation episode\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    # 1 support sample\n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    # Query samples\n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_logits = torch.mm(val_query_embeddings, val_prototypes.t()) * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | Val: {val_acc_avg:.3f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c37e960f-24a1-490c-b749-37a7631d479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ENSEMBLE EVALUATION WITH TTA\n",
      "================================================================================\n",
      "ðŸŽ¯ Running Diverse Ensemble (n=5, k_shot=2)\n",
      "  Ensemble 1: 0.7378\n",
      "  Ensemble 2: 0.7381\n",
      "  Ensemble 3: 0.7371\n",
      "  Ensemble 4: 0.7363\n",
      "  Ensemble 5: 0.7380\n",
      "\n",
      "ðŸ“Š Enhanced Ensemble Results:\n",
      "  Final Accuracy: 0.7384\n",
      "  Precision: 0.7591\n",
      "  Recall: 0.7384\n",
      "  F1-Score: 0.7257\n",
      "  Std Dev: 0.0007\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Amphibolis      0.823        0.714        0.765        5375      \n",
      "Background      0.794        0.426        0.554        4706      \n",
      "Halophila       0.654        0.994        0.789        5669      \n",
      "Posidonia       0.776        0.768        0.772        5122      \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg       0.762        0.726        0.720        20872     \n",
      "Weighted Avg    0.759        0.738        0.726        20872     \n",
      "âœ… Confusion matrix saved to confusion_matrix_ensemble.png\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\n",
      "================================================================================\n",
      "ðŸŽ¯ Comprehensive Analysis for All Test Images...\n",
      "Processing 20872 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [06:23<00:00, 54.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Overall Accuracy: 0.7352\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Amphibolis      0.797        0.746        0.771        5375      \n",
      "Background      0.847        0.412        0.555        4706      \n",
      "Halophila       0.651        0.990        0.786        5669      \n",
      "Posidonia       0.767        0.739        0.753        5122      \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg       0.766        0.722        0.716        20872     \n",
      "Weighted Avg    0.761        0.735        0.722        20872     \n",
      "âœ… Confusion matrix saved to confusion_matrix_comprehensive.png\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FINAL SUMMARY\n",
      "================================================================================\n",
      "Enhanced Ensemble: 0.7384 (Â±0.0007)\n",
      "Comprehensive Analysis: 0.7352\n",
      "Total Test Images Processed: 20872\n",
      "Best Single Model: 0.7381\n",
      "Ensemble Weighted F1-Score: 0.7257\n",
      "\n",
      "âœ… Results saved to 'evaluation_results_metrics_only.json'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create idx_to_label mapping\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "def create_fixed_support_set(data, n_way, k_shot):\n",
    "    \"\"\"Create a fixed support set for evaluation\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            selected = available[:k_shot]\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def create_diverse_support_set(data, k_shot=2, diversity_factor=0):\n",
    "    \"\"\"Create diverse support sets for ensemble\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    random.seed(diversity_factor * 42)\n",
    "    \n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            start_idx = (diversity_factor * 2) % max(1, len(available) - k_shot)\n",
    "            selected = available[start_idx:start_idx + k_shot]\n",
    "            if len(selected) < k_shot:\n",
    "                selected.extend(random.sample(available, k_shot - len(selected)))\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3):\n",
    "    \"\"\"Test with Test Time Augmentation - returns only predictions for metrics\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transform_eval,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in test_data:\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                \n",
    "                tta_logits = []\n",
    "                for transform in tta_transforms[:n_tta]:\n",
    "                    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                    query_embedding = model(img_tensor)\n",
    "                    logits = torch.mm(query_embedding, prototypes.t()) * model.temperature\n",
    "                    tta_logits.append(logits)\n",
    "                \n",
    "                avg_logits = torch.stack(tta_logits).mean(0)\n",
    "                pred_idx = avg_logits.argmax().item()\n",
    "                \n",
    "                predictions.append({\n",
    "                    'predicted': pred_idx,\n",
    "                    'true': label_to_idx[sample['label']]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy = sum(1 for p in predictions if p['predicted'] == p['true']) / len(predictions) if predictions else 0.0\n",
    "    return accuracy, predictions\n",
    "\n",
    "def ensemble_with_diversity(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, n_ensembles=5, k_shot=2):\n",
    "    \"\"\"Enhanced ensemble with diversity and TTA - metrics only\"\"\"\n",
    "    print(f\"ðŸŽ¯ Running Diverse Ensemble (n={n_ensembles}, k_shot={k_shot})\")\n",
    "    \n",
    "    ensemble_predictions = []\n",
    "    ensemble_accuracies = []\n",
    "    \n",
    "    for i in range(n_ensembles):\n",
    "        support_set = create_diverse_support_set(train_data, k_shot=k_shot, diversity_factor=i)\n",
    "        \n",
    "        model.eval()\n",
    "        support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_embeddings = model(support_images)\n",
    "            \n",
    "            prototypes = []\n",
    "            for cls_idx in range(len(all_labels)):\n",
    "                cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "                if len(cls_indices) > 0:\n",
    "                    proto = support_embeddings[cls_indices].mean(0)\n",
    "                    prototypes.append(F.normalize(proto, dim=0))\n",
    "                else:\n",
    "                    prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "            prototypes = torch.stack(prototypes)\n",
    "            \n",
    "            accuracy, predictions = test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3)\n",
    "            \n",
    "        ensemble_accuracies.append(accuracy)\n",
    "        ensemble_predictions.append(predictions)\n",
    "        print(f\"  Ensemble {i+1}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Majority voting\n",
    "    final_y_true = []\n",
    "    final_y_pred = []\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        votes = [pred[i]['predicted'] for pred in ensemble_predictions if i < len(pred)]\n",
    "        \n",
    "        if not votes:\n",
    "            continue\n",
    "        \n",
    "        # Simple majority voting\n",
    "        vote_counts = {}\n",
    "        for vote in votes:\n",
    "            vote_counts[vote] = vote_counts.get(vote, 0) + 1\n",
    "        \n",
    "        final_pred = max(vote_counts.items(), key=lambda x: x[1])[0]\n",
    "        true_label = ensemble_predictions[0][i]['true']\n",
    "        \n",
    "        final_y_true.append(true_label)\n",
    "        final_y_pred.append(final_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_accuracy = accuracy_score(final_y_true, final_y_pred)\n",
    "    precision = precision_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Enhanced Ensemble Results:\")\n",
    "    print(f\"  Final Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(ensemble_accuracies):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': final_y_true,\n",
    "        'y_pred': final_y_pred,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'individual_accuracies': ensemble_accuracies,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'std_dev': np.std(ensemble_accuracies),\n",
    "        'mean_accuracy': np.mean(ensemble_accuracies),\n",
    "        'min_accuracy': np.min(ensemble_accuracies),\n",
    "        'max_accuracy': np.max(ensemble_accuracies)\n",
    "    }\n",
    "\n",
    "def comprehensive_analysis(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, k_shot=2):\n",
    "    \"\"\"Comprehensive analysis - metrics only, no predictions saved\"\"\"\n",
    "    print(\"ðŸŽ¯ Comprehensive Analysis for All Test Images...\")\n",
    "    \n",
    "    support_set = create_fixed_support_set(train_data, n_way=len(all_labels), k_shot=k_shot)\n",
    "    model.eval()\n",
    "    support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(support_images)\n",
    "        \n",
    "        prototypes = []\n",
    "        for cls_idx in range(len(all_labels)):\n",
    "            cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "            if len(cls_indices) > 0:\n",
    "                proto = support_embeddings[cls_indices].mean(0)\n",
    "                prototypes.append(F.normalize(proto, dim=0))\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        print(f\"Processing {len(test_data)} test images...\")\n",
    "        \n",
    "        for sample in tqdm(test_data, desc=\"Analyzing test images\"):\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                img_tensor = transform_eval(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                query_embedding = F.normalize(model(img_tensor).squeeze(), dim=0)\n",
    "                \n",
    "                similarities = F.cosine_similarity(query_embedding.unsqueeze(0), prototypes, dim=1)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                \n",
    "                y_true.append(label_to_idx[sample['label']])\n",
    "                y_pred.append(pred_idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"ðŸŽ¯ Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return y_true, y_pred, overall_accuracy\n",
    "\n",
    "def create_confusion_matrix_plot(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Create and save confusion matrix visualization\"\"\"\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Count'})\n",
    "        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Confusion matrix saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating confusion matrix: {e}\")\n",
    "\n",
    "def print_classification_report(y_true, y_pred, all_labels):\n",
    "    \"\"\"Print detailed classification report\"\"\"\n",
    "    print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    class_report = classification_report(y_true, y_pred, target_names=all_labels, \n",
    "                                        output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_name in all_labels:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"{class_name:<15} {metrics['precision']:<12.3f} {metrics['recall']:<12.3f} \"\n",
    "              f\"{metrics['f1-score']:<12.3f} {int(metrics['support']):<10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Averages\n",
    "    macro_avg = class_report['macro avg']\n",
    "    weighted_avg = class_report['weighted avg']\n",
    "    \n",
    "    print(f\"{'Macro Avg':<15} {macro_avg['precision']:<12.3f} {macro_avg['recall']:<12.3f} \"\n",
    "          f\"{macro_avg['f1-score']:<12.3f} {int(macro_avg['support']):<10}\")\n",
    "    print(f\"{'Weighted Avg':<15} {weighted_avg['precision']:<12.3f} {weighted_avg['recall']:<12.3f} \"\n",
    "          f\"{weighted_avg['f1-score']:<12.3f} {int(weighted_avg['support']):<10}\")\n",
    "    \n",
    "    return class_report\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== ENSEMBLE WITH DIVERSITY AND TTA =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ENSEMBLE EVALUATION WITH TTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_results = ensemble_with_diversity(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, n_ensembles=5, k_shot=2\n",
    ")\n",
    "\n",
    "# Print ensemble metrics\n",
    "ensemble_report = print_classification_report(ensemble_results['y_true'], \n",
    "                                             ensemble_results['y_pred'], all_labels)\n",
    "\n",
    "# Create ensemble confusion matrix\n",
    "create_confusion_matrix_plot(ensemble_results['y_true'], ensemble_results['y_pred'], \n",
    "                            all_labels, save_path='confusion_matrix_ensemble.png')\n",
    "\n",
    "# ===== COMPREHENSIVE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_true_comp, y_pred_comp, overall_accuracy = comprehensive_analysis(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, k_shot=2\n",
    ")\n",
    "\n",
    "# Print comprehensive metrics\n",
    "comp_report = print_classification_report(y_true_comp, y_pred_comp, all_labels)\n",
    "\n",
    "# Create comprehensive confusion matrix\n",
    "create_confusion_matrix_plot(y_true_comp, y_pred_comp, all_labels, \n",
    "                            save_path='confusion_matrix_comprehensive.png')\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Enhanced Ensemble: {ensemble_results['final_accuracy']:.4f} (Â±{ensemble_results['std_dev']:.4f})\")\n",
    "print(f\"Comprehensive Analysis: {overall_accuracy:.4f}\")\n",
    "print(f\"Total Test Images Processed: {len(test_data)}\")\n",
    "print(f\"Best Single Model: {max(ensemble_results['individual_accuracies']):.4f}\")\n",
    "print(f\"Ensemble Weighted F1-Score: {ensemble_results['f1']:.4f}\")\n",
    "\n",
    "# ===== SAVE RESULTS =====\n",
    "final_results = {\n",
    "    'evaluation_setup': {\n",
    "        'support_shots': 2,\n",
    "        'n_way': len(all_labels),\n",
    "        'test_samples': len(test_data),\n",
    "        'classes': all_labels,\n",
    "        'ensemble_size': 5\n",
    "    },\n",
    "    'enhanced_ensemble': {\n",
    "        'final_accuracy': ensemble_results['final_accuracy'],\n",
    "        'individual_accuracies': ensemble_results['individual_accuracies'],\n",
    "        'mean_accuracy': ensemble_results['mean_accuracy'],\n",
    "        'std_dev': ensemble_results['std_dev'],\n",
    "        'min_accuracy': ensemble_results['min_accuracy'],\n",
    "        'max_accuracy': ensemble_results['max_accuracy'],\n",
    "        'precision': ensemble_results['precision'],\n",
    "        'recall': ensemble_results['recall'],\n",
    "        'f1': ensemble_results['f1'],\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': ensemble_report[cls]['precision'],\n",
    "                'recall': ensemble_report[cls]['recall'],\n",
    "                'f1_score': ensemble_report[cls]['f1-score'],\n",
    "                'support': int(ensemble_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'comprehensive_analysis': {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': comp_report[cls]['precision'],\n",
    "                'recall': comp_report[cls]['recall'],\n",
    "                'f1_score': comp_report[cls]['f1-score'],\n",
    "                'support': int(comp_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'summary': {\n",
    "        'best_ensemble_accuracy': ensemble_results['final_accuracy'],\n",
    "        'comprehensive_accuracy': overall_accuracy,\n",
    "        'ensemble_std': ensemble_results['std_dev'],\n",
    "        'weighted_f1': ensemble_results['f1'],\n",
    "        'total_images_processed': len(test_data)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation_results_metrics_only.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to 'evaluation_results_metrics_only.json'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715d3873-bbf9-4877-a927-64f1f63121c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting 1+1 shot training (2-shot total per class)\n",
      "============================================================\n",
      "  - Original shots per class: 1\n",
      "  - Enhanced shots per class: 1\n",
      "  - Query samples per class: 15\n",
      "  - Total training episodes: 4000\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 1.0627 | Train: 0.583 | Val: 0.617 | LR: 2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:14<16:29:21, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:18<1:26:44,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.0131 | Train: 1.000 | Val: 0.667 | LR: 2.21e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:36<6:56:23,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:37<1:18:43,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.0098 | Train: 1.000 | Val: 0.728 | LR: 2.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [02:56<7:03:22,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [04:08<4:59:58,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.0096 | Train: 1.000 | Val: 0.698 | LR: 3.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 200/4000 [04:56<1:26:50,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.0096 | Train: 1.000 | Val: 0.731 | LR: 5.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [05:16<7:08:56,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [06:36<6:36:42,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.0095 | Train: 1.000 | Val: 0.716 | LR: 7.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 300/4000 [07:40<1:19:59,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.0094 | Train: 1.000 | Val: 0.739 | LR: 9.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [08:01<7:14:27,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [09:18<6:11:23,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.0096 | Train: 1.000 | Val: 0.682 | LR: 1.15e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [10:34<6:12:43,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.0090 | Train: 1.000 | Val: 0.691 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [11:54<6:04:38,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.0094 | Train: 1.000 | Val: 0.659 | LR: 1.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [13:11<5:53:06,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.0089 | Train: 1.000 | Val: 0.638 | LR: 1.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [14:33<5:41:54,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.0084 | Train: 1.000 | Val: 0.650 | LR: 2.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [15:48<5:46:47,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.0081 | Train: 1.000 | Val: 0.655 | LR: 2.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [17:05<5:39:29,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.0081 | Train: 1.000 | Val: 0.635 | LR: 2.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [18:22<5:16:32,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.0101 | Train: 1.000 | Val: 0.628 | LR: 3.23e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [19:41<5:21:32,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.0102 | Train: 1.000 | Val: 0.639 | LR: 3.53e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [20:57<5:21:50,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.0101 | Train: 1.000 | Val: 0.674 | LR: 3.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [22:11<5:20:54,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.0098 | Train: 1.000 | Val: 0.654 | LR: 4.07e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [23:29<5:15:26,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.0097 | Train: 1.000 | Val: 0.593 | LR: 4.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [24:48<5:04:04,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.0114 | Train: 1.000 | Val: 0.583 | LR: 4.51e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [26:08<4:57:09,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.0097 | Train: 1.000 | Val: 0.624 | LR: 4.68e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [27:20<4:20:30,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.0096 | Train: 1.000 | Val: 0.692 | LR: 4.82e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [28:34<4:27:56,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.0093 | Train: 1.000 | Val: 0.640 | LR: 4.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [29:45<3:11:53,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.0090 | Train: 1.000 | Val: 0.639 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [31:00<4:07:43,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.0089 | Train: 1.000 | Val: 0.650 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [32:18<4:00:25,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.0089 | Train: 1.000 | Val: 0.648 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [33:32<4:21:01,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.0094 | Train: 1.000 | Val: 0.678 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [34:47<4:28:21,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.0088 | Train: 1.000 | Val: 0.683 | LR: 4.96e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [36:06<4:16:46,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.0090 | Train: 1.000 | Val: 0.689 | LR: 4.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [37:23<4:17:53,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.0083 | Train: 1.000 | Val: 0.692 | LR: 4.90e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [38:42<4:04:23,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.0082 | Train: 1.000 | Val: 0.622 | LR: 4.86e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [39:55<4:01:15,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.0081 | Train: 1.000 | Val: 0.633 | LR: 4.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [41:12<4:04:13,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.0084 | Train: 1.000 | Val: 0.629 | LR: 4.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [42:30<3:44:28,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.0080 | Train: 1.000 | Val: 0.636 | LR: 4.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [43:50<3:51:23,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.0091 | Train: 1.000 | Val: 0.626 | LR: 4.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [45:05<3:36:03,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.0079 | Train: 1.000 | Val: 0.638 | LR: 4.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [46:19<3:31:13,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.0077 | Train: 1.000 | Val: 0.606 | LR: 4.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [47:36<3:33:06,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.0077 | Train: 1.000 | Val: 0.656 | LR: 4.36e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [48:52<3:27:26,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.0078 | Train: 1.000 | Val: 0.615 | LR: 4.26e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [50:11<3:25:37,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.0079 | Train: 1.000 | Val: 0.598 | LR: 4.16e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 2001/4000 [51:24<3:08:46,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.0076 | Train: 1.000 | Val: 0.654 | LR: 4.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 2051/4000 [52:40<3:09:19,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.0077 | Train: 1.000 | Val: 0.567 | LR: 3.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 2101/4000 [53:58<3:05:53,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.0074 | Train: 1.000 | Val: 0.620 | LR: 3.83e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 2151/4000 [55:15<3:01:45,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.0076 | Train: 1.000 | Val: 0.564 | LR: 3.70e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 2201/4000 [56:34<2:47:34,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.0073 | Train: 1.000 | Val: 0.600 | LR: 3.58e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2251/4000 [57:48<2:48:48,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.0089 | Train: 1.000 | Val: 0.605 | LR: 3.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 2301/4000 [59:03<2:45:47,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.0084 | Train: 1.000 | Val: 0.640 | LR: 3.32e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2351/4000 [1:00:20<2:33:50,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.0082 | Train: 1.000 | Val: 0.615 | LR: 3.19e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2401/4000 [1:01:37<2:29:15,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.0083 | Train: 1.000 | Val: 0.625 | LR: 3.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:02:53<2:23:01,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.0081 | Train: 1.000 | Val: 0.622 | LR: 2.91e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:04:04<2:00:53,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.0085 | Train: 1.000 | Val: 0.609 | LR: 2.77e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:05:20<2:18:07,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.0083 | Train: 1.000 | Val: 0.611 | LR: 2.63e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:06:36<2:09:41,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.0125 | Train: 1.000 | Val: 0.593 | LR: 2.49e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:07:56<2:14:58,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.0080 | Train: 1.000 | Val: 0.629 | LR: 2.35e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:09:11<1:59:43,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.0079 | Train: 1.000 | Val: 0.630 | LR: 2.21e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:10:24<1:55:31,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.0081 | Train: 1.000 | Val: 0.629 | LR: 2.08e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:11:41<1:52:33,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.0081 | Train: 1.000 | Val: 0.619 | LR: 1.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:12:58<1:50:13,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.0078 | Train: 1.000 | Val: 0.610 | LR: 1.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:14:17<1:42:05,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.0083 | Train: 1.000 | Val: 0.635 | LR: 1.67e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:15:28<1:38:52,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.0078 | Train: 1.000 | Val: 0.628 | LR: 1.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:16:44<1:35:27,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.0080 | Train: 1.000 | Val: 0.639 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:18:02<1:30:56,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.0077 | Train: 1.000 | Val: 0.661 | LR: 1.29e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:19:18<1:23:25,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.0076 | Train: 1.000 | Val: 0.606 | LR: 1.17e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:20:36<1:17:06,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.0076 | Train: 1.000 | Val: 0.643 | LR: 1.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:21:49<1:16:42,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.0072 | Train: 1.000 | Val: 0.628 | LR: 9.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:23:06<1:12:14,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.0073 | Train: 1.000 | Val: 0.670 | LR: 8.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 3301/4000 [1:24:22<1:02:20,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.0078 | Train: 1.000 | Val: 0.658 | LR: 7.28e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:25:40<1:02:17,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.0075 | Train: 1.000 | Val: 0.573 | LR: 6.32e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 3401/4000 [1:26:54<54:26,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.0073 | Train: 1.000 | Val: 0.592 | LR: 5.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 3451/4000 [1:28:07<49:20,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.0082 | Train: 1.000 | Val: 0.640 | LR: 4.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:29:23<47:35,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.0075 | Train: 1.000 | Val: 0.637 | LR: 3.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:30:40<41:20,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.0074 | Train: 1.000 | Val: 0.629 | LR: 3.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:31:59<37:42,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.0079 | Train: 1.000 | Val: 0.661 | LR: 2.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:33:12<28:49,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.0078 | Train: 1.000 | Val: 0.673 | LR: 1.88e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3701/4000 [1:34:25<27:39,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 | Loss: 0.0072 | Train: 1.000 | Val: 0.637 | LR: 1.38e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3751/4000 [1:35:44<24:43,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3750 | Loss: 0.0071 | Train: 1.000 | Val: 0.646 | LR: 9.62e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3801/4000 [1:36:59<18:46,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 | Loss: 0.0073 | Train: 1.000 | Val: 0.658 | LR: 6.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3851/4000 [1:38:17<12:40,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3850 | Loss: 0.0071 | Train: 1.000 | Val: 0.615 | LR: 3.44e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3901/4000 [1:39:30<09:03,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 | Loss: 0.0080 | Train: 1.000 | Val: 0.626 | LR: 1.51e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3951/4000 [1:40:45<04:44,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3950 | Loss: 0.0078 | Train: 1.000 | Val: 0.585 | LR: 3.64e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [1:41:47<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 0.739\n",
      "============================================================\n",
      "\n",
      "Best model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Enhanced transforms with stronger augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.5 + dissim * 0.5\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last few layers instead of freezing everything\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with dropout and batch norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Temperature parameter for distance scaling\n",
    "        self.temperature = nn.Parameter(torch.tensor(10.0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return F.normalize(self.projection(features), dim=1)\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# Model and training setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, total_steps=4000, pct_start=0.3\n",
    ")\n",
    "\n",
    "# Training loop - 2-shot (1+1)\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "total_k_shot = k_shot_original + k_shot_enhanced\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 400\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting 1+1 shot training (2-shot total per class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Original shots per class: {k_shot_original}\")\n",
    "print(f\"  - Enhanced shots per class: {k_shot_enhanced}\")\n",
    "print(f\"  - Query samples per class: {q_query}\")\n",
    "print(f\"  - Total training episodes: 4000\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    # Forward pass\n",
    "    support_embeddings = model(support_images)\n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Compute prototypes (2 samples per class)\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    # Cosine similarity with temperature scaling\n",
    "    logits = torch.mm(query_embeddings, prototypes.t()) * model.temperature\n",
    "    loss = F.cross_entropy(logits, query_labels)\n",
    "    \n",
    "    # Add prototype regularization\n",
    "    proto_reg = 0.01 * torch.mean(torch.norm(prototypes, dim=1))\n",
    "    loss += proto_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            for _ in range(20):\n",
    "                # Create standard 1-shot validation episode\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    # 1 support sample\n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    # Query samples\n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_logits = torch.mm(val_query_embeddings, val_prototypes.t()) * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | Val: {val_acc_avg:.3f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f428727-9d26-4bfb-8b0a-7f20244f6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ENSEMBLE EVALUATION WITH TTA\n",
      "================================================================================\n",
      "ðŸŽ¯ Running Diverse Ensemble (n=5, k_shot=2)\n",
      "  Ensemble 1: 0.7413\n",
      "  Ensemble 2: 0.7405\n",
      "  Ensemble 3: 0.7409\n",
      "  Ensemble 4: 0.7412\n",
      "  Ensemble 5: 0.7405\n",
      "\n",
      "ðŸ“Š Enhanced Ensemble Results:\n",
      "  Final Accuracy: 0.7426\n",
      "  Precision: 0.7636\n",
      "  Recall: 0.7426\n",
      "  F1-Score: 0.7305\n",
      "  Std Dev: 0.0004\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Amphibolis      0.795        0.773        0.784        5375      \n",
      "Background      0.800        0.434        0.562        4706      \n",
      "Halophila       0.655        0.995        0.790        5669      \n",
      "Posidonia       0.818        0.716        0.763        5122      \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg       0.767        0.729        0.725        20872     \n",
      "Weighted Avg    0.764        0.743        0.730        20872     \n",
      "âœ… Confusion matrix saved to confusion_matrix_ensemble.png\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\n",
      "================================================================================\n",
      "ðŸŽ¯ Comprehensive Analysis for All Test Images...\n",
      "Processing 20872 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [06:35<00:00, 52.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Overall Accuracy: 0.7267\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Amphibolis      0.752        0.773        0.762        5375      \n",
      "Background      0.795        0.432        0.560        4706      \n",
      "Halophila       0.653        0.990        0.787        5669      \n",
      "Posidonia       0.802        0.658        0.723        5122      \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg       0.751        0.713        0.708        20872     \n",
      "Weighted Avg    0.747        0.727        0.714        20872     \n",
      "âœ… Confusion matrix saved to confusion_matrix_comprehensive.png\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FINAL SUMMARY\n",
      "================================================================================\n",
      "Enhanced Ensemble: 0.7426 (Â±0.0004)\n",
      "Comprehensive Analysis: 0.7267\n",
      "Total Test Images Processed: 20872\n",
      "Best Single Model: 0.7413\n",
      "Ensemble Weighted F1-Score: 0.7305\n",
      "\n",
      "âœ… Results saved to 'evaluation_results_metrics_only.json'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create idx_to_label mapping\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "def create_fixed_support_set(data, n_way, k_shot):\n",
    "    \"\"\"Create a fixed support set for evaluation\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            selected = available[:k_shot]\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def create_diverse_support_set(data, k_shot=2, diversity_factor=0):\n",
    "    \"\"\"Create diverse support sets for ensemble\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    random.seed(diversity_factor * 42)\n",
    "    \n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            start_idx = (diversity_factor * 2) % max(1, len(available) - k_shot)\n",
    "            selected = available[start_idx:start_idx + k_shot]\n",
    "            if len(selected) < k_shot:\n",
    "                selected.extend(random.sample(available, k_shot - len(selected)))\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3):\n",
    "    \"\"\"Test with Test Time Augmentation - returns only predictions for metrics\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transform_eval,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in test_data:\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                \n",
    "                tta_logits = []\n",
    "                for transform in tta_transforms[:n_tta]:\n",
    "                    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                    query_embedding = model(img_tensor)\n",
    "                    logits = torch.mm(query_embedding, prototypes.t()) * model.temperature\n",
    "                    tta_logits.append(logits)\n",
    "                \n",
    "                avg_logits = torch.stack(tta_logits).mean(0)\n",
    "                pred_idx = avg_logits.argmax().item()\n",
    "                \n",
    "                predictions.append({\n",
    "                    'predicted': pred_idx,\n",
    "                    'true': label_to_idx[sample['label']]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy = sum(1 for p in predictions if p['predicted'] == p['true']) / len(predictions) if predictions else 0.0\n",
    "    return accuracy, predictions\n",
    "\n",
    "def ensemble_with_diversity(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, n_ensembles=5, k_shot=2):\n",
    "    \"\"\"Enhanced ensemble with diversity and TTA - metrics only\"\"\"\n",
    "    print(f\"ðŸŽ¯ Running Diverse Ensemble (n={n_ensembles}, k_shot={k_shot})\")\n",
    "    \n",
    "    ensemble_predictions = []\n",
    "    ensemble_accuracies = []\n",
    "    \n",
    "    for i in range(n_ensembles):\n",
    "        support_set = create_diverse_support_set(train_data, k_shot=k_shot, diversity_factor=i)\n",
    "        \n",
    "        model.eval()\n",
    "        support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_embeddings = model(support_images)\n",
    "            \n",
    "            prototypes = []\n",
    "            for cls_idx in range(len(all_labels)):\n",
    "                cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "                if len(cls_indices) > 0:\n",
    "                    proto = support_embeddings[cls_indices].mean(0)\n",
    "                    prototypes.append(F.normalize(proto, dim=0))\n",
    "                else:\n",
    "                    prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "            prototypes = torch.stack(prototypes)\n",
    "            \n",
    "            accuracy, predictions = test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3)\n",
    "            \n",
    "        ensemble_accuracies.append(accuracy)\n",
    "        ensemble_predictions.append(predictions)\n",
    "        print(f\"  Ensemble {i+1}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Majority voting\n",
    "    final_y_true = []\n",
    "    final_y_pred = []\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        votes = [pred[i]['predicted'] for pred in ensemble_predictions if i < len(pred)]\n",
    "        \n",
    "        if not votes:\n",
    "            continue\n",
    "        \n",
    "        # Simple majority voting\n",
    "        vote_counts = {}\n",
    "        for vote in votes:\n",
    "            vote_counts[vote] = vote_counts.get(vote, 0) + 1\n",
    "        \n",
    "        final_pred = max(vote_counts.items(), key=lambda x: x[1])[0]\n",
    "        true_label = ensemble_predictions[0][i]['true']\n",
    "        \n",
    "        final_y_true.append(true_label)\n",
    "        final_y_pred.append(final_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_accuracy = accuracy_score(final_y_true, final_y_pred)\n",
    "    precision = precision_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Enhanced Ensemble Results:\")\n",
    "    print(f\"  Final Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(ensemble_accuracies):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': final_y_true,\n",
    "        'y_pred': final_y_pred,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'individual_accuracies': ensemble_accuracies,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'std_dev': np.std(ensemble_accuracies),\n",
    "        'mean_accuracy': np.mean(ensemble_accuracies),\n",
    "        'min_accuracy': np.min(ensemble_accuracies),\n",
    "        'max_accuracy': np.max(ensemble_accuracies)\n",
    "    }\n",
    "\n",
    "def comprehensive_analysis(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, k_shot=2):\n",
    "    \"\"\"Comprehensive analysis - metrics only, no predictions saved\"\"\"\n",
    "    print(\"ðŸŽ¯ Comprehensive Analysis for All Test Images...\")\n",
    "    \n",
    "    support_set = create_fixed_support_set(train_data, n_way=len(all_labels), k_shot=k_shot)\n",
    "    model.eval()\n",
    "    support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(support_images)\n",
    "        \n",
    "        prototypes = []\n",
    "        for cls_idx in range(len(all_labels)):\n",
    "            cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "            if len(cls_indices) > 0:\n",
    "                proto = support_embeddings[cls_indices].mean(0)\n",
    "                prototypes.append(F.normalize(proto, dim=0))\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        print(f\"Processing {len(test_data)} test images...\")\n",
    "        \n",
    "        for sample in tqdm(test_data, desc=\"Analyzing test images\"):\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                img_tensor = transform_eval(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                query_embedding = F.normalize(model(img_tensor).squeeze(), dim=0)\n",
    "                \n",
    "                similarities = F.cosine_similarity(query_embedding.unsqueeze(0), prototypes, dim=1)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                \n",
    "                y_true.append(label_to_idx[sample['label']])\n",
    "                y_pred.append(pred_idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"ðŸŽ¯ Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return y_true, y_pred, overall_accuracy\n",
    "\n",
    "def create_confusion_matrix_plot(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Create and save confusion matrix visualization\"\"\"\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Count'})\n",
    "        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Confusion matrix saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating confusion matrix: {e}\")\n",
    "\n",
    "def print_classification_report(y_true, y_pred, all_labels):\n",
    "    \"\"\"Print detailed classification report\"\"\"\n",
    "    print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    class_report = classification_report(y_true, y_pred, target_names=all_labels, \n",
    "                                        output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_name in all_labels:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"{class_name:<15} {metrics['precision']:<12.3f} {metrics['recall']:<12.3f} \"\n",
    "              f\"{metrics['f1-score']:<12.3f} {int(metrics['support']):<10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Averages\n",
    "    macro_avg = class_report['macro avg']\n",
    "    weighted_avg = class_report['weighted avg']\n",
    "    \n",
    "    print(f\"{'Macro Avg':<15} {macro_avg['precision']:<12.3f} {macro_avg['recall']:<12.3f} \"\n",
    "          f\"{macro_avg['f1-score']:<12.3f} {int(macro_avg['support']):<10}\")\n",
    "    print(f\"{'Weighted Avg':<15} {weighted_avg['precision']:<12.3f} {weighted_avg['recall']:<12.3f} \"\n",
    "          f\"{weighted_avg['f1-score']:<12.3f} {int(weighted_avg['support']):<10}\")\n",
    "    \n",
    "    return class_report\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== ENSEMBLE WITH DIVERSITY AND TTA =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ENSEMBLE EVALUATION WITH TTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_results = ensemble_with_diversity(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, n_ensembles=5, k_shot=2\n",
    ")\n",
    "\n",
    "# Print ensemble metrics\n",
    "ensemble_report = print_classification_report(ensemble_results['y_true'], \n",
    "                                             ensemble_results['y_pred'], all_labels)\n",
    "\n",
    "# Create ensemble confusion matrix\n",
    "create_confusion_matrix_plot(ensemble_results['y_true'], ensemble_results['y_pred'], \n",
    "                            all_labels, save_path='confusion_matrix_ensemble.png')\n",
    "\n",
    "# ===== COMPREHENSIVE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_true_comp, y_pred_comp, overall_accuracy = comprehensive_analysis(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, k_shot=2\n",
    ")\n",
    "\n",
    "# Print comprehensive metrics\n",
    "comp_report = print_classification_report(y_true_comp, y_pred_comp, all_labels)\n",
    "\n",
    "# Create comprehensive confusion matrix\n",
    "create_confusion_matrix_plot(y_true_comp, y_pred_comp, all_labels, \n",
    "                            save_path='confusion_matrix_comprehensive.png')\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Enhanced Ensemble: {ensemble_results['final_accuracy']:.4f} (Â±{ensemble_results['std_dev']:.4f})\")\n",
    "print(f\"Comprehensive Analysis: {overall_accuracy:.4f}\")\n",
    "print(f\"Total Test Images Processed: {len(test_data)}\")\n",
    "print(f\"Best Single Model: {max(ensemble_results['individual_accuracies']):.4f}\")\n",
    "print(f\"Ensemble Weighted F1-Score: {ensemble_results['f1']:.4f}\")\n",
    "\n",
    "# ===== SAVE RESULTS =====\n",
    "final_results = {\n",
    "    'evaluation_setup': {\n",
    "        'support_shots': 2,\n",
    "        'n_way': len(all_labels),\n",
    "        'test_samples': len(test_data),\n",
    "        'classes': all_labels,\n",
    "        'ensemble_size': 5\n",
    "    },\n",
    "    'enhanced_ensemble': {\n",
    "        'final_accuracy': ensemble_results['final_accuracy'],\n",
    "        'individual_accuracies': ensemble_results['individual_accuracies'],\n",
    "        'mean_accuracy': ensemble_results['mean_accuracy'],\n",
    "        'std_dev': ensemble_results['std_dev'],\n",
    "        'min_accuracy': ensemble_results['min_accuracy'],\n",
    "        'max_accuracy': ensemble_results['max_accuracy'],\n",
    "        'precision': ensemble_results['precision'],\n",
    "        'recall': ensemble_results['recall'],\n",
    "        'f1': ensemble_results['f1'],\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': ensemble_report[cls]['precision'],\n",
    "                'recall': ensemble_report[cls]['recall'],\n",
    "                'f1_score': ensemble_report[cls]['f1-score'],\n",
    "                'support': int(ensemble_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'comprehensive_analysis': {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': comp_report[cls]['precision'],\n",
    "                'recall': comp_report[cls]['recall'],\n",
    "                'f1_score': comp_report[cls]['f1-score'],\n",
    "                'support': int(comp_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'summary': {\n",
    "        'best_ensemble_accuracy': ensemble_results['final_accuracy'],\n",
    "        'comprehensive_accuracy': overall_accuracy,\n",
    "        'ensemble_std': ensemble_results['std_dev'],\n",
    "        'weighted_f1': ensemble_results['f1'],\n",
    "        'total_images_processed': len(test_data)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation_results_metrics_only.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to 'evaluation_results_metrics_only.json'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26f8b525-417f-4b3e-a871-146b0cfbb655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting 1+1 shot training (2-shot total per class)\n",
      "============================================================\n",
      "  - Original shots per class: 1\n",
      "  - Enhanced shots per class: 1\n",
      "  - Query samples per class: 15\n",
      "  - Total training episodes: 4000\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 1.1598 | Train: 0.450 | Val: 0.588 | LR: 2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:17<19:14:11, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:19<1:23:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.0105 | Train: 1.000 | Val: 0.680 | LR: 2.21e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:36<6:38:10,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:40<1:26:49,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.0097 | Train: 1.000 | Val: 0.720 | LR: 2.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [03:00<7:14:21,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [04:17<6:39:37,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.0096 | Train: 1.000 | Val: 0.713 | LR: 3.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [05:33<6:41:07,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.0096 | Train: 1.000 | Val: 0.686 | LR: 5.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [06:53<6:37:55,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.0096 | Train: 1.000 | Val: 0.713 | LR: 7.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [08:09<6:35:06,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.0095 | Train: 1.000 | Val: 0.670 | LR: 9.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [09:30<6:19:28,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.0093 | Train: 1.000 | Val: 0.660 | LR: 1.15e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [10:47<6:16:55,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.0089 | Train: 1.000 | Val: 0.690 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [12:06<6:18:09,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.0086 | Train: 1.000 | Val: 0.659 | LR: 1.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [13:25<5:43:48,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.0081 | Train: 1.000 | Val: 0.670 | LR: 1.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [14:45<6:01:56,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.0078 | Train: 1.000 | Val: 0.616 | LR: 2.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [16:01<5:25:33,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.0082 | Train: 1.000 | Val: 0.639 | LR: 2.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [17:16<5:35:39,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.0076 | Train: 1.000 | Val: 0.650 | LR: 2.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [18:34<5:29:24,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.0075 | Train: 1.000 | Val: 0.677 | LR: 3.23e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [19:51<5:37:21,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.0074 | Train: 1.000 | Val: 0.698 | LR: 3.53e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [21:12<5:21:57,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.0073 | Train: 1.000 | Val: 0.633 | LR: 3.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [22:26<4:59:13,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.0072 | Train: 1.000 | Val: 0.666 | LR: 4.07e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [23:40<5:05:45,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.0071 | Train: 1.000 | Val: 0.686 | LR: 4.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [24:56<4:39:54,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.0071 | Train: 1.000 | Val: 0.634 | LR: 4.51e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [26:14<5:04:48,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.0071 | Train: 1.000 | Val: 0.636 | LR: 4.68e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [27:34<4:48:42,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.0071 | Train: 1.000 | Val: 0.589 | LR: 4.82e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [28:46<4:35:14,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.0123 | Train: 1.000 | Val: 0.523 | LR: 4.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [30:04<4:42:55,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.0096 | Train: 1.000 | Val: 0.489 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [31:21<4:28:45,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.0100 | Train: 1.000 | Val: 0.624 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [32:37<4:33:10,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.0099 | Train: 1.000 | Val: 0.563 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [33:55<4:15:50,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.0101 | Train: 1.000 | Val: 0.638 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [35:06<4:05:57,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.0099 | Train: 1.000 | Val: 0.629 | LR: 4.96e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [36:21<4:09:47,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.0096 | Train: 1.000 | Val: 0.632 | LR: 4.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [37:36<3:53:40,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.0092 | Train: 1.000 | Val: 0.598 | LR: 4.90e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [38:53<4:00:53,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.0092 | Train: 1.000 | Val: 0.569 | LR: 4.86e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [40:07<3:38:52,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.0093 | Train: 1.000 | Val: 0.570 | LR: 4.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [41:19<3:51:14,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.0093 | Train: 1.000 | Val: 0.580 | LR: 4.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [42:34<3:46:00,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.0087 | Train: 1.000 | Val: 0.562 | LR: 4.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [43:49<3:38:51,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.0089 | Train: 1.000 | Val: 0.590 | LR: 4.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [45:09<3:40:29,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.0084 | Train: 1.000 | Val: 0.591 | LR: 4.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [46:22<3:20:07,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.0081 | Train: 1.000 | Val: 0.594 | LR: 4.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [47:34<3:26:41,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.0083 | Train: 1.000 | Val: 0.583 | LR: 4.36e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [48:52<3:25:08,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.0086 | Train: 1.000 | Val: 0.582 | LR: 4.26e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [50:10<3:26:55,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.0080 | Train: 1.000 | Val: 0.612 | LR: 4.16e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 2001/4000 [51:29<3:09:48,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.0084 | Train: 1.000 | Val: 0.583 | LR: 4.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 2051/4000 [52:41<2:56:00,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.0079 | Train: 1.000 | Val: 0.590 | LR: 3.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 2101/4000 [53:55<2:58:42,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.0077 | Train: 1.000 | Val: 0.573 | LR: 3.83e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 2151/4000 [55:12<2:46:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.0082 | Train: 1.000 | Val: 0.586 | LR: 3.70e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 2201/4000 [56:29<2:50:09,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.0082 | Train: 1.000 | Val: 0.580 | LR: 3.58e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2251/4000 [57:46<2:40:08,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.0080 | Train: 1.000 | Val: 0.523 | LR: 3.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 2301/4000 [58:58<2:37:10,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.0080 | Train: 1.000 | Val: 0.648 | LR: 3.32e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2351/4000 [1:00:15<2:46:48,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.0082 | Train: 1.000 | Val: 0.608 | LR: 3.19e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2401/4000 [1:01:31<2:26:07,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.0080 | Train: 1.000 | Val: 0.568 | LR: 3.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:02:49<2:29:11,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.0078 | Train: 1.000 | Val: 0.578 | LR: 2.91e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:04:04<2:19:22,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.0079 | Train: 1.000 | Val: 0.612 | LR: 2.77e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:05:17<2:16:39,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.0078 | Train: 1.000 | Val: 0.603 | LR: 2.63e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:06:32<2:12:19,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.0078 | Train: 1.000 | Val: 0.623 | LR: 2.49e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:07:47<2:01:27,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.0081 | Train: 1.000 | Val: 0.615 | LR: 2.35e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:09:07<2:09:38,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.0079 | Train: 1.000 | Val: 0.604 | LR: 2.21e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:10:21<1:56:46,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.0078 | Train: 1.000 | Val: 0.628 | LR: 2.08e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:11:33<1:54:14,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.0076 | Train: 1.000 | Val: 0.605 | LR: 1.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:12:49<1:50:02,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.0078 | Train: 1.000 | Val: 0.631 | LR: 1.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:14:03<1:38:37,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.0076 | Train: 1.000 | Val: 0.532 | LR: 1.67e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:15:22<1:38:00,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.0077 | Train: 1.000 | Val: 0.645 | LR: 1.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:16:35<1:33:23,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.0077 | Train: 1.000 | Val: 0.638 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:17:48<1:27:02,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.0076 | Train: 1.000 | Val: 0.623 | LR: 1.29e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:19:05<1:25:02,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.0075 | Train: 1.000 | Val: 0.615 | LR: 1.17e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:20:22<1:21:28,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.0077 | Train: 1.000 | Val: 0.603 | LR: 1.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:21:40<1:14:53,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.0077 | Train: 1.000 | Val: 0.613 | LR: 9.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:22:52<1:09:11,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.0079 | Train: 1.000 | Val: 0.627 | LR: 8.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 3301/4000 [1:24:05<1:07:07,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.0075 | Train: 1.000 | Val: 0.618 | LR: 7.28e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:25:22<1:01:58,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.0077 | Train: 1.000 | Val: 0.601 | LR: 6.32e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 3401/4000 [1:26:38<57:42,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.0077 | Train: 1.000 | Val: 0.606 | LR: 5.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 3451/4000 [1:27:52<48:26,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.0076 | Train: 1.000 | Val: 0.606 | LR: 4.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:29:04<47:46,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.0074 | Train: 1.000 | Val: 0.591 | LR: 3.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:30:19<42:05,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.0076 | Train: 1.000 | Val: 0.572 | LR: 3.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:31:34<36:53,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.0079 | Train: 1.000 | Val: 0.624 | LR: 2.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:32:53<35:01,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.0075 | Train: 1.000 | Val: 0.597 | LR: 1.88e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3701/4000 [1:34:05<25:37,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 | Loss: 0.0079 | Train: 1.000 | Val: 0.659 | LR: 1.38e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3751/4000 [1:35:18<24:27,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3750 | Loss: 0.0079 | Train: 1.000 | Val: 0.630 | LR: 9.62e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3801/4000 [1:36:33<18:57,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 | Loss: 0.0075 | Train: 1.000 | Val: 0.635 | LR: 6.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3851/4000 [1:37:48<13:52,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3850 | Loss: 0.0074 | Train: 1.000 | Val: 0.653 | LR: 3.44e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3901/4000 [1:39:06<09:35,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 | Loss: 0.0076 | Train: 1.000 | Val: 0.615 | LR: 1.51e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3951/4000 [1:40:20<04:29,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3950 | Loss: 0.0075 | Train: 1.000 | Val: 0.608 | LR: 3.64e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [1:41:17<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 0.720\n",
      "============================================================\n",
      "\n",
      "Best model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Enhanced transforms with stronger augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.5 + dissim * 0.5\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last few layers instead of freezing everything\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with dropout and batch norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Temperature parameter for distance scaling\n",
    "        self.temperature = nn.Parameter(torch.tensor(10.0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return F.normalize(self.projection(features), dim=1)\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# Model and training setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, total_steps=4000, pct_start=0.3\n",
    ")\n",
    "\n",
    "# Training loop - 2-shot (1+1)\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "total_k_shot = k_shot_original + k_shot_enhanced\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 400\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting 1+1 shot training (2-shot total per class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Original shots per class: {k_shot_original}\")\n",
    "print(f\"  - Enhanced shots per class: {k_shot_enhanced}\")\n",
    "print(f\"  - Query samples per class: {q_query}\")\n",
    "print(f\"  - Total training episodes: 4000\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    # Forward pass\n",
    "    support_embeddings = model(support_images)\n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Compute prototypes (2 samples per class)\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    # Cosine similarity with temperature scaling\n",
    "    logits = torch.mm(query_embeddings, prototypes.t()) * model.temperature\n",
    "    loss = F.cross_entropy(logits, query_labels)\n",
    "    \n",
    "    # Add prototype regularization\n",
    "    proto_reg = 0.01 * torch.mean(torch.norm(prototypes, dim=1))\n",
    "    loss += proto_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            for _ in range(20):\n",
    "                # Create standard 1-shot validation episode\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    # 1 support sample\n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    # Query samples\n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_logits = torch.mm(val_query_embeddings, val_prototypes.t()) * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | Val: {val_acc_avg:.3f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb5f9e-ce86-4ec2-b0d2-15cf1b78a952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b243c07-5dc1-42e2-9112-5b7fd8921452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ENSEMBLE EVALUATION WITH TTA\n",
      "================================================================================\n",
      "ðŸŽ¯ Running Diverse Ensemble (n=5, k_shot=2)\n",
      "  Ensemble 1: 0.7412\n",
      "  Ensemble 2: 0.7419\n",
      "  Ensemble 3: 0.7425\n",
      "  Ensemble 4: 0.7420\n",
      "  Ensemble 5: 0.7428\n",
      "\n",
      "ðŸ“Š Enhanced Ensemble Results:\n",
      "  Final Accuracy: 0.7430\n",
      "  Precision: 0.7626\n",
      "  Recall: 0.7430\n",
      "  F1-Score: 0.7310\n",
      "  Std Dev: 0.0005\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Amphibolis      0.828        0.736        0.779        5375      \n",
      "Background      0.779        0.431        0.555        4706      \n",
      "Halophila       0.653        0.995        0.788        5669      \n",
      "Posidonia       0.800        0.758        0.778        5122      \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg       0.765        0.730        0.725        20872     \n",
      "Weighted Avg    0.763        0.743        0.731        20872     \n",
      "âœ… Confusion matrix saved to confusion_matrix_ensemble.png\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\n",
      "================================================================================\n",
      "ðŸŽ¯ Comprehensive Analysis for All Test Images...\n",
      "Processing 20872 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20872/20872 [06:27<00:00, 53.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Overall Accuracy: 0.7371\n",
      "\n",
      "ðŸ“Š Per-Class Performance:\n",
      "Class           Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Amphibolis      0.796        0.749        0.771        5375      \n",
      "Background      0.782        0.438        0.562        4706      \n",
      "Halophila       0.655        0.991        0.788        5669      \n",
      "Posidonia       0.801        0.719        0.758        5122      \n",
      "--------------------------------------------------------------------------------\n",
      "Macro Avg       0.758        0.724        0.720        20872     \n",
      "Weighted Avg    0.755        0.737        0.725        20872     \n",
      "âœ… Confusion matrix saved to confusion_matrix_comprehensive.png\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FINAL SUMMARY\n",
      "================================================================================\n",
      "Enhanced Ensemble: 0.7430 (Â±0.0005)\n",
      "Comprehensive Analysis: 0.7371\n",
      "Total Test Images Processed: 20872\n",
      "Best Single Model: 0.7428\n",
      "Ensemble Weighted F1-Score: 0.7310\n",
      "\n",
      "âœ… Results saved to 'evaluation_results_metrics_only.json'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create idx_to_label mapping\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "def create_fixed_support_set(data, n_way, k_shot):\n",
    "    \"\"\"Create a fixed support set for evaluation\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            selected = available[:k_shot]\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def create_diverse_support_set(data, k_shot=2, diversity_factor=0):\n",
    "    \"\"\"Create diverse support sets for ensemble\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    random.seed(diversity_factor * 42)\n",
    "    \n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            start_idx = (diversity_factor * 2) % max(1, len(available) - k_shot)\n",
    "            selected = available[start_idx:start_idx + k_shot]\n",
    "            if len(selected) < k_shot:\n",
    "                selected.extend(random.sample(available, k_shot - len(selected)))\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3):\n",
    "    \"\"\"Test with Test Time Augmentation - returns only predictions for metrics\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transform_eval,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in test_data:\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                \n",
    "                tta_logits = []\n",
    "                for transform in tta_transforms[:n_tta]:\n",
    "                    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                    query_embedding = model(img_tensor)\n",
    "                    logits = torch.mm(query_embedding, prototypes.t()) * model.temperature\n",
    "                    tta_logits.append(logits)\n",
    "                \n",
    "                avg_logits = torch.stack(tta_logits).mean(0)\n",
    "                pred_idx = avg_logits.argmax().item()\n",
    "                \n",
    "                predictions.append({\n",
    "                    'predicted': pred_idx,\n",
    "                    'true': label_to_idx[sample['label']]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy = sum(1 for p in predictions if p['predicted'] == p['true']) / len(predictions) if predictions else 0.0\n",
    "    return accuracy, predictions\n",
    "\n",
    "def ensemble_with_diversity(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, n_ensembles=5, k_shot=2):\n",
    "    \"\"\"Enhanced ensemble with diversity and TTA - metrics only\"\"\"\n",
    "    print(f\"ðŸŽ¯ Running Diverse Ensemble (n={n_ensembles}, k_shot={k_shot})\")\n",
    "    \n",
    "    ensemble_predictions = []\n",
    "    ensemble_accuracies = []\n",
    "    \n",
    "    for i in range(n_ensembles):\n",
    "        support_set = create_diverse_support_set(train_data, k_shot=k_shot, diversity_factor=i)\n",
    "        \n",
    "        model.eval()\n",
    "        support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_embeddings = model(support_images)\n",
    "            \n",
    "            prototypes = []\n",
    "            for cls_idx in range(len(all_labels)):\n",
    "                cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "                if len(cls_indices) > 0:\n",
    "                    proto = support_embeddings[cls_indices].mean(0)\n",
    "                    prototypes.append(F.normalize(proto, dim=0))\n",
    "                else:\n",
    "                    prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "            prototypes = torch.stack(prototypes)\n",
    "            \n",
    "            accuracy, predictions = test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3)\n",
    "            \n",
    "        ensemble_accuracies.append(accuracy)\n",
    "        ensemble_predictions.append(predictions)\n",
    "        print(f\"  Ensemble {i+1}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Majority voting\n",
    "    final_y_true = []\n",
    "    final_y_pred = []\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        votes = [pred[i]['predicted'] for pred in ensemble_predictions if i < len(pred)]\n",
    "        \n",
    "        if not votes:\n",
    "            continue\n",
    "        \n",
    "        # Simple majority voting\n",
    "        vote_counts = {}\n",
    "        for vote in votes:\n",
    "            vote_counts[vote] = vote_counts.get(vote, 0) + 1\n",
    "        \n",
    "        final_pred = max(vote_counts.items(), key=lambda x: x[1])[0]\n",
    "        true_label = ensemble_predictions[0][i]['true']\n",
    "        \n",
    "        final_y_true.append(true_label)\n",
    "        final_y_pred.append(final_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_accuracy = accuracy_score(final_y_true, final_y_pred)\n",
    "    precision = precision_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Enhanced Ensemble Results:\")\n",
    "    print(f\"  Final Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(ensemble_accuracies):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': final_y_true,\n",
    "        'y_pred': final_y_pred,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'individual_accuracies': ensemble_accuracies,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'std_dev': np.std(ensemble_accuracies),\n",
    "        'mean_accuracy': np.mean(ensemble_accuracies),\n",
    "        'min_accuracy': np.min(ensemble_accuracies),\n",
    "        'max_accuracy': np.max(ensemble_accuracies)\n",
    "    }\n",
    "\n",
    "def comprehensive_analysis(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, k_shot=2):\n",
    "    \"\"\"Comprehensive analysis - metrics only, no predictions saved\"\"\"\n",
    "    print(\"ðŸŽ¯ Comprehensive Analysis for All Test Images...\")\n",
    "    \n",
    "    support_set = create_fixed_support_set(train_data, n_way=len(all_labels), k_shot=k_shot)\n",
    "    model.eval()\n",
    "    support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(support_images)\n",
    "        \n",
    "        prototypes = []\n",
    "        for cls_idx in range(len(all_labels)):\n",
    "            cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "            if len(cls_indices) > 0:\n",
    "                proto = support_embeddings[cls_indices].mean(0)\n",
    "                prototypes.append(F.normalize(proto, dim=0))\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        print(f\"Processing {len(test_data)} test images...\")\n",
    "        \n",
    "        for sample in tqdm(test_data, desc=\"Analyzing test images\"):\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                img_tensor = transform_eval(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                query_embedding = F.normalize(model(img_tensor).squeeze(), dim=0)\n",
    "                \n",
    "                similarities = F.cosine_similarity(query_embedding.unsqueeze(0), prototypes, dim=1)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                \n",
    "                y_true.append(label_to_idx[sample['label']])\n",
    "                y_pred.append(pred_idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"ðŸŽ¯ Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return y_true, y_pred, overall_accuracy\n",
    "\n",
    "def create_confusion_matrix_plot(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Create and save confusion matrix visualization\"\"\"\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Count'})\n",
    "        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Confusion matrix saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating confusion matrix: {e}\")\n",
    "\n",
    "def print_classification_report(y_true, y_pred, all_labels):\n",
    "    \"\"\"Print detailed classification report\"\"\"\n",
    "    print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    class_report = classification_report(y_true, y_pred, target_names=all_labels, \n",
    "                                        output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_name in all_labels:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"{class_name:<15} {metrics['precision']:<12.3f} {metrics['recall']:<12.3f} \"\n",
    "              f\"{metrics['f1-score']:<12.3f} {int(metrics['support']):<10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Averages\n",
    "    macro_avg = class_report['macro avg']\n",
    "    weighted_avg = class_report['weighted avg']\n",
    "    \n",
    "    print(f\"{'Macro Avg':<15} {macro_avg['precision']:<12.3f} {macro_avg['recall']:<12.3f} \"\n",
    "          f\"{macro_avg['f1-score']:<12.3f} {int(macro_avg['support']):<10}\")\n",
    "    print(f\"{'Weighted Avg':<15} {weighted_avg['precision']:<12.3f} {weighted_avg['recall']:<12.3f} \"\n",
    "          f\"{weighted_avg['f1-score']:<12.3f} {int(weighted_avg['support']):<10}\")\n",
    "    \n",
    "    return class_report\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== ENSEMBLE WITH DIVERSITY AND TTA =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ENSEMBLE EVALUATION WITH TTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_results = ensemble_with_diversity(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, n_ensembles=5, k_shot=2\n",
    ")\n",
    "\n",
    "# Print ensemble metrics\n",
    "ensemble_report = print_classification_report(ensemble_results['y_true'], \n",
    "                                             ensemble_results['y_pred'], all_labels)\n",
    "\n",
    "# Create ensemble confusion matrix\n",
    "create_confusion_matrix_plot(ensemble_results['y_true'], ensemble_results['y_pred'], \n",
    "                            all_labels, save_path='confusion_matrix_ensemble.png')\n",
    "\n",
    "# ===== COMPREHENSIVE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_true_comp, y_pred_comp, overall_accuracy = comprehensive_analysis(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, k_shot=2\n",
    ")\n",
    "\n",
    "# Print comprehensive metrics\n",
    "comp_report = print_classification_report(y_true_comp, y_pred_comp, all_labels)\n",
    "\n",
    "# Create comprehensive confusion matrix\n",
    "create_confusion_matrix_plot(y_true_comp, y_pred_comp, all_labels, \n",
    "                            save_path='confusion_matrix_comprehensive.png')\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Enhanced Ensemble: {ensemble_results['final_accuracy']:.4f} (Â±{ensemble_results['std_dev']:.4f})\")\n",
    "print(f\"Comprehensive Analysis: {overall_accuracy:.4f}\")\n",
    "print(f\"Total Test Images Processed: {len(test_data)}\")\n",
    "print(f\"Best Single Model: {max(ensemble_results['individual_accuracies']):.4f}\")\n",
    "print(f\"Ensemble Weighted F1-Score: {ensemble_results['f1']:.4f}\")\n",
    "\n",
    "# ===== SAVE RESULTS =====\n",
    "final_results = {\n",
    "    'evaluation_setup': {\n",
    "        'support_shots': 2,\n",
    "        'n_way': len(all_labels),\n",
    "        'test_samples': len(test_data),\n",
    "        'classes': all_labels,\n",
    "        'ensemble_size': 5\n",
    "    },\n",
    "    'enhanced_ensemble': {\n",
    "        'final_accuracy': ensemble_results['final_accuracy'],\n",
    "        'individual_accuracies': ensemble_results['individual_accuracies'],\n",
    "        'mean_accuracy': ensemble_results['mean_accuracy'],\n",
    "        'std_dev': ensemble_results['std_dev'],\n",
    "        'min_accuracy': ensemble_results['min_accuracy'],\n",
    "        'max_accuracy': ensemble_results['max_accuracy'],\n",
    "        'precision': ensemble_results['precision'],\n",
    "        'recall': ensemble_results['recall'],\n",
    "        'f1': ensemble_results['f1'],\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': ensemble_report[cls]['precision'],\n",
    "                'recall': ensemble_report[cls]['recall'],\n",
    "                'f1_score': ensemble_report[cls]['f1-score'],\n",
    "                'support': int(ensemble_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'comprehensive_analysis': {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': comp_report[cls]['precision'],\n",
    "                'recall': comp_report[cls]['recall'],\n",
    "                'f1_score': comp_report[cls]['f1-score'],\n",
    "                'support': int(comp_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'summary': {\n",
    "        'best_ensemble_accuracy': ensemble_results['final_accuracy'],\n",
    "        'comprehensive_accuracy': overall_accuracy,\n",
    "        'ensemble_std': ensemble_results['std_dev'],\n",
    "        'weighted_f1': ensemble_results['f1'],\n",
    "        'total_images_processed': len(test_data)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation_results_metrics_only.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to 'evaluation_results_metrics_only.json'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7faed72f-6d80-4326-8568-100702502d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting 1+1 shot training (2-shot total per class)\n",
      "============================================================\n",
      "  - Original shots per class: 1\n",
      "  - Enhanced shots per class: 1\n",
      "  - Query samples per class: 15\n",
      "  - Total training episodes: 4000\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 1.1230 | Train: 0.500 | Val: 0.575 | LR: 2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:16<18:42:13, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:18<1:17:42,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.0105 | Train: 1.000 | Val: 0.653 | LR: 2.21e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:34<6:20:18,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:32<1:20:29,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.0097 | Train: 1.000 | Val: 0.685 | LR: 2.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [02:50<6:39:48,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 150/4000 [03:52<1:22:48,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.0096 | Train: 1.000 | Val: 0.698 | LR: 3.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [04:10<6:31:02,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [05:04<3:00:57,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.0096 | Train: 1.000 | Val: 0.652 | LR: 5.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [05:54<2:58:43,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.0094 | Train: 1.000 | Val: 0.673 | LR: 7.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [06:44<3:02:05,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.0093 | Train: 1.000 | Val: 0.673 | LR: 9.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [07:45<5:34:21,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.0090 | Train: 1.000 | Val: 0.683 | LR: 1.15e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [08:59<5:35:08,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.0093 | Train: 1.000 | Val: 0.676 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [10:15<5:42:36,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.0110 | Train: 1.000 | Val: 0.654 | LR: 1.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [11:32<5:28:08,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.0091 | Train: 1.000 | Val: 0.634 | LR: 1.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [12:52<5:32:17,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.0095 | Train: 1.000 | Val: 0.625 | LR: 2.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 600/4000 [13:49<1:04:26,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.0105 | Train: 1.000 | Val: 0.703 | LR: 2.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [14:06<5:25:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [15:23<5:13:39,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.0098 | Train: 1.000 | Val: 0.647 | LR: 2.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [16:40<5:06:03,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.0093 | Train: 1.000 | Val: 0.658 | LR: 3.23e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [17:57<5:22:49,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.0086 | Train: 1.000 | Val: 0.643 | LR: 3.53e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [19:15<4:47:23,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.0083 | Train: 1.000 | Val: 0.601 | LR: 3.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [20:27<4:49:34,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.0082 | Train: 1.000 | Val: 0.641 | LR: 4.07e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [21:43<5:00:00,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.0079 | Train: 1.000 | Val: 0.659 | LR: 4.30e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [22:59<4:49:56,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.0100 | Train: 1.000 | Val: 0.565 | LR: 4.51e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [24:17<4:45:17,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.0084 | Train: 1.000 | Val: 0.516 | LR: 4.68e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [25:32<4:32:30,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.0082 | Train: 1.000 | Val: 0.555 | LR: 4.82e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [26:44<4:40:22,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.0087 | Train: 1.000 | Val: 0.633 | LR: 4.92e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [27:58<4:38:33,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.0079 | Train: 1.000 | Val: 0.615 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [29:13<4:14:50,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.0078 | Train: 1.000 | Val: 0.633 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [30:32<4:26:24,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.0083 | Train: 1.000 | Val: 0.537 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [31:46<4:08:36,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.0096 | Train: 1.000 | Val: 0.590 | LR: 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [32:58<4:10:22,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.0112 | Train: 1.000 | Val: 0.453 | LR: 4.96e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [34:15<4:17:33,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.0098 | Train: 1.000 | Val: 0.633 | LR: 4.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [35:30<4:00:54,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.0098 | Train: 1.000 | Val: 0.623 | LR: 4.90e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [36:48<3:50:05,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.0094 | Train: 1.000 | Val: 0.588 | LR: 4.86e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [38:00<3:44:32,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.0094 | Train: 1.000 | Val: 0.590 | LR: 4.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [39:11<3:27:59,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.0096 | Train: 1.000 | Val: 0.624 | LR: 4.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [40:30<3:44:53,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.0090 | Train: 1.000 | Val: 0.590 | LR: 4.69e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [41:45<3:28:58,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.0090 | Train: 1.000 | Val: 0.610 | LR: 4.61e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [43:03<3:30:15,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.0087 | Train: 1.000 | Val: 0.644 | LR: 4.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [44:16<3:28:51,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.0085 | Train: 1.000 | Val: 0.647 | LR: 4.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [45:29<3:26:22,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.0087 | Train: 1.000 | Val: 0.595 | LR: 4.36e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [46:45<3:15:49,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.0083 | Train: 1.000 | Val: 0.648 | LR: 4.26e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [48:02<3:14:11,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.0079 | Train: 1.000 | Val: 0.638 | LR: 4.16e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 2001/4000 [49:20<3:07:19,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.0081 | Train: 1.000 | Val: 0.594 | LR: 4.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 2051/4000 [50:32<3:08:30,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.0081 | Train: 1.000 | Val: 0.583 | LR: 3.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 2101/4000 [51:48<3:05:27,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.0079 | Train: 1.000 | Val: 0.556 | LR: 3.83e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 2151/4000 [53:03<2:52:33,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.0078 | Train: 1.000 | Val: 0.621 | LR: 3.70e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 2201/4000 [54:21<2:51:28,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.0079 | Train: 1.000 | Val: 0.627 | LR: 3.58e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2251/4000 [55:36<2:41:38,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.0076 | Train: 1.000 | Val: 0.588 | LR: 3.45e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 2301/4000 [56:49<2:37:02,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.0085 | Train: 1.000 | Val: 0.648 | LR: 3.32e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2351/4000 [58:05<2:39:05,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.0075 | Train: 1.000 | Val: 0.622 | LR: 3.19e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2401/4000 [59:20<2:23:53,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.0076 | Train: 1.000 | Val: 0.558 | LR: 3.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:00:39<2:26:34,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.0075 | Train: 1.000 | Val: 0.549 | LR: 2.91e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:01:51<2:16:39,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.0077 | Train: 1.000 | Val: 0.615 | LR: 2.77e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:03:05<2:19:59,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.0075 | Train: 1.000 | Val: 0.584 | LR: 2.63e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:04:20<1:59:11,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.0074 | Train: 1.000 | Val: 0.608 | LR: 2.49e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:05:15<1:26:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.0074 | Train: 1.000 | Val: 0.634 | LR: 2.35e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:06:34<2:03:30,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.0075 | Train: 1.000 | Val: 0.576 | LR: 2.21e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:07:47<2:00:59,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.0072 | Train: 1.000 | Val: 0.610 | LR: 2.08e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:08:59<1:54:16,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.0074 | Train: 1.000 | Val: 0.624 | LR: 1.94e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:10:19<1:59:20,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.0075 | Train: 1.000 | Val: 0.600 | LR: 1.80e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:11:34<1:47:17,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.0076 | Train: 1.000 | Val: 0.518 | LR: 1.67e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:12:54<1:41:56,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.0072 | Train: 1.000 | Val: 0.625 | LR: 1.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:14:08<1:39:08,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.0075 | Train: 1.000 | Val: 0.618 | LR: 1.41e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:15:22<1:30:08,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.0072 | Train: 1.000 | Val: 0.605 | LR: 1.29e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:16:40<1:27:09,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.0076 | Train: 1.000 | Val: 0.594 | LR: 1.17e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:17:56<1:22:52,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.0071 | Train: 1.000 | Val: 0.568 | LR: 1.05e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:19:15<1:17:15,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.0070 | Train: 1.000 | Val: 0.571 | LR: 9.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:20:27<1:11:16,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.0073 | Train: 1.000 | Val: 0.599 | LR: 8.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 3301/4000 [1:21:41<1:06:22,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.0073 | Train: 1.000 | Val: 0.588 | LR: 7.28e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:22:58<1:01:30,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.0074 | Train: 1.000 | Val: 0.569 | LR: 6.32e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 3401/4000 [1:24:15<57:57,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.0072 | Train: 1.000 | Val: 0.630 | LR: 5.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 3451/4000 [1:25:32<50:33,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.0071 | Train: 1.000 | Val: 0.615 | LR: 4.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:26:45<49:09,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.0070 | Train: 1.000 | Val: 0.577 | LR: 3.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:28:00<43:25,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.0070 | Train: 1.000 | Val: 0.539 | LR: 3.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:29:16<36:50,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.0073 | Train: 1.000 | Val: 0.590 | LR: 2.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:30:34<32:50,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.0073 | Train: 1.000 | Val: 0.579 | LR: 1.88e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3701/4000 [1:31:46<27:03,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 | Loss: 0.0071 | Train: 1.000 | Val: 0.637 | LR: 1.38e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3751/4000 [1:32:59<23:54,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3750 | Loss: 0.0070 | Train: 1.000 | Val: 0.648 | LR: 9.62e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3801/4000 [1:34:14<18:46,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 | Loss: 0.0070 | Train: 1.000 | Val: 0.601 | LR: 6.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3851/4000 [1:35:28<13:27,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3850 | Loss: 0.0070 | Train: 1.000 | Val: 0.639 | LR: 3.44e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3901/4000 [1:36:46<09:24,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 | Loss: 0.0070 | Train: 1.000 | Val: 0.603 | LR: 1.51e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3951/4000 [1:37:58<04:27,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3950 | Loss: 0.0072 | Train: 1.000 | Val: 0.588 | LR: 3.64e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [1:38:56<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 0.703\n",
      "============================================================\n",
      "\n",
      "Best model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Enhanced transforms with stronger augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.4 + dissim * 0.6\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last few layers instead of freezing everything\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with dropout and batch norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Temperature parameter for distance scaling\n",
    "        self.temperature = nn.Parameter(torch.tensor(10.0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return F.normalize(self.projection(features), dim=1)\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# Model and training setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, total_steps=4000, pct_start=0.3\n",
    ")\n",
    "\n",
    "# Training loop - 2-shot (1+1)\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "total_k_shot = k_shot_original + k_shot_enhanced\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 400\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting 1+1 shot training (2-shot total per class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Original shots per class: {k_shot_original}\")\n",
    "print(f\"  - Enhanced shots per class: {k_shot_enhanced}\")\n",
    "print(f\"  - Query samples per class: {q_query}\")\n",
    "print(f\"  - Total training episodes: 4000\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    # Forward pass\n",
    "    support_embeddings = model(support_images)\n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Compute prototypes (2 samples per class)\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    # Cosine similarity with temperature scaling\n",
    "    logits = torch.mm(query_embeddings, prototypes.t()) * model.temperature\n",
    "    loss = F.cross_entropy(logits, query_labels)\n",
    "    \n",
    "    # Add prototype regularization\n",
    "    proto_reg = 0.01 * torch.mean(torch.norm(prototypes, dim=1))\n",
    "    loss += proto_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            for _ in range(20):\n",
    "                # Create standard 1-shot validation episode\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    # 1 support sample\n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    # Query samples\n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_logits = torch.mm(val_query_embeddings, val_prototypes.t()) * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | Val: {val_acc_avg:.3f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c01f6-c05f-4e49-b2c5-52b05feae899",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create idx_to_label mapping\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "def create_fixed_support_set(data, n_way, k_shot):\n",
    "    \"\"\"Create a fixed support set for evaluation\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            selected = available[:k_shot]\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def create_diverse_support_set(data, k_shot=2, diversity_factor=0):\n",
    "    \"\"\"Create diverse support sets for ensemble\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    random.seed(diversity_factor * 42)\n",
    "    \n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        available = class_samples[cls]\n",
    "        if len(available) >= k_shot:\n",
    "            start_idx = (diversity_factor * 2) % max(1, len(available) - k_shot)\n",
    "            selected = available[start_idx:start_idx + k_shot]\n",
    "            if len(selected) < k_shot:\n",
    "                selected.extend(random.sample(available, k_shot - len(selected)))\n",
    "        else:\n",
    "            selected = available * (k_shot // len(available) + 1)\n",
    "            selected = selected[:k_shot]\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3):\n",
    "    \"\"\"Test with Test Time Augmentation - returns only predictions for metrics\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transform_eval,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in test_data:\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                \n",
    "                tta_logits = []\n",
    "                for transform in tta_transforms[:n_tta]:\n",
    "                    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                    query_embedding = model(img_tensor)\n",
    "                    logits = torch.mm(query_embedding, prototypes.t()) * model.temperature\n",
    "                    tta_logits.append(logits)\n",
    "                \n",
    "                avg_logits = torch.stack(tta_logits).mean(0)\n",
    "                pred_idx = avg_logits.argmax().item()\n",
    "                \n",
    "                predictions.append({\n",
    "                    'predicted': pred_idx,\n",
    "                    'true': label_to_idx[sample['label']]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy = sum(1 for p in predictions if p['predicted'] == p['true']) / len(predictions) if predictions else 0.0\n",
    "    return accuracy, predictions\n",
    "\n",
    "def ensemble_with_diversity(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, n_ensembles=5, k_shot=2):\n",
    "    \"\"\"Enhanced ensemble with diversity and TTA - metrics only\"\"\"\n",
    "    print(f\"ðŸŽ¯ Running Diverse Ensemble (n={n_ensembles}, k_shot={k_shot})\")\n",
    "    \n",
    "    ensemble_predictions = []\n",
    "    ensemble_accuracies = []\n",
    "    \n",
    "    for i in range(n_ensembles):\n",
    "        support_set = create_diverse_support_set(train_data, k_shot=k_shot, diversity_factor=i)\n",
    "        \n",
    "        model.eval()\n",
    "        support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_embeddings = model(support_images)\n",
    "            \n",
    "            prototypes = []\n",
    "            for cls_idx in range(len(all_labels)):\n",
    "                cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "                if len(cls_indices) > 0:\n",
    "                    proto = support_embeddings[cls_indices].mean(0)\n",
    "                    prototypes.append(F.normalize(proto, dim=0))\n",
    "                else:\n",
    "                    prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "            prototypes = torch.stack(prototypes)\n",
    "            \n",
    "            accuracy, predictions = test_with_tta(model, test_data, prototypes, transform_eval, device, label_to_idx, n_tta=3)\n",
    "            \n",
    "        ensemble_accuracies.append(accuracy)\n",
    "        ensemble_predictions.append(predictions)\n",
    "        print(f\"  Ensemble {i+1}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Majority voting\n",
    "    final_y_true = []\n",
    "    final_y_pred = []\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        votes = [pred[i]['predicted'] for pred in ensemble_predictions if i < len(pred)]\n",
    "        \n",
    "        if not votes:\n",
    "            continue\n",
    "        \n",
    "        # Simple majority voting\n",
    "        vote_counts = {}\n",
    "        for vote in votes:\n",
    "            vote_counts[vote] = vote_counts.get(vote, 0) + 1\n",
    "        \n",
    "        final_pred = max(vote_counts.items(), key=lambda x: x[1])[0]\n",
    "        true_label = ensemble_predictions[0][i]['true']\n",
    "        \n",
    "        final_y_true.append(true_label)\n",
    "        final_y_pred.append(final_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_accuracy = accuracy_score(final_y_true, final_y_pred)\n",
    "    precision = precision_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(final_y_true, final_y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Enhanced Ensemble Results:\")\n",
    "    print(f\"  Final Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(ensemble_accuracies):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': final_y_true,\n",
    "        'y_pred': final_y_pred,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'individual_accuracies': ensemble_accuracies,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'std_dev': np.std(ensemble_accuracies),\n",
    "        'mean_accuracy': np.mean(ensemble_accuracies),\n",
    "        'min_accuracy': np.min(ensemble_accuracies),\n",
    "        'max_accuracy': np.max(ensemble_accuracies)\n",
    "    }\n",
    "\n",
    "def comprehensive_analysis(model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "                          transform_eval, device, k_shot=2):\n",
    "    \"\"\"Comprehensive analysis - metrics only, no predictions saved\"\"\"\n",
    "    print(\"ðŸŽ¯ Comprehensive Analysis for All Test Images...\")\n",
    "    \n",
    "    support_set = create_fixed_support_set(train_data, n_way=len(all_labels), k_shot=k_shot)\n",
    "    model.eval()\n",
    "    support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(support_images)\n",
    "        \n",
    "        prototypes = []\n",
    "        for cls_idx in range(len(all_labels)):\n",
    "            cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "            if len(cls_indices) > 0:\n",
    "                proto = support_embeddings[cls_indices].mean(0)\n",
    "                prototypes.append(F.normalize(proto, dim=0))\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        print(f\"Processing {len(test_data)} test images...\")\n",
    "        \n",
    "        for sample in tqdm(test_data, desc=\"Analyzing test images\"):\n",
    "            try:\n",
    "                img = Image.open(sample['path']).convert('RGB')\n",
    "                img_tensor = transform_eval(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                query_embedding = F.normalize(model(img_tensor).squeeze(), dim=0)\n",
    "                \n",
    "                similarities = F.cosine_similarity(query_embedding.unsqueeze(0), prototypes, dim=1)\n",
    "                pred_idx = similarities.argmax().item()\n",
    "                \n",
    "                y_true.append(label_to_idx[sample['label']])\n",
    "                y_pred.append(pred_idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['path']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"ðŸŽ¯ Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    return y_true, y_pred, overall_accuracy\n",
    "\n",
    "def create_confusion_matrix_plot(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Create and save confusion matrix visualization\"\"\"\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    cbar_kws={'label': 'Count'})\n",
    "        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Confusion matrix saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating confusion matrix: {e}\")\n",
    "\n",
    "def print_classification_report(y_true, y_pred, all_labels):\n",
    "    \"\"\"Print detailed classification report\"\"\"\n",
    "    print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    class_report = classification_report(y_true, y_pred, target_names=all_labels, \n",
    "                                        output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_name in all_labels:\n",
    "        metrics = class_report[class_name]\n",
    "        print(f\"{class_name:<15} {metrics['precision']:<12.3f} {metrics['recall']:<12.3f} \"\n",
    "              f\"{metrics['f1-score']:<12.3f} {int(metrics['support']):<10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Averages\n",
    "    macro_avg = class_report['macro avg']\n",
    "    weighted_avg = class_report['weighted avg']\n",
    "    \n",
    "    print(f\"{'Macro Avg':<15} {macro_avg['precision']:<12.3f} {macro_avg['recall']:<12.3f} \"\n",
    "          f\"{macro_avg['f1-score']:<12.3f} {int(macro_avg['support']):<10}\")\n",
    "    print(f\"{'Weighted Avg':<15} {weighted_avg['precision']:<12.3f} {weighted_avg['recall']:<12.3f} \"\n",
    "          f\"{weighted_avg['f1-score']:<12.3f} {int(weighted_avg['support']):<10}\")\n",
    "    \n",
    "    return class_report\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ COMPREHENSIVE PROTONET EVALUATION - METRICS ONLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== ENSEMBLE WITH DIVERSITY AND TTA =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ENSEMBLE EVALUATION WITH TTA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_results = ensemble_with_diversity(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, n_ensembles=5, k_shot=2\n",
    ")\n",
    "\n",
    "# Print ensemble metrics\n",
    "ensemble_report = print_classification_report(ensemble_results['y_true'], \n",
    "                                             ensemble_results['y_pred'], all_labels)\n",
    "\n",
    "# Create ensemble confusion matrix\n",
    "create_confusion_matrix_plot(ensemble_results['y_true'], ensemble_results['y_pred'], \n",
    "                            all_labels, save_path='confusion_matrix_ensemble.png')\n",
    "\n",
    "# ===== COMPREHENSIVE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPREHENSIVE SINGLE MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_true_comp, y_pred_comp, overall_accuracy = comprehensive_analysis(\n",
    "    model, test_data, train_data, all_labels, label_to_idx, idx_to_label, \n",
    "    transform_eval, device, k_shot=2\n",
    ")\n",
    "\n",
    "# Print comprehensive metrics\n",
    "comp_report = print_classification_report(y_true_comp, y_pred_comp, all_labels)\n",
    "\n",
    "# Create comprehensive confusion matrix\n",
    "create_confusion_matrix_plot(y_true_comp, y_pred_comp, all_labels, \n",
    "                            save_path='confusion_matrix_comprehensive.png')\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Enhanced Ensemble: {ensemble_results['final_accuracy']:.4f} (Â±{ensemble_results['std_dev']:.4f})\")\n",
    "print(f\"Comprehensive Analysis: {overall_accuracy:.4f}\")\n",
    "print(f\"Total Test Images Processed: {len(test_data)}\")\n",
    "print(f\"Best Single Model: {max(ensemble_results['individual_accuracies']):.4f}\")\n",
    "print(f\"Ensemble Weighted F1-Score: {ensemble_results['f1']:.4f}\")\n",
    "\n",
    "# ===== SAVE RESULTS =====\n",
    "final_results = {\n",
    "    'evaluation_setup': {\n",
    "        'support_shots': 2,\n",
    "        'n_way': len(all_labels),\n",
    "        'test_samples': len(test_data),\n",
    "        'classes': all_labels,\n",
    "        'ensemble_size': 5\n",
    "    },\n",
    "    'enhanced_ensemble': {\n",
    "        'final_accuracy': ensemble_results['final_accuracy'],\n",
    "        'individual_accuracies': ensemble_results['individual_accuracies'],\n",
    "        'mean_accuracy': ensemble_results['mean_accuracy'],\n",
    "        'std_dev': ensemble_results['std_dev'],\n",
    "        'min_accuracy': ensemble_results['min_accuracy'],\n",
    "        'max_accuracy': ensemble_results['max_accuracy'],\n",
    "        'precision': ensemble_results['precision'],\n",
    "        'recall': ensemble_results['recall'],\n",
    "        'f1': ensemble_results['f1'],\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': ensemble_report[cls]['precision'],\n",
    "                'recall': ensemble_report[cls]['recall'],\n",
    "                'f1_score': ensemble_report[cls]['f1-score'],\n",
    "                'support': int(ensemble_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'comprehensive_analysis': {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'per_class_metrics': {\n",
    "            cls: {\n",
    "                'precision': comp_report[cls]['precision'],\n",
    "                'recall': comp_report[cls]['recall'],\n",
    "                'f1_score': comp_report[cls]['f1-score'],\n",
    "                'support': int(comp_report[cls]['support'])\n",
    "            }\n",
    "            for cls in all_labels\n",
    "        }\n",
    "    },\n",
    "    'summary': {\n",
    "        'best_ensemble_accuracy': ensemble_results['final_accuracy'],\n",
    "        'comprehensive_accuracy': overall_accuracy,\n",
    "        'ensemble_std': ensemble_results['std_dev'],\n",
    "        'weighted_f1': ensemble_results['f1'],\n",
    "        'total_images_processed': len(test_data)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation_results_metrics_only.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to 'evaluation_results_metrics_only.json'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83edd7a-4436-4b04-9c65-9dac01629c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5c5e4dd-528f-4ce7-aa18-ee1165d51460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting 1+1 shot training (2-shot total per class)\n",
      "============================================================\n",
      "  - Original shots per class: 1\n",
      "  - Enhanced shots per class: 1\n",
      "  - Query samples per class: 15\n",
      "  - Total training episodes: 4000\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 0.9663 | Train: 0.600 | Val: 0.554Â±0.082 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:18<20:29:45, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:17<1:17:58,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.0012 | Train: 1.000 | Val: 0.655Â±0.076 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:34<6:43:07,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:30<1:10:03,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.0009 | Train: 1.000 | Val: 0.671Â±0.063 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [02:46<6:11:52,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [03:59<6:23:46,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.0005 | Train: 1.000 | Val: 0.628Â±0.112 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [05:09<6:04:29,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.0004 | Train: 1.000 | Val: 0.659Â±0.081 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [06:26<6:15:07,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.0002 | Train: 1.000 | Val: 0.650Â±0.105 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [07:41<5:47:40,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.0002 | Train: 1.000 | Val: 0.632Â±0.065 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [08:52<5:43:12,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.0001 | Train: 1.000 | Val: 0.648Â±0.096 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [10:05<5:40:21,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.0001 | Train: 1.000 | Val: 0.669Â±0.084 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 450/4000 [11:00<1:10:34,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.0001 | Train: 1.000 | Val: 0.687Â±0.070 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [11:18<6:06:17,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [12:36<5:38:10,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.0001 | Train: 1.000 | Val: 0.659Â±0.081 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [13:50<5:18:36,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.0002 | Train: 1.000 | Val: 0.646Â±0.091 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [15:02<5:26:18,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.0001 | Train: 1.000 | Val: 0.650Â±0.086 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [16:17<5:27:09,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.0001 | Train: 1.000 | Val: 0.588Â±0.081 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [17:30<5:30:01,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.0002 | Train: 1.000 | Val: 0.663Â±0.098 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [18:48<5:19:25,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.0001 | Train: 1.000 | Val: 0.638Â±0.106 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [20:01<5:03:30,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.0001 | Train: 1.000 | Val: 0.650Â±0.095 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [21:12<5:00:45,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.0001 | Train: 1.000 | Val: 0.671Â±0.080 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [22:28<4:58:26,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.0001 | Train: 1.000 | Val: 0.662Â±0.076 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [23:43<5:02:31,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.0000 | Train: 1.000 | Val: 0.683Â±0.078 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [25:01<4:55:36,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.0000 | Train: 1.000 | Val: 0.674Â±0.074 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [26:13<4:36:03,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.0001 | Train: 1.000 | Val: 0.683Â±0.089 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [27:25<4:47:54,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.0000 | Train: 1.000 | Val: 0.646Â±0.109 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [28:35<4:11:50,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.0001 | Train: 1.000 | Val: 0.680Â±0.088 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [29:49<4:37:01,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.0000 | Train: 1.000 | Val: 0.670Â±0.079 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [31:06<4:12:38,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.0000 | Train: 1.000 | Val: 0.653Â±0.096 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [32:20<4:28:03,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.0001 | Train: 1.000 | Val: 0.669Â±0.105 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [33:33<4:21:32,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.0001 | Train: 1.000 | Val: 0.667Â±0.104 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [34:47<4:03:27,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.0000 | Train: 1.000 | Val: 0.651Â±0.086 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                           | 1450/4000 [35:43<50:56,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.0000 | Train: 1.000 | Val: 0.692Â±0.070 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [36:01<4:21:23,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [37:15<3:43:04,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.0001 | Train: 1.000 | Val: 0.659Â±0.077 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [38:27<3:59:46,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.0000 | Train: 1.000 | Val: 0.653Â±0.096 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [39:39<3:52:08,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.0000 | Train: 1.000 | Val: 0.661Â±0.100 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [40:52<3:39:19,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.0000 | Train: 1.000 | Val: 0.653Â±0.107 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [42:08<3:55:43,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.0000 | Train: 1.000 | Val: 0.648Â±0.085 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [43:23<3:33:43,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.0000 | Train: 1.000 | Val: 0.638Â±0.088 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [44:36<3:34:39,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.0001 | Train: 1.000 | Val: 0.663Â±0.085 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [45:48<3:24:18,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.0000 | Train: 1.000 | Val: 0.636Â±0.108 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [46:57<3:16:42,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.0000 | Train: 1.000 | Val: 0.673Â±0.122 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [48:15<3:27:34,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.0000 | Train: 1.000 | Val: 0.663Â±0.101 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 2001/4000 [49:30<3:11:14,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.0000 | Train: 1.000 | Val: 0.651Â±0.098 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 2051/4000 [50:43<3:08:22,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.0000 | Train: 1.000 | Val: 0.660Â±0.072 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 2101/4000 [51:57<3:05:45,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.0000 | Train: 1.000 | Val: 0.659Â±0.088 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 2151/4000 [53:10<2:57:02,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.0000 | Train: 1.000 | Val: 0.671Â±0.097 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 2201/4000 [54:25<2:42:06,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.0000 | Train: 1.000 | Val: 0.666Â±0.106 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2251/4000 [55:39<2:44:39,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.0000 | Train: 1.000 | Val: 0.674Â±0.085 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 2301/4000 [56:51<2:45:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.0000 | Train: 1.000 | Val: 0.668Â±0.109 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2351/4000 [58:01<2:28:35,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.0000 | Train: 1.000 | Val: 0.688Â±0.079 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2400/4000 [58:56<31:35,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.0000 | Train: 1.000 | Val: 0.694Â±0.073 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2401/4000 [59:13<2:35:39,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:00:29<2:31:26,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.0000 | Train: 1.000 | Val: 0.643Â±0.106 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:01:43<2:18:07,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.0000 | Train: 1.000 | Val: 0.616Â±0.098 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:02:56<2:29:26,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.0000 | Train: 1.000 | Val: 0.658Â±0.094 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:04:08<2:09:43,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.0000 | Train: 1.000 | Val: 0.632Â±0.099 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:05:22<2:15:58,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.0000 | Train: 1.000 | Val: 0.660Â±0.087 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:06:40<2:06:03,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.0000 | Train: 1.000 | Val: 0.646Â±0.107 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:07:53<1:56:43,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.0000 | Train: 1.000 | Val: 0.649Â±0.113 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:09:06<1:54:09,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.0000 | Train: 1.000 | Val: 0.622Â±0.122 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:10:19<1:49:03,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.0000 | Train: 1.000 | Val: 0.654Â±0.094 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:11:33<1:47:08,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.0000 | Train: 1.000 | Val: 0.616Â±0.092 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:12:48<1:32:42,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.0000 | Train: 1.000 | Val: 0.620Â±0.094 | LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:14:00<1:26:57,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.0000 | Train: 1.000 | Val: 0.673Â±0.077 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:15:13<1:35:55,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.0000 | Train: 1.000 | Val: 0.628Â±0.130 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:16:27<1:22:42,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.0000 | Train: 1.000 | Val: 0.670Â±0.087 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:17:39<1:17:11,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.0000 | Train: 1.000 | Val: 0.621Â±0.102 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:18:56<1:13:57,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.0000 | Train: 1.000 | Val: 0.648Â±0.097 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:20:09<1:08:53,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.0000 | Train: 1.000 | Val: 0.667Â±0.098 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 3301/4000 [1:21:19<57:17,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.0000 | Train: 1.000 | Val: 0.674Â±0.099 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:22:30<1:01:38,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.0000 | Train: 1.000 | Val: 0.648Â±0.086 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 3401/4000 [1:23:41<56:25,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.0000 | Train: 1.000 | Val: 0.641Â±0.118 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 3451/4000 [1:24:58<52:57,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.0000 | Train: 1.000 | Val: 0.651Â±0.102 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:26:10<44:02,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.0000 | Train: 1.000 | Val: 0.639Â±0.092 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:27:24<44:07,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.0000 | Train: 1.000 | Val: 0.681Â±0.103 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:28:37<36:42,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.0000 | Train: 1.000 | Val: 0.648Â±0.114 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:29:52<34:59,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.0000 | Train: 1.000 | Val: 0.649Â±0.097 | LR: 6.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3700/4000 [1:31:00<07:22,  1.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 371\u001b[0m\n\u001b[0;32m    368\u001b[0m val_support \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m val_episode_data \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_support\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m    369\u001b[0m val_query \u001b[38;5;241m=\u001b[39m [q \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m val_episode_data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m q[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_support\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m--> 371\u001b[0m val_support_images, val_support_labels \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m val_query_images, val_query_labels \u001b[38;5;241m=\u001b[39m prepare_batch(val_query, transform_eval)\n\u001b[0;32m    374\u001b[0m val_support_embeddings \u001b[38;5;241m=\u001b[39m model(val_support_images)\n",
      "Cell \u001b[1;32mIn[25], line 254\u001b[0m, in \u001b[0;36mprepare_batch\u001b[1;34m(batch, transform)\u001b[0m\n\u001b[0;32m    252\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m--> 254\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(transform(img))\n\u001b[0;32m    256\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\ImageFile.py:314\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_end()\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m err_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# FIXED: More conservative augmentation to prevent overfitting\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Very mild crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.4 + dissim * 0.6\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "# FIXED: Simpler, more stable architecture\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune only the last layer to prevent overfitting\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Simpler projection to prevent overfitting\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),  # Higher dropout\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Fixed temperature (no learning to prevent instability)\n",
    "        self.temperature = 10.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        projected = self.projection(features)\n",
    "        return F.normalize(projected, dim=1)\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "# FIXED: More conservative optimizer settings\n",
    "model = ImprovedProtoNet().to(device)\n",
    "\n",
    "# Single optimizer with conservative settings\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "\n",
    "# Simple step LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "# Training loop - 2-shot (1+1)\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "total_k_shot = k_shot_original + k_shot_enhanced\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 300\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting 1+1 shot training (2-shot total per class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Original shots per class: {k_shot_original}\")\n",
    "print(f\"  - Enhanced shots per class: {k_shot_enhanced}\")\n",
    "print(f\"  - Query samples per class: {q_query}\")\n",
    "print(f\"  - Total training episodes: 4000\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    # Forward pass\n",
    "    support_embeddings = model(support_images)\n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Compute prototypes (simple averaging)\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    # Compute distances (negative Euclidean distance)\n",
    "    dists = torch.cdist(query_embeddings, prototypes)\n",
    "    logits = -dists * model.temperature\n",
    "    \n",
    "    # Simple cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, query_labels)\n",
    "    \n",
    "    # FIXED: Small L2 regularization on embeddings only\n",
    "    l2_reg = 0.001 * torch.mean(torch.norm(query_embeddings, dim=1))\n",
    "    total_loss = loss + l2_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            for _ in range(20):\n",
    "                # Create standard 1-shot validation episode\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    # 1 support sample\n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    # Query samples\n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_dists = torch.cdist(val_query_embeddings, val_prototypes)\n",
    "                val_logits = -val_dists * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            val_acc_std = np.std(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | \"\n",
    "                  f\"Val: {val_acc_avg:.3f}Â±{val_acc_std:.3f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "730111cf-5559-429e-90a2-eda817d026eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting ENHANCED 1+1 shot training\n",
      "============================================================\n",
      "  - Architecture: Multi-stage projection with BatchNorm\n",
      "  - Loss: Label smoothing (0.05)\n",
      "  - Augmentation: Dual-strength with multi-view\n",
      "  - Temperature: Learnable with constraints [5, 20]\n",
      "  - Episodes: 4000 with cosine annealing\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 1.1806 | Train: 0.483 | Val: 0.573Â±0.094 | Temp: 10.00 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:14<16:35:06, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:21<1:36:10,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.2168 | Train: 1.000 | Val: 0.643Â±0.097 | Temp: 9.99 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:42<8:09:54,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:50<1:14:41,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.2110 | Train: 1.000 | Val: 0.664Â±0.070 | Temp: 9.97 | LR: 9.98e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [03:13<8:02:59,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [04:46<8:07:23,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.2140 | Train: 1.000 | Val: 0.664Â±0.080 | Temp: 9.96 | LR: 9.97e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [06:24<7:03:05,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.2091 | Train: 1.000 | Val: 0.661Â±0.088 | Temp: 9.95 | LR: 9.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [07:53<7:44:40,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.2149 | Train: 1.000 | Val: 0.651Â±0.057 | Temp: 9.94 | LR: 9.90e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [09:20<7:02:35,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.2086 | Train: 1.000 | Val: 0.628Â±0.083 | Temp: 9.93 | LR: 9.86e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [10:47<7:26:10,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.2106 | Train: 1.000 | Val: 0.655Â±0.099 | Temp: 9.92 | LR: 9.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [12:13<7:07:22,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.2184 | Train: 1.000 | Val: 0.658Â±0.078 | Temp: 9.91 | LR: 9.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [13:38<6:53:54,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.2139 | Train: 1.000 | Val: 0.655Â±0.101 | Temp: 9.91 | LR: 9.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 500/4000 [14:50<1:09:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.2094 | Train: 1.000 | Val: 0.669Â±0.115 | Temp: 9.90 | LR: 9.62e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [15:11<6:49:42,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [16:32<6:19:47,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.2080 | Train: 1.000 | Val: 0.665Â±0.097 | Temp: 9.90 | LR: 9.54e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [18:04<6:48:52,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.2077 | Train: 1.000 | Val: 0.633Â±0.107 | Temp: 9.89 | LR: 9.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [19:27<6:41:28,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.2083 | Train: 1.000 | Val: 0.608Â±0.098 | Temp: 9.89 | LR: 9.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [21:08<6:49:46,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.2079 | Train: 1.000 | Val: 0.649Â±0.094 | Temp: 9.89 | LR: 9.27e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [22:34<6:48:06,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.2058 | Train: 1.000 | Val: 0.643Â±0.122 | Temp: 9.89 | LR: 9.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [24:04<6:15:23,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.2053 | Train: 1.000 | Val: 0.617Â±0.088 | Temp: 9.89 | LR: 9.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [25:30<6:20:31,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.2072 | Train: 1.000 | Val: 0.666Â±0.064 | Temp: 9.89 | LR: 8.93e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [27:00<6:44:50,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.2080 | Train: 1.000 | Val: 0.639Â±0.104 | Temp: 9.89 | LR: 8.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [28:31<5:47:49,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.2052 | Train: 1.000 | Val: 0.616Â±0.128 | Temp: 9.89 | LR: 8.68e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [29:58<5:43:03,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.2082 | Train: 1.000 | Val: 0.665Â±0.097 | Temp: 9.89 | LR: 8.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [31:27<5:54:30,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.2044 | Train: 1.000 | Val: 0.620Â±0.123 | Temp: 9.89 | LR: 8.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [32:55<5:42:38,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.2119 | Train: 1.000 | Val: 0.644Â±0.106 | Temp: 9.89 | LR: 8.26e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [34:21<6:05:08,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.2025 | Train: 1.000 | Val: 0.628Â±0.108 | Temp: 9.89 | LR: 8.11e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [35:51<5:21:36,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.2049 | Train: 1.000 | Val: 0.638Â±0.085 | Temp: 9.89 | LR: 7.96e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [37:18<5:28:39,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.2044 | Train: 1.000 | Val: 0.660Â±0.125 | Temp: 9.90 | LR: 7.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [38:48<5:36:12,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.2056 | Train: 1.000 | Val: 0.620Â±0.081 | Temp: 9.90 | LR: 7.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [40:17<5:36:02,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.2036 | Train: 1.000 | Val: 0.621Â±0.114 | Temp: 9.90 | LR: 7.47e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [41:53<4:46:43,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.2050 | Train: 1.000 | Val: 0.627Â±0.125 | Temp: 9.90 | LR: 7.29e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [43:20<5:04:08,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.2048 | Train: 1.000 | Val: 0.648Â±0.087 | Temp: 9.91 | LR: 7.12e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [44:47<4:49:12,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.2071 | Train: 1.000 | Val: 0.635Â±0.100 | Temp: 9.91 | LR: 6.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [46:12<4:42:04,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.2039 | Train: 1.000 | Val: 0.598Â±0.089 | Temp: 9.91 | LR: 6.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [47:41<4:34:14,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.2084 | Train: 1.000 | Val: 0.639Â±0.109 | Temp: 9.92 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [49:08<4:29:49,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.2037 | Train: 1.000 | Val: 0.643Â±0.081 | Temp: 9.92 | LR: 6.39e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [50:29<4:40:23,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.2039 | Train: 1.000 | Val: 0.663Â±0.095 | Temp: 9.92 | LR: 6.20e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [51:53<4:32:10,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.2036 | Train: 1.000 | Val: 0.635Â±0.092 | Temp: 9.93 | LR: 6.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [53:21<4:24:22,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.2045 | Train: 1.000 | Val: 0.641Â±0.077 | Temp: 9.93 | LR: 5.82e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [54:46<3:57:01,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.2041 | Train: 1.000 | Val: 0.612Â±0.117 | Temp: 9.93 | LR: 5.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [56:13<4:14:35,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.2063 | Train: 1.000 | Val: 0.611Â±0.105 | Temp: 9.94 | LR: 5.43e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [57:36<3:58:59,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.2045 | Train: 1.000 | Val: 0.607Â±0.072 | Temp: 9.94 | LR: 5.24e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 2001/4000 [59:08<4:13:01,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.2041 | Train: 1.000 | Val: 0.641Â±0.090 | Temp: 9.94 | LR: 5.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 2051/4000 [1:00:33<3:49:10,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.2041 | Train: 1.000 | Val: 0.612Â±0.093 | Temp: 9.95 | LR: 4.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 2101/4000 [1:01:55<3:40:53,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.2040 | Train: 1.000 | Val: 0.643Â±0.104 | Temp: 9.95 | LR: 4.66e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 2151/4000 [1:03:30<3:34:44,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.2038 | Train: 1.000 | Val: 0.613Â±0.082 | Temp: 9.95 | LR: 4.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 2200/4000 [1:04:36<52:16,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.2029 | Train: 1.000 | Val: 0.677Â±0.086 | Temp: 9.95 | LR: 4.27e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2201/4000 [1:04:58<3:48:55,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 2251/4000 [1:06:31<3:15:56,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.2039 | Train: 1.000 | Val: 0.643Â±0.093 | Temp: 9.96 | LR: 4.08e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2301/4000 [1:07:55<3:19:13,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.2045 | Train: 1.000 | Val: 0.613Â±0.106 | Temp: 9.96 | LR: 3.89e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2351/4000 [1:09:25<3:25:18,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.2031 | Train: 1.000 | Val: 0.596Â±0.132 | Temp: 9.96 | LR: 3.70e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2401/4000 [1:10:59<3:13:15,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.2026 | Train: 1.000 | Val: 0.647Â±0.120 | Temp: 9.97 | LR: 3.52e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:12:28<2:50:11,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.2033 | Train: 1.000 | Val: 0.615Â±0.091 | Temp: 9.97 | LR: 3.33e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:13:53<2:57:13,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.2032 | Train: 1.000 | Val: 0.621Â±0.107 | Temp: 9.97 | LR: 3.15e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:15:23<2:43:56,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.2031 | Train: 1.000 | Val: 0.607Â±0.123 | Temp: 9.97 | LR: 2.97e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:16:52<2:56:44,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.2036 | Train: 1.000 | Val: 0.609Â±0.128 | Temp: 9.98 | LR: 2.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:18:17<2:33:13,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.2033 | Train: 1.000 | Val: 0.600Â±0.094 | Temp: 9.98 | LR: 2.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:19:46<2:35:21,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.2033 | Train: 1.000 | Val: 0.627Â±0.083 | Temp: 9.98 | LR: 2.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:21:11<2:24:20,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.2031 | Train: 1.000 | Val: 0.633Â±0.103 | Temp: 9.98 | LR: 2.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:22:30<2:08:42,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.2031 | Train: 1.000 | Val: 0.625Â±0.119 | Temp: 9.99 | LR: 2.14e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:24:04<2:11:13,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.2038 | Train: 1.000 | Val: 0.613Â±0.114 | Temp: 9.99 | LR: 1.98e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:25:35<2:11:58,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.2037 | Train: 1.000 | Val: 0.612Â±0.113 | Temp: 9.99 | LR: 1.83e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:26:57<2:04:42,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.2039 | Train: 1.000 | Val: 0.587Â±0.105 | Temp: 9.99 | LR: 1.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:28:30<2:06:41,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.2031 | Train: 1.000 | Val: 0.619Â±0.081 | Temp: 9.99 | LR: 1.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:29:59<1:51:09,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.2053 | Train: 1.000 | Val: 0.651Â±0.098 | Temp: 10.00 | LR: 1.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:31:25<1:44:40,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.2032 | Train: 1.000 | Val: 0.599Â±0.119 | Temp: 10.00 | LR: 1.28e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:32:56<1:41:28,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.2031 | Train: 1.000 | Val: 0.647Â±0.102 | Temp: 10.00 | LR: 1.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:34:21<1:34:46,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.2031 | Train: 1.000 | Val: 0.633Â±0.093 | Temp: 10.00 | LR: 1.04e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:35:49<1:25:16,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.2029 | Train: 1.000 | Val: 0.660Â±0.098 | Temp: 10.00 | LR: 9.32e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 3301/4000 [1:37:13<1:21:19,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.2029 | Train: 1.000 | Val: 0.610Â±0.084 | Temp: 10.00 | LR: 8.27e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:38:38<1:14:30,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.2035 | Train: 1.000 | Val: 0.608Â±0.114 | Temp: 10.00 | LR: 7.29e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 3401/4000 [1:40:08<1:15:50,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.2030 | Train: 1.000 | Val: 0.633Â±0.092 | Temp: 10.00 | LR: 6.38e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 3451/4000 [1:41:41<1:02:07,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.2027 | Train: 1.000 | Val: 0.617Â±0.084 | Temp: 10.00 | LR: 5.53e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:43:06<57:08,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.2041 | Train: 1.000 | Val: 0.599Â±0.097 | Temp: 10.00 | LR: 4.75e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:44:27<56:37,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.2028 | Train: 1.000 | Val: 0.630Â±0.099 | Temp: 10.00 | LR: 4.05e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:45:51<44:25,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.2039 | Train: 1.000 | Val: 0.623Â±0.104 | Temp: 10.00 | LR: 3.41e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:47:20<43:22,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.2027 | Train: 1.000 | Val: 0.640Â±0.102 | Temp: 10.00 | LR: 2.85e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3701/4000 [1:48:41<33:36,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 | Loss: 0.2034 | Train: 1.000 | Val: 0.621Â±0.115 | Temp: 10.00 | LR: 2.36e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3751/4000 [1:50:11<30:39,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3750 | Loss: 0.2032 | Train: 1.000 | Val: 0.589Â±0.136 | Temp: 10.00 | LR: 1.94e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3801/4000 [1:51:41<23:34,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 | Loss: 0.2035 | Train: 1.000 | Val: 0.631Â±0.094 | Temp: 10.00 | LR: 1.60e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3851/4000 [1:53:10<18:46,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3850 | Loss: 0.2033 | Train: 1.000 | Val: 0.659Â±0.113 | Temp: 10.00 | LR: 1.34e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3901/4000 [1:54:32<10:42,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 | Loss: 0.2036 | Train: 1.000 | Val: 0.655Â±0.102 | Temp: 10.00 | LR: 1.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3951/4000 [1:55:57<05:48,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3950 | Loss: 0.2035 | Train: 1.000 | Val: 0.636Â±0.120 | Temp: 10.00 | LR: 1.04e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [1:57:05<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 0.677\n",
      "============================================================\n",
      "\n",
      "Best model loaded for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# ENHANCED: Multiple augmentation views with test-time augmentation compatibility\n",
    "transform_train_strong = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2, hue=0.08),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_train_weak = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.95, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.4 + dissim * 0.6\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "# ENHANCED: Better architecture with residual connections and multi-scale features\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last 2 layers for better feature extraction\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with residual-like connections\n",
    "        self.projection1 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.projection2 = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.projection3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Learnable temperature with constraints\n",
    "        self.log_temperature = nn.Parameter(torch.log(torch.tensor(10.0)))\n",
    "        \n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Multi-stage projection\n",
    "        h1 = self.projection1(features)\n",
    "        h2 = self.projection2(h1)\n",
    "        h3 = self.projection3(h2)\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return F.normalize(h3, dim=1), F.normalize(h2, dim=1)\n",
    "        return F.normalize(h3, dim=1)\n",
    "    \n",
    "    @property\n",
    "    def temperature(self):\n",
    "        # Constrain temperature to reasonable range [5, 20]\n",
    "        return torch.clamp(torch.exp(self.log_temperature), 5.0, 20.0)\n",
    "\n",
    "def prepare_batch(batch, transform, multi_view=False):\n",
    "    \"\"\"Prepare batch with optional multi-view augmentation\"\"\"\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    \n",
    "    if multi_view and random.random() < 0.3:  # 30% chance to use second view\n",
    "        images2 = []\n",
    "        for sample in batch:\n",
    "            img = Image.open(sample['path']).convert('RGB')\n",
    "            images2.append(transform(img))\n",
    "        return torch.stack(images).to(device), torch.stack(images2).to(device), torch.tensor(labels).to(device)\n",
    "    \n",
    "    return torch.stack(images).to(device), None, torch.tensor(labels).to(device)\n",
    "\n",
    "# ENHANCED: Multi-prototype approach\n",
    "def compute_prototypes_robust(embeddings, labels, n_way, num_augments=1):\n",
    "    \"\"\"Compute prototypes with optional multi-view averaging\"\"\"\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(labels == cls)[0]\n",
    "        cls_embeddings = embeddings[cls_indices]\n",
    "        \n",
    "        # Average over all support samples for this class\n",
    "        proto = cls_embeddings.mean(0)\n",
    "        prototypes.append(proto)\n",
    "    \n",
    "    return torch.stack(prototypes)\n",
    "\n",
    "# ENHANCED: Label smoothing for better generalization\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_class = pred.size(1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
    "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
    "        log_prob = F.log_softmax(pred, dim=1)\n",
    "        return -(one_hot * log_prob).sum(dim=1).mean()\n",
    "\n",
    "# Model setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "\n",
    "# Differential learning rates\n",
    "clip_params = [p for n, p in model.named_parameters() if 'clip_model' in n or 'encoder' in n]\n",
    "other_params = [p for n, p in model.named_parameters() if not ('clip_model' in n or 'encoder' in n)]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': 2e-5, 'weight_decay': 1e-4},\n",
    "    {'params': other_params, 'lr': 1e-4, 'weight_decay': 1e-3}\n",
    "])\n",
    "\n",
    "# Cosine annealing with warmup\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4000, eta_min=1e-6)\n",
    "\n",
    "# Label smoothing loss\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.05)\n",
    "\n",
    "# Training configuration\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 400\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting ENHANCED 1+1 shot training\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Architecture: Multi-stage projection with BatchNorm\")\n",
    "print(f\"  - Loss: Label smoothing (0.05)\")\n",
    "print(f\"  - Augmentation: Dual-strength with multi-view\")\n",
    "print(f\"  - Temperature: Learnable with constraints [5, 20]\")\n",
    "print(f\"  - Episodes: 4000 with cosine annealing\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    \n",
    "    # Alternate between strong and weak augmentation\n",
    "    use_strong_aug = episode % 2 == 0\n",
    "    current_transform = transform_train_strong if use_strong_aug else transform_train_weak\n",
    "    \n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    # Prepare with multi-view augmentation\n",
    "    support_images, support_images2, support_labels = prepare_batch(support_set, transform_eval, multi_view=False)\n",
    "    query_images, query_images2, query_labels = prepare_batch(query_set, current_transform, multi_view=True)\n",
    "    \n",
    "    # Forward pass - support\n",
    "    support_embeddings = model(support_images)\n",
    "    \n",
    "    # Forward pass - query (with optional second view)\n",
    "    query_embeddings = model(query_images)\n",
    "    if query_images2 is not None:\n",
    "        query_embeddings2 = model(query_images2)\n",
    "        query_embeddings = (query_embeddings + query_embeddings2) / 2  # Average views\n",
    "    \n",
    "    # Compute prototypes\n",
    "    prototypes = compute_prototypes_robust(support_embeddings, support_labels, n_way)\n",
    "    \n",
    "    # Compute distances with learnable temperature\n",
    "    dists = torch.cdist(query_embeddings, prototypes)\n",
    "    logits = -dists * model.temperature\n",
    "    \n",
    "    # Label smoothing loss\n",
    "    loss = criterion(logits, query_labels)\n",
    "    \n",
    "    # Regularization: encourage diverse prototypes\n",
    "    proto_sim = torch.mm(prototypes, prototypes.t())\n",
    "    proto_mask = 1 - torch.eye(n_way).to(device)\n",
    "    proto_diversity = 0.02 * (proto_sim * proto_mask).sum() / (n_way * (n_way - 1))\n",
    "    \n",
    "    # Regularization: prevent temperature from becoming too extreme\n",
    "    temp_reg = 0.001 * (model.temperature - 10.0).abs()\n",
    "    \n",
    "    total_loss = loss + proto_diversity + temp_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            \n",
    "            for _ in range(25):  # More validation episodes\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, _, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, _, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = compute_prototypes_robust(val_support_embeddings, val_support_labels, n_way)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_dists = torch.cdist(val_query_embeddings, val_prototypes)\n",
    "                val_logits = -val_dists * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            val_acc_std = np.std(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[1]['lr']\n",
    "            current_temp = model.temperature.item()\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | \"\n",
    "                  f\"Val: {val_acc_avg:.3f}Â±{val_acc_std:.3f} | Temp: {current_temp:.2f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90b683a5-c08d-44b7-b7ad-df108a18f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Filtered training data: 4 samples from indexes [1, 16, 31, 46]\n",
      "Validation data: 9113 samples\n",
      "Test data: 20872 samples\n",
      "Classes: ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
      "\n",
      "Starting meta-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 1/5000 [00:11<16:19:00, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 â€” Loss: 1.2690 â€” Val Loss: 1.1547 â€” Train Acc: 40.00% â€” Val Acc: 59.63% â€” LR: 0.000040\n",
      "âœ… New best model with validation accuracy: 59.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–ˆâ–Œ                                                                           | 101/5000 [01:49<5:52:50,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 â€” Loss: 0.5425 â€” Val Loss: 1.0543 â€” Train Acc: 92.50% â€” Val Acc: 61.50% â€” LR: 0.000051\n",
      "âœ… New best model with validation accuracy: 61.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆ                                                                          | 201/5000 [03:25<5:50:53,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 â€” Loss: 0.4554 â€” Val Loss: 0.9209 â€” Train Acc: 95.00% â€” Val Acc: 67.50% â€” LR: 0.000082\n",
      "âœ… New best model with validation accuracy: 67.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                        | 301/5000 [05:07<5:56:01,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300 â€” Loss: 0.3987 â€” Val Loss: 0.9395 â€” Train Acc: 100.00% â€” Val Acc: 65.38% â€” LR: 0.000132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                      | 401/5000 [06:48<5:43:05,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400 â€” Loss: 0.3706 â€” Val Loss: 0.9116 â€” Train Acc: 100.00% â€” Val Acc: 68.63% â€” LR: 0.000200\n",
      "âœ… New best model with validation accuracy: 68.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                     | 501/5000 [08:26<5:26:06,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500 â€” Loss: 0.3799 â€” Val Loss: 0.9278 â€” Train Acc: 100.00% â€” Val Acc: 66.50% â€” LR: 0.000281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                   | 601/5000 [10:04<5:38:34,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600 â€” Loss: 0.3630 â€” Val Loss: 0.9109 â€” Train Acc: 100.00% â€” Val Acc: 67.75% â€” LR: 0.000373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                  | 701/5000 [11:44<4:35:53,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700 â€” Loss: 0.3592 â€” Val Loss: 0.9118 â€” Train Acc: 100.00% â€” Val Acc: 69.00% â€” LR: 0.000471\n",
      "âœ… New best model with validation accuracy: 69.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                | 801/5000 [13:20<4:32:33,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800 â€” Loss: 0.3624 â€” Val Loss: 0.8950 â€” Train Acc: 100.00% â€” Val Acc: 68.63% â€” LR: 0.000572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                               | 901/5000 [14:56<4:35:40,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900 â€” Loss: 0.3634 â€” Val Loss: 0.9443 â€” Train Acc: 100.00% â€” Val Acc: 66.50% â€” LR: 0.000670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                            | 1001/5000 [16:37<4:42:30,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 â€” Loss: 0.3606 â€” Val Loss: 0.8984 â€” Train Acc: 100.00% â€” Val Acc: 70.88% â€” LR: 0.000761\n",
      "âœ… New best model with validation accuracy: 70.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                           | 1101/5000 [18:17<4:26:05,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 â€” Loss: 0.3626 â€” Val Loss: 0.8703 â€” Train Acc: 100.00% â€” Val Acc: 71.00% â€” LR: 0.000842\n",
      "âœ… New best model with validation accuracy: 71.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                         | 1201/5000 [19:48<4:18:36,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 â€” Loss: 0.3659 â€” Val Loss: 0.9461 â€” Train Acc: 100.00% â€” Val Acc: 66.00% â€” LR: 0.000909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                        | 1301/5000 [21:24<3:53:18,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 â€” Loss: 0.3530 â€” Val Loss: 0.8945 â€” Train Acc: 100.00% â€” Val Acc: 72.00% â€” LR: 0.000959\n",
      "âœ… New best model with validation accuracy: 72.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                      | 1401/5000 [23:02<3:24:29,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 â€” Loss: 0.3579 â€” Val Loss: 0.8654 â€” Train Acc: 100.00% â€” Val Acc: 70.50% â€” LR: 0.000990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                     | 1501/5000 [24:32<3:35:50,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 â€” Loss: 0.3542 â€” Val Loss: 0.8474 â€” Train Acc: 100.00% â€” Val Acc: 71.50% â€” LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                   | 1601/5000 [26:10<3:45:56,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 â€” Loss: 0.3563 â€” Val Loss: 0.8675 â€” Train Acc: 100.00% â€” Val Acc: 71.13% â€” LR: 0.000998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                  | 1701/5000 [27:47<3:32:34,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 â€” Loss: 0.3549 â€” Val Loss: 0.9151 â€” Train Acc: 100.00% â€” Val Acc: 69.25% â€” LR: 0.000992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 1801/5000 [29:30<3:53:47,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 â€” Loss: 0.3542 â€” Val Loss: 0.9168 â€” Train Acc: 100.00% â€” Val Acc: 71.00% â€” LR: 0.000982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1901/5000 [31:05<3:44:32,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 â€” Loss: 0.3563 â€” Val Loss: 0.8992 â€” Train Acc: 100.00% â€” Val Acc: 68.88% â€” LR: 0.000968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 2001/5000 [32:40<3:21:22,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 â€” Loss: 0.3518 â€” Val Loss: 0.9339 â€” Train Acc: 100.00% â€” Val Acc: 68.38% â€” LR: 0.000950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2101/5000 [34:19<3:16:33,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 â€” Loss: 0.3621 â€” Val Loss: 0.9300 â€” Train Acc: 100.00% â€” Val Acc: 68.88% â€” LR: 0.000929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 2201/5000 [36:00<3:28:25,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 â€” Loss: 0.3544 â€” Val Loss: 0.9352 â€” Train Acc: 100.00% â€” Val Acc: 66.38% â€” LR: 0.000904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                         | 2301/5000 [37:38<3:17:55,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 â€” Loss: 0.3524 â€” Val Loss: 0.8868 â€” Train Acc: 100.00% â€” Val Acc: 74.38% â€” LR: 0.000876\n",
      "âœ… New best model with validation accuracy: 74.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 2401/5000 [39:19<3:17:58,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 â€” Loss: 0.3540 â€” Val Loss: 0.9089 â€” Train Acc: 100.00% â€” Val Acc: 66.75% â€” LR: 0.000845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 2501/5000 [41:04<2:53:23,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 â€” Loss: 0.3543 â€” Val Loss: 0.8601 â€” Train Acc: 100.00% â€” Val Acc: 71.75% â€” LR: 0.000811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 2601/5000 [42:38<2:58:24,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 â€” Loss: 0.3524 â€” Val Loss: 0.9578 â€” Train Acc: 100.00% â€” Val Acc: 68.13% â€” LR: 0.000775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2701/5000 [44:14<2:32:16,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 â€” Loss: 0.3524 â€” Val Loss: 0.9531 â€” Train Acc: 100.00% â€” Val Acc: 69.00% â€” LR: 0.000736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 2801/5000 [45:41<2:36:15,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 â€” Loss: 0.3574 â€” Val Loss: 0.9042 â€” Train Acc: 100.00% â€” Val Acc: 69.88% â€” LR: 0.000696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 2901/5000 [47:22<2:13:21,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 â€” Loss: 0.3528 â€” Val Loss: 0.9136 â€” Train Acc: 100.00% â€” Val Acc: 67.63% â€” LR: 0.000654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 3001/5000 [49:01<2:28:33,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 â€” Loss: 0.3526 â€” Val Loss: 0.9244 â€” Train Acc: 100.00% â€” Val Acc: 69.50% â€” LR: 0.000610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 3101/5000 [50:37<2:14:59,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 â€” Loss: 0.3547 â€” Val Loss: 0.9268 â€” Train Acc: 100.00% â€” Val Acc: 70.38% â€” LR: 0.000566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 3201/5000 [52:03<1:23:59,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 â€” Loss: 0.3520 â€” Val Loss: 0.9542 â€” Train Acc: 100.00% â€” Val Acc: 68.63% â€” LR: 0.000522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 3301/5000 [53:30<1:56:56,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 â€” Loss: 0.3530 â€” Val Loss: 0.8714 â€” Train Acc: 100.00% â€” Val Acc: 70.00% â€” LR: 0.000477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 3401/5000 [55:06<1:51:11,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 â€” Loss: 0.3539 â€” Val Loss: 0.9152 â€” Train Acc: 100.00% â€” Val Acc: 70.75% â€” LR: 0.000432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 3501/5000 [56:42<1:37:57,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 â€” Loss: 0.3546 â€” Val Loss: 0.9012 â€” Train Acc: 100.00% â€” Val Acc: 69.00% â€” LR: 0.000388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 3601/5000 [58:22<1:35:28,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 â€” Loss: 0.3528 â€” Val Loss: 0.8524 â€” Train Acc: 100.00% â€” Val Acc: 74.50% â€” LR: 0.000345\n",
      "âœ… New best model with validation accuracy: 74.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 3701/5000 [1:00:00<1:30:35,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 â€” Loss: 0.3520 â€” Val Loss: 0.8820 â€” Train Acc: 100.00% â€” Val Acc: 74.25% â€” LR: 0.000303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 3801/5000 [1:01:35<1:28:06,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 â€” Loss: 0.3550 â€” Val Loss: 0.8877 â€” Train Acc: 100.00% â€” Val Acc: 72.63% â€” LR: 0.000262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 3901/5000 [1:03:07<1:08:01,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 â€” Loss: 0.3535 â€” Val Loss: 0.8866 â€” Train Acc: 100.00% â€” Val Acc: 70.63% â€” LR: 0.000224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 4001/5000 [1:04:47<1:13:46,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000 â€” Loss: 0.3532 â€” Val Loss: 0.8580 â€” Train Acc: 100.00% â€” Val Acc: 72.25% â€” LR: 0.000188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 4101/5000 [1:06:26<1:07:45,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4100 â€” Loss: 0.3538 â€” Val Loss: 0.9857 â€” Train Acc: 100.00% â€” Val Acc: 62.63% â€” LR: 0.000154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 4201/5000 [1:07:59<55:26,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4200 â€” Loss: 0.3526 â€” Val Loss: 0.9243 â€” Train Acc: 100.00% â€” Val Acc: 66.50% â€” LR: 0.000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 4301/5000 [1:09:31<45:39,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4300 â€” Loss: 0.3523 â€” Val Loss: 0.9057 â€” Train Acc: 100.00% â€” Val Acc: 72.38% â€” LR: 0.000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 4401/5000 [1:11:15<45:42,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4400 â€” Loss: 0.3554 â€” Val Loss: 0.9109 â€” Train Acc: 100.00% â€” Val Acc: 70.88% â€” LR: 0.000070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 4501/5000 [1:12:50<37:03,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4500 â€” Loss: 0.3523 â€” Val Loss: 0.8921 â€” Train Acc: 100.00% â€” Val Acc: 74.38% â€” LR: 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 4601/5000 [1:14:26<28:11,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4600 â€” Loss: 0.3524 â€” Val Loss: 0.9352 â€” Train Acc: 100.00% â€” Val Acc: 70.00% â€” LR: 0.000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4701/5000 [1:16:03<22:11,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4700 â€” Loss: 0.3535 â€” Val Loss: 0.9413 â€” Train Acc: 100.00% â€” Val Acc: 66.50% â€” LR: 0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 4801/5000 [1:17:42<13:43,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4800 â€” Loss: 0.3537 â€” Val Loss: 0.9526 â€” Train Acc: 100.00% â€” Val Acc: 67.00% â€” LR: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4901/5000 [1:19:19<07:16,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4900 â€” Loss: 0.3516 â€” Val Loss: 0.9150 â€” Train Acc: 100.00% â€” Val Acc: 68.75% â€” LR: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [1:20:43<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation accuracy: 74.50%\n",
      "Best model loaded for final evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, dict) and 'images' in data:\n",
    "        return data['images']\n",
    "    elif isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected JSON structure in {json_path}. Expected either a list or dict with 'images' key.\")\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    \"\"\"Filter data to only include items with specific indexes\"\"\"\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load data from new paths\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Filter training data to only include indexes 1, 16, 31, 46\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "print(f\"Filtered training data: {len(train_data)} samples from indexes {target_indexes}\")\n",
    "print(f\"Validation data: {len(val_data)} samples\")\n",
    "print(f\"Test data: {len(test_data)} samples\")\n",
    "\n",
    "# New class labels based on your data\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "print(f\"Classes: {all_labels}\")\n",
    "\n",
    "# Advanced training transforms with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop(224, padding=20, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomAutocontrast(p=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Simpler evaluation transforms\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode(data, n_way, k_shot, q_query, allow_training_in_query=True, max_training_in_query=10):\n",
    "    episode = []\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        available_samples = class_samples[cls]\n",
    "        \n",
    "        if len(available_samples) < k_shot + q_query:\n",
    "            support_samples = random.sample(available_samples, min(k_shot, len(available_samples)))\n",
    "            \n",
    "            if allow_training_in_query:\n",
    "                remaining_needed = q_query\n",
    "                query_samples = []\n",
    "                \n",
    "                non_support_samples = [s for s in available_samples if s not in support_samples]\n",
    "                if non_support_samples:\n",
    "                    take_from_non_support = min(remaining_needed, len(non_support_samples))\n",
    "                    query_samples.extend(random.sample(non_support_samples, take_from_non_support))\n",
    "                    remaining_needed -= take_from_non_support\n",
    "                \n",
    "                if remaining_needed > 0:\n",
    "                    additional_samples = random.choices(available_samples, k=min(remaining_needed, max_training_in_query))\n",
    "                    query_samples.extend(additional_samples)\n",
    "            else:\n",
    "                query_samples = random.sample(available_samples, q_query)\n",
    "        else:\n",
    "            samples = random.sample(available_samples, k_shot + q_query)\n",
    "            support_samples = samples[:k_shot]\n",
    "            query_samples = samples[k_shot:]\n",
    "        \n",
    "        for i, sample in enumerate(support_samples):\n",
    "            episode.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "        \n",
    "        for sample in query_samples:\n",
    "            episode.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "# ProtoNet class definition with improvements\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Freeze base CLIP parameters\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.encoder = self.clip_model.visual\n",
    "            \n",
    "        # Improved projection head with dropout and layer norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "def prepare_batch(batch, transform):\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    return torch.stack(images).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "model = ProtoNet().to(device)\n",
    "\n",
    "# Improved optimizer settings\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,  # Higher initial learning rate\n",
    "    weight_decay=1e-3,  # Stronger regularization\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# More aggressive learning rate schedule\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    total_steps=5000,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)  # Add label smoothing\n",
    "\n",
    "# Training parameters\n",
    "n_way = 4\n",
    "k_shot = 1\n",
    "q_query = 10\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\nStarting meta-training...\")\n",
    "for episode in tqdm(range(5000)):  # Increased to 5000 episodes\n",
    "    model.train()\n",
    "    episode_data, _ = create_episode(train_data, n_way, k_shot, q_query, \n",
    "                                     allow_training_in_query=True, \n",
    "                                     max_training_in_query=10)\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    support_images, support_labels = prepare_batch(support_set, transform_train)\n",
    "    query_images, query_labels = prepare_batch(query_set, transform_train)\n",
    "    \n",
    "    support_embeddings = model(support_images)\n",
    "    \n",
    "    # Compute class prototypes\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(support_labels == cls)[0]\n",
    "        cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "        prototypes.append(cls_proto)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    query_embeddings = model(query_images)\n",
    "    \n",
    "    # Use negative squared euclidean distance with temperature scaling\n",
    "    temperature = 10.0  # Temperature parameter for better gradients\n",
    "    dists = -torch.cdist(query_embeddings, prototypes)**2 / temperature\n",
    "    loss = loss_fn(dists, query_labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping to prevent instability\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            val_losses = []\n",
    "            \n",
    "            for val_ep in range(20):  # More validation episodes for stability\n",
    "                val_episode_data, _ = create_episode(val_data, n_way, k_shot, q_query)\n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = []\n",
    "                for cls in range(n_way):\n",
    "                    cls_indices = torch.where(val_support_labels == cls)[0]\n",
    "                    val_proto = val_support_embeddings[cls_indices].mean(0)\n",
    "                    val_prototypes.append(val_proto)\n",
    "                val_prototypes = torch.stack(val_prototypes)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_dists = -torch.cdist(val_query_embeddings, val_prototypes)**2 / temperature\n",
    "                val_preds = torch.argmax(val_dists, dim=1)\n",
    "                \n",
    "                val_loss = loss_fn(val_dists, val_query_labels)\n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                \n",
    "                val_accuracies.append(val_acc)\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            train_acc = (torch.argmax(dists, dim=1) == query_labels).float().mean().item()\n",
    "            val_acc_avg = sum(val_accuracies) / len(val_accuracies)\n",
    "            val_loss_avg = sum(val_losses) / len(val_losses)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"Episode {episode} â€” Loss: {loss.item():.4f} â€” Val Loss: {val_loss_avg:.4f} â€” Train Acc: {train_acc:.2%} â€” Val Acc: {val_acc_avg:.2%} â€” LR: {current_lr:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"âœ… New best model with validation accuracy: {best_val_acc:.2%}\")\n",
    "\n",
    "# Save best model\n",
    "torch.save(best_model_state, 'clip_seagrass_1shot_filtered_improved.pth')\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.2%}\")\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load('clip_seagrass_1shot_filtered_improved.pth'))\n",
    "print(\"Best model loaded for final evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44466fba-aac9-4ca0-87c9-adef6f1b5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total loaded: 20872 samples from enhanced results\n",
      "\n",
      "First sample keys: ['image_path', 'true_label', 'predicted_label', 'is_correct', 'predicted_class_idx', 'confidence', 'entropy', 'distance_to_prototype', 'margin', 'max_confidence', 'avg_dissimilarity', 'min_dissimilarity', 'max_dissimilarity', 'closest_support_image', 'most_dissimilar_support_image', 'dissimilarity_std', 'combined_score']\n",
      "First sample preview: [('image_path', 'C:/Users/matin/ECU/Test_Patches/Amphibolis/Amphibolis_DBCA_test_100_patch_0.jpg'), ('true_label', 'Amphibolis'), ('predicted_label', 'Amphibolis')]\n",
      "\n",
      "Processing summary:\n",
      "  - Total samples processed: 20872\n",
      "  - Correct predictions: 13543\n",
      "  - Incorrect predictions: 7329\n",
      "  - Skipped (no true_label): 0\n",
      "Training (original): 4\n",
      "Enhanced data available (correct predictions only):\n",
      "  Amphibolis: 4993 images (avg conf: 0.994, avg dissim: 0.095)\n",
      "  Background: 1596 images (avg conf: 0.988, avg dissim: 0.094)\n",
      "  Halophila: 5460 images (avg conf: 0.996, avg dissim: 0.064)\n",
      "  Posidonia: 1494 images (avg conf: 0.952, avg dissim: 0.182)\n",
      "Validation: 9113, Test: 20872\n",
      "\n",
      "============================================================\n",
      "Starting ENHANCED 1+1 shot training\n",
      "============================================================\n",
      "  - Architecture: Multi-stage projection with BatchNorm\n",
      "  - Loss: Label smoothing (0.05)\n",
      "  - Augmentation: Dual-strength with multi-view\n",
      "  - Temperature: Learnable with constraints [5, 20]\n",
      "  - Episodes: 4000 with cosine annealing\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Loss: 1.4142 | Train: 0.317 | Val: 0.580Â±0.124 | Temp: 10.00 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                    | 1/4000 [00:23<25:33:21, 23.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 50/4000 [01:29<1:25:05,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   50 | Loss: 0.2334 | Train: 1.000 | Val: 0.634Â±0.075 | Temp: 9.99 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|â–Š                                                                   | 51/4000 [01:51<8:28:45,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–ˆâ–‹                                                                 | 100/4000 [02:57<1:23:13,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Loss: 0.2109 | Train: 1.000 | Val: 0.687Â±0.067 | Temp: 9.98 | LR: 9.98e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–ˆâ–‹                                                                 | 101/4000 [03:18<7:58:38,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… New best: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–ˆâ–ˆâ–Œ                                                                | 151/4000 [04:57<8:20:15,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  150 | Loss: 0.2146 | Train: 1.000 | Val: 0.643Â±0.071 | Temp: 9.96 | LR: 9.97e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–ˆâ–ˆâ–ˆâ–Ž                                                               | 201/4000 [06:28<8:18:10,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Loss: 0.2087 | Train: 1.000 | Val: 0.645Â±0.106 | Temp: 9.95 | LR: 9.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 251/4000 [07:59<8:14:25,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  250 | Loss: 0.2103 | Train: 1.000 | Val: 0.615Â±0.090 | Temp: 9.94 | LR: 9.90e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 301/4000 [09:31<7:54:55,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  300 | Loss: 0.2122 | Train: 1.000 | Val: 0.633Â±0.091 | Temp: 9.93 | LR: 9.86e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                             | 351/4000 [11:04<7:56:22,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  350 | Loss: 0.2173 | Train: 1.000 | Val: 0.634Â±0.076 | Temp: 9.92 | LR: 9.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 401/4000 [12:33<7:07:25,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  400 | Loss: 0.2074 | Train: 1.000 | Val: 0.602Â±0.096 | Temp: 9.92 | LR: 9.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 451/4000 [14:10<7:38:42,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  450 | Loss: 0.2070 | Train: 1.000 | Val: 0.637Â±0.092 | Temp: 9.91 | LR: 9.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 501/4000 [15:41<7:39:33,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  500 | Loss: 0.2078 | Train: 1.000 | Val: 0.669Â±0.092 | Temp: 9.91 | LR: 9.62e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 551/4000 [17:07<6:48:35,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  550 | Loss: 0.2060 | Train: 1.000 | Val: 0.631Â±0.108 | Temp: 9.90 | LR: 9.54e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 601/4000 [18:32<7:03:26,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  600 | Loss: 0.2073 | Train: 1.000 | Val: 0.612Â±0.083 | Temp: 9.90 | LR: 9.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 651/4000 [20:08<6:47:39,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  650 | Loss: 0.2093 | Train: 1.000 | Val: 0.632Â±0.093 | Temp: 9.90 | LR: 9.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                       | 701/4000 [21:43<6:22:21,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  700 | Loss: 0.2083 | Train: 1.000 | Val: 0.680Â±0.098 | Temp: 9.90 | LR: 9.27e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 751/4000 [23:14<6:36:54,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  750 | Loss: 0.2061 | Train: 1.000 | Val: 0.643Â±0.100 | Temp: 9.90 | LR: 9.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 801/4000 [24:44<6:33:52,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  800 | Loss: 0.2049 | Train: 1.000 | Val: 0.643Â±0.101 | Temp: 9.90 | LR: 9.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 851/4000 [26:22<7:03:19,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  850 | Loss: 0.2057 | Train: 1.000 | Val: 0.616Â±0.090 | Temp: 9.90 | LR: 8.93e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 901/4000 [27:54<6:40:05,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  900 | Loss: 0.2062 | Train: 1.000 | Val: 0.611Â±0.098 | Temp: 9.90 | LR: 8.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 951/4000 [29:30<6:17:01,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  950 | Loss: 0.2044 | Train: 1.000 | Val: 0.635Â±0.096 | Temp: 9.90 | LR: 8.68e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 1001/4000 [31:03<6:07:56,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Loss: 0.2063 | Train: 1.000 | Val: 0.657Â±0.085 | Temp: 9.90 | LR: 8.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1051/4000 [32:34<6:05:15,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050 | Loss: 0.2051 | Train: 1.000 | Val: 0.602Â±0.116 | Temp: 9.90 | LR: 8.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 1101/4000 [34:06<5:58:36,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100 | Loss: 0.2067 | Train: 1.000 | Val: 0.626Â±0.080 | Temp: 9.90 | LR: 8.26e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                               | 1151/4000 [35:34<5:25:19,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150 | Loss: 0.2062 | Train: 1.000 | Val: 0.623Â±0.105 | Temp: 9.91 | LR: 8.11e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 1201/4000 [36:59<5:53:40,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200 | Loss: 0.2083 | Train: 1.000 | Val: 0.638Â±0.086 | Temp: 9.91 | LR: 7.96e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 1251/4000 [38:27<5:45:10,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250 | Loss: 0.2059 | Train: 1.000 | Val: 0.661Â±0.077 | Temp: 9.91 | LR: 7.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1301/4000 [40:00<5:12:49,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300 | Loss: 0.2050 | Train: 1.000 | Val: 0.603Â±0.086 | Temp: 9.91 | LR: 7.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 1351/4000 [41:27<5:10:24,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350 | Loss: 0.2029 | Train: 1.000 | Val: 0.609Â±0.099 | Temp: 9.92 | LR: 7.47e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 1401/4000 [42:56<5:11:12,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400 | Loss: 0.2037 | Train: 1.000 | Val: 0.600Â±0.101 | Temp: 9.92 | LR: 7.29e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1451/4000 [44:27<5:13:23,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1450 | Loss: 0.2044 | Train: 1.000 | Val: 0.639Â±0.080 | Temp: 9.92 | LR: 7.12e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 1501/4000 [45:46<4:50:29,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500 | Loss: 0.2038 | Train: 1.000 | Val: 0.613Â±0.090 | Temp: 9.93 | LR: 6.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 1551/4000 [47:19<4:35:53,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550 | Loss: 0.2044 | Train: 1.000 | Val: 0.585Â±0.095 | Temp: 9.93 | LR: 6.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1601/4000 [48:46<5:08:29,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600 | Loss: 0.2050 | Train: 1.000 | Val: 0.572Â±0.083 | Temp: 9.93 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1651/4000 [50:15<5:06:03,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1650 | Loss: 0.2039 | Train: 1.000 | Val: 0.635Â±0.098 | Temp: 9.94 | LR: 6.39e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 1701/4000 [51:37<4:15:15,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700 | Loss: 0.2036 | Train: 1.000 | Val: 0.579Â±0.131 | Temp: 9.94 | LR: 6.20e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 1751/4000 [53:06<4:27:27,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750 | Loss: 0.2026 | Train: 1.000 | Val: 0.597Â±0.112 | Temp: 9.94 | LR: 6.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 1801/4000 [54:33<4:40:30,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 | Loss: 0.2029 | Train: 1.000 | Val: 0.593Â±0.092 | Temp: 9.95 | LR: 5.82e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 1851/4000 [56:01<4:42:16,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1850 | Loss: 0.2035 | Train: 1.000 | Val: 0.595Â±0.088 | Temp: 9.95 | LR: 5.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 1901/4000 [57:29<4:09:11,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900 | Loss: 0.2048 | Train: 1.000 | Val: 0.623Â±0.124 | Temp: 9.95 | LR: 5.43e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 1951/4000 [58:59<4:09:23,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1950 | Loss: 0.2057 | Train: 1.000 | Val: 0.593Â±0.099 | Temp: 9.96 | LR: 5.24e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 2001/4000 [1:00:26<4:02:27,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000 | Loss: 0.2028 | Train: 1.000 | Val: 0.563Â±0.103 | Temp: 9.96 | LR: 5.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 2051/4000 [1:02:05<4:16:22,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2050 | Loss: 0.2033 | Train: 1.000 | Val: 0.645Â±0.082 | Temp: 9.96 | LR: 4.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 2101/4000 [1:03:31<3:35:33,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100 | Loss: 0.2030 | Train: 1.000 | Val: 0.605Â±0.101 | Temp: 9.97 | LR: 4.66e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 2151/4000 [1:05:03<3:37:42,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2150 | Loss: 0.2039 | Train: 1.000 | Val: 0.595Â±0.071 | Temp: 9.97 | LR: 4.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2201/4000 [1:06:28<3:47:33,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200 | Loss: 0.2030 | Train: 1.000 | Val: 0.591Â±0.099 | Temp: 9.98 | LR: 4.27e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 2251/4000 [1:07:55<3:30:58,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250 | Loss: 0.2033 | Train: 1.000 | Val: 0.609Â±0.084 | Temp: 9.98 | LR: 4.08e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 2301/4000 [1:09:24<3:10:22,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300 | Loss: 0.2027 | Train: 1.000 | Val: 0.577Â±0.077 | Temp: 9.98 | LR: 3.89e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2351/4000 [1:10:48<3:18:35,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2350 | Loss: 0.2021 | Train: 1.000 | Val: 0.567Â±0.100 | Temp: 9.98 | LR: 3.70e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2401/4000 [1:12:12<3:17:56,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400 | Loss: 0.2037 | Train: 1.000 | Val: 0.577Â±0.110 | Temp: 9.99 | LR: 3.52e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 2451/4000 [1:13:37<3:03:59,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2450 | Loss: 0.2031 | Train: 1.000 | Val: 0.561Â±0.124 | Temp: 9.99 | LR: 3.33e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 2501/4000 [1:15:07<2:44:48,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500 | Loss: 0.2043 | Train: 1.000 | Val: 0.566Â±0.115 | Temp: 9.99 | LR: 3.15e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 2551/4000 [1:16:33<2:49:25,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2550 | Loss: 0.2033 | Train: 1.000 | Val: 0.599Â±0.091 | Temp: 10.00 | LR: 2.97e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 2601/4000 [1:18:06<2:47:36,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600 | Loss: 0.2041 | Train: 1.000 | Val: 0.623Â±0.087 | Temp: 10.00 | LR: 2.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 2651/4000 [1:19:18<1:49:46,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2650 | Loss: 0.2028 | Train: 1.000 | Val: 0.577Â±0.103 | Temp: 10.00 | LR: 2.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 2701/4000 [1:20:35<2:14:25,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700 | Loss: 0.2031 | Train: 1.000 | Val: 0.615Â±0.092 | Temp: 10.00 | LR: 2.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 2751/4000 [1:22:03<2:30:36,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2750 | Loss: 0.2025 | Train: 1.000 | Val: 0.583Â±0.089 | Temp: 10.00 | LR: 2.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 2801/4000 [1:23:29<2:20:20,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800 | Loss: 0.2030 | Train: 1.000 | Val: 0.575Â±0.095 | Temp: 10.00 | LR: 2.14e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 2851/4000 [1:24:55<2:22:50,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2850 | Loss: 0.2031 | Train: 1.000 | Val: 0.547Â±0.123 | Temp: 10.00 | LR: 1.98e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2901/4000 [1:26:32<2:13:34,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900 | Loss: 0.2035 | Train: 1.000 | Val: 0.613Â±0.085 | Temp: 10.00 | LR: 1.83e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 2951/4000 [1:27:56<2:09:12,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2950 | Loss: 0.2030 | Train: 1.000 | Val: 0.598Â±0.123 | Temp: 10.00 | LR: 1.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 3001/4000 [1:29:26<2:08:16,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000 | Loss: 0.2037 | Train: 1.000 | Val: 0.575Â±0.107 | Temp: 10.00 | LR: 1.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 3051/4000 [1:30:53<1:58:56,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3050 | Loss: 0.2027 | Train: 1.000 | Val: 0.598Â±0.094 | Temp: 10.00 | LR: 1.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 3101/4000 [1:32:26<2:00:47,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3100 | Loss: 0.2032 | Train: 1.000 | Val: 0.591Â±0.114 | Temp: 10.00 | LR: 1.28e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 3151/4000 [1:33:57<1:52:40,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3150 | Loss: 0.2033 | Train: 1.000 | Val: 0.580Â±0.111 | Temp: 10.00 | LR: 1.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 3201/4000 [1:35:20<1:14:56,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3200 | Loss: 0.2023 | Train: 1.000 | Val: 0.604Â±0.093 | Temp: 10.00 | LR: 1.04e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 3251/4000 [1:36:47<1:34:42,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3250 | Loss: 0.2036 | Train: 1.000 | Val: 0.568Â±0.085 | Temp: 10.00 | LR: 9.32e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 3301/4000 [1:38:15<1:28:52,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3300 | Loss: 0.2027 | Train: 1.000 | Val: 0.582Â±0.097 | Temp: 10.00 | LR: 8.27e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 3351/4000 [1:39:39<1:17:30,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3350 | Loss: 0.2030 | Train: 1.000 | Val: 0.573Â±0.126 | Temp: 10.00 | LR: 7.29e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 3401/4000 [1:41:07<1:10:16,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3400 | Loss: 0.2032 | Train: 1.000 | Val: 0.573Â±0.078 | Temp: 10.00 | LR: 6.38e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 3451/4000 [1:42:32<1:06:35,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3450 | Loss: 0.2027 | Train: 1.000 | Val: 0.558Â±0.105 | Temp: 10.00 | LR: 5.53e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 3501/4000 [1:43:59<58:24,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3500 | Loss: 0.2024 | Train: 1.000 | Val: 0.589Â±0.104 | Temp: 10.00 | LR: 4.75e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 3551/4000 [1:45:31<57:04,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3550 | Loss: 0.2024 | Train: 1.000 | Val: 0.597Â±0.076 | Temp: 10.00 | LR: 4.05e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 3601/4000 [1:47:02<43:24,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3600 | Loss: 0.2024 | Train: 1.000 | Val: 0.591Â±0.126 | Temp: 10.00 | LR: 3.41e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 3651/4000 [1:48:32<42:35,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3650 | Loss: 0.2023 | Train: 1.000 | Val: 0.572Â±0.121 | Temp: 10.00 | LR: 2.85e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3701/4000 [1:50:02<35:18,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3700 | Loss: 0.2027 | Train: 1.000 | Val: 0.533Â±0.114 | Temp: 10.00 | LR: 2.36e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 3751/4000 [1:51:25<27:09,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3750 | Loss: 0.2034 | Train: 1.000 | Val: 0.553Â±0.121 | Temp: 10.00 | LR: 1.94e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3801/4000 [1:52:53<24:15,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3800 | Loss: 0.2027 | Train: 1.000 | Val: 0.603Â±0.096 | Temp: 10.00 | LR: 1.60e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3851/4000 [1:54:23<17:16,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3850 | Loss: 0.2038 | Train: 1.000 | Val: 0.533Â±0.110 | Temp: 10.00 | LR: 1.34e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3901/4000 [1:55:49<12:05,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3900 | Loss: 0.2031 | Train: 1.000 | Val: 0.628Â±0.097 | Temp: 10.00 | LR: 1.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3951/4000 [1:57:16<05:23,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3950 | Loss: 0.2030 | Train: 1.000 | Val: 0.587Â±0.127 | Temp: 10.00 | LR: 1.04e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [1:58:27<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 0.687\n",
      "============================================================\n",
      "\n",
      "Best model loaded for evaluation.\n",
      "\n",
      "================================================================================\n",
      "PREDICTING ALL TEST IMAGES AND GETTING TOP 15 BY CONFIDENCE\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 617\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREDICTING ALL TEST IMAGES AND GETTING TOP 15 BY CONFIDENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m--> 617\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_all_with_confidence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Display top 15 most confident predictions\u001b[39;00m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTOP 15 MOST CONFIDENT PREDICTIONS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 529\u001b[0m, in \u001b[0;36mpredict_all_with_confidence\u001b[1;34m(model, test_data)\u001b[0m\n\u001b[0;32m    526\u001b[0m support_set \u001b[38;5;241m=\u001b[39m create_fixed_support_set(train_data, n_way\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, k_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    528\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 529\u001b[0m support_images, support_labels \u001b[38;5;241m=\u001b[39m prepare_batch(support_set, transform_eval)\n\u001b[0;32m    531\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['images'] if isinstance(data, dict) and 'images' in data else data\n",
    "\n",
    "def filter_by_index(data, target_indexes):\n",
    "    return [item for item in data if item['index'] in target_indexes]\n",
    "\n",
    "# Load and filter data\n",
    "train_data_full = load_data(r\"C:\\Users\\matin\\ECU\\code\\newcodes\\jsoncreation\\trainseed123.json\")\n",
    "val_data = load_data(r\"C:\\Users\\matin\\ECU\\Validation_Patches\\validation.json\")\n",
    "test_data = load_data(r\"C:\\Users\\matin\\ECU\\Test_Patches\\Test.json\")\n",
    "\n",
    "# Load enhanced results for additional shots\n",
    "enhanced_results_path = r\"C:\\Users\\matin\\ECU\\code\\newcodes\\enhanced_protonet_results.json\"\n",
    "with open(enhanced_results_path, 'r') as f:\n",
    "    enhanced_data_raw = json.load(f)\n",
    "\n",
    "# Extract ALL predictions from the JSON structure\n",
    "enhanced_data = []\n",
    "if isinstance(enhanced_data_raw, dict):\n",
    "    if 'all_predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['all_predictions']\n",
    "    elif 'predictions' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['predictions']\n",
    "    elif 'top_predictions_per_class' in enhanced_data_raw:\n",
    "        print(\"Loading from 'top_predictions_per_class'...\")\n",
    "        for class_name, predictions in enhanced_data_raw['top_predictions_per_class'].items():\n",
    "            if isinstance(predictions, list):\n",
    "                enhanced_data.extend(predictions)\n",
    "                print(f\"  - {class_name}: {len(predictions)} predictions\")\n",
    "    elif 'results' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['results']\n",
    "    elif 'images' in enhanced_data_raw:\n",
    "        enhanced_data = enhanced_data_raw['images']\n",
    "    else:\n",
    "        print(f\"Available keys in JSON: {list(enhanced_data_raw.keys())}\")\n",
    "        enhanced_data = []\n",
    "elif isinstance(enhanced_data_raw, list):\n",
    "    enhanced_data = enhanced_data_raw\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure. Type: {type(enhanced_data_raw)}\")\n",
    "\n",
    "print(f\"\\nTotal loaded: {len(enhanced_data)} samples from enhanced results\")\n",
    "\n",
    "target_indexes = [1, 16, 31, 46]\n",
    "train_data = filter_by_index(train_data_full, target_indexes)\n",
    "\n",
    "all_labels = ['Amphibolis', 'Background', 'Halophila', 'Posidonia']\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Organize enhanced data by true label - ONLY CORRECT predictions\n",
    "enhanced_by_label = {label: [] for label in all_labels}\n",
    "skipped_count = 0\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "if len(enhanced_data) > 0:\n",
    "    print(f\"\\nFirst sample keys: {list(enhanced_data[0].keys())}\")\n",
    "    print(f\"First sample preview: {list(enhanced_data[0].items())[:3]}\")\n",
    "\n",
    "for item in enhanced_data:\n",
    "    if 'true_label' not in item:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    true_label = item['true_label']\n",
    "    is_correct = item.get('is_correct', False)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    \n",
    "    # CRITICAL: Only include correctly predicted samples\n",
    "    if true_label in enhanced_by_label and is_correct:\n",
    "        enhanced_by_label[true_label].append({\n",
    "            'path': item.get('image_path', ''),\n",
    "            'label': true_label,\n",
    "            'confidence': item.get('confidence', 0.0),\n",
    "            'is_correct': item['is_correct'],\n",
    "            'distance_to_prototype': item.get('distance_to_prototype', 0.0),\n",
    "            'max_dissimilarity': item.get('max_dissimilarity', 0.0),\n",
    "            'entropy': item.get('entropy', 0.0)\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"  - Total samples processed: {len(enhanced_data)}\")\n",
    "print(f\"  - Correct predictions: {correct_count}\")\n",
    "print(f\"  - Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"  - Skipped (no true_label): {skipped_count}\")\n",
    "\n",
    "print(f\"Training (original): {len(train_data)}\")\n",
    "print(f\"Enhanced data available (correct predictions only):\")\n",
    "for label, items in enhanced_by_label.items():\n",
    "    if len(items) > 0:\n",
    "        avg_conf = np.mean([x['confidence'] for x in items])\n",
    "        avg_dissim = np.mean([x['max_dissimilarity'] for x in items])\n",
    "        print(f\"  {label}: {len(items)} images (avg conf: {avg_conf:.3f}, avg dissim: {avg_dissim:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {label}: {len(items)} images\")\n",
    "print(f\"Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# ENHANCED: Multiple augmentation views with test-time augmentation compatibility\n",
    "transform_train_strong = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2, hue=0.08),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_train_weak = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.95, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_episode_1plus1(data, enhanced_data_by_label, n_way, k_shot_original, k_shot_enhanced, q_query):\n",
    "    \"\"\"\n",
    "    Create episode with 1+1 shot learning (2 shots total per class):\n",
    "    - k_shot_original: shots from original training data\n",
    "    - k_shot_enhanced: shots from enhanced results (CORRECT, HIGH CONFIDENCE, MOST DISSIMILAR)\n",
    "    Total support set per class = 2\n",
    "    \"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    selected_classes = random.sample(all_labels, n_way)\n",
    "    episode = []\n",
    "    \n",
    "    for cls_idx, cls in enumerate(selected_classes):\n",
    "        # ===== PART 1: Original training samples (1 shot) =====\n",
    "        original_samples = class_samples[cls]\n",
    "        if len(original_samples) < k_shot_original + q_query:\n",
    "            selected_original = random.choices(original_samples, k=k_shot_original + q_query)\n",
    "        else:\n",
    "            selected_original = random.sample(original_samples, k_shot_original + q_query)\n",
    "        \n",
    "        # ===== PART 2: Enhanced samples (1 shot - STRATEGIC SELECTION) =====\n",
    "        enhanced_samples = enhanced_by_label[cls]\n",
    "        \n",
    "        if len(enhanced_samples) >= k_shot_enhanced:\n",
    "            # Strategy: Select samples with HIGH CONFIDENCE and HIGH DISSIMILARITY\n",
    "            scored_samples = []\n",
    "            for sample in enhanced_samples:\n",
    "                conf = sample['confidence']\n",
    "                dissim = sample['max_dissimilarity']\n",
    "                \n",
    "                # Combined score: prioritize high confidence AND high dissimilarity\n",
    "                score = conf * 0.4 + dissim * 0.6\n",
    "                scored_samples.append((score, sample))\n",
    "            \n",
    "            # Sort by score (descending) and select top k\n",
    "            scored_samples.sort(key=lambda x: x[0], reverse=True)\n",
    "            selected_enhanced = [item[1] for item in scored_samples[:k_shot_enhanced]]\n",
    "            \n",
    "        elif len(enhanced_samples) > 0:\n",
    "            selected_enhanced = random.choices(enhanced_samples, k=k_shot_enhanced)\n",
    "        else:\n",
    "            # Fallback: use additional original samples\n",
    "            if len(original_samples) >= k_shot_original + k_shot_enhanced + q_query:\n",
    "                selected_enhanced = [original_samples[k_shot_original]]\n",
    "            else:\n",
    "                selected_enhanced = random.choices(original_samples, k=k_shot_enhanced)\n",
    "        \n",
    "        # ===== Add samples to episode =====\n",
    "        # Add 1 original support sample\n",
    "        for i in range(k_shot_original):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'original'\n",
    "            })\n",
    "        \n",
    "        # Add 1 enhanced support sample\n",
    "        for enhanced_item in selected_enhanced:\n",
    "            episode.append({\n",
    "                'path': enhanced_item['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True,\n",
    "                'source': 'enhanced',\n",
    "                'confidence': enhanced_item['confidence'],\n",
    "                'dissimilarity': enhanced_item['max_dissimilarity']\n",
    "            })\n",
    "        \n",
    "        # Query samples (from original data only)\n",
    "        for i in range(k_shot_original, k_shot_original + q_query):\n",
    "            episode.append({\n",
    "                'path': selected_original[i]['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': False,\n",
    "                'source': 'original'\n",
    "            })\n",
    "    \n",
    "    return episode, selected_classes\n",
    "\n",
    "# ENHANCED: Better architecture with residual connections and multi-scale features\n",
    "class ImprovedProtoNet(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        import clip\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # Fine-tune last 2 layers for better feature extraction\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            if 'transformer.resblocks.11' in name or 'transformer.resblocks.10' in name or 'ln_post' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = self.clip_model.visual\n",
    "        \n",
    "        # Enhanced projection with residual-like connections\n",
    "        self.projection1 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.projection2 = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.projection3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Learnable temperature with constraints\n",
    "        self.log_temperature = nn.Parameter(torch.log(torch.tensor(10.0)))\n",
    "        \n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Multi-stage projection\n",
    "        h1 = self.projection1(features)\n",
    "        h2 = self.projection2(h1)\n",
    "        h3 = self.projection3(h2)\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return F.normalize(h3, dim=1), F.normalize(h2, dim=1)\n",
    "        return F.normalize(h3, dim=1)\n",
    "    \n",
    "    @property\n",
    "    def temperature(self):\n",
    "        # Constrain temperature to reasonable range [5, 20]\n",
    "        return torch.clamp(torch.exp(self.log_temperature), 5.0, 20.0)\n",
    "\n",
    "def prepare_batch(batch, transform, multi_view=False):\n",
    "    \"\"\"Prepare batch with optional multi-view augmentation\"\"\"\n",
    "    images, labels = [], []\n",
    "    for sample in batch:\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        labels.append(sample['episode_label'])\n",
    "    \n",
    "    if multi_view and random.random() < 0.3:  # 30% chance to use second view\n",
    "        images2 = []\n",
    "        for sample in batch:\n",
    "            img = Image.open(sample['path']).convert('RGB')\n",
    "            images2.append(transform(img))\n",
    "        return torch.stack(images).to(device), torch.stack(images2).to(device), torch.tensor(labels).to(device)\n",
    "    \n",
    "    return torch.stack(images).to(device), None, torch.tensor(labels).to(device)\n",
    "\n",
    "# ENHANCED: Multi-prototype approach\n",
    "def compute_prototypes_robust(embeddings, labels, n_way, num_augments=1):\n",
    "    \"\"\"Compute prototypes with optional multi-view averaging\"\"\"\n",
    "    prototypes = []\n",
    "    for cls in range(n_way):\n",
    "        cls_indices = torch.where(labels == cls)[0]\n",
    "        cls_embeddings = embeddings[cls_indices]\n",
    "        \n",
    "        # Average over all support samples for this class\n",
    "        proto = cls_embeddings.mean(0)\n",
    "        prototypes.append(proto)\n",
    "    \n",
    "    return torch.stack(prototypes)\n",
    "\n",
    "# ENHANCED: Label smoothing for better generalization\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_class = pred.size(1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
    "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
    "        log_prob = F.log_softmax(pred, dim=1)\n",
    "        return -(one_hot * log_prob).sum(dim=1).mean()\n",
    "\n",
    "# Model setup\n",
    "model = ImprovedProtoNet().to(device)\n",
    "\n",
    "# Differential learning rates\n",
    "clip_params = [p for n, p in model.named_parameters() if 'clip_model' in n or 'encoder' in n]\n",
    "other_params = [p for n, p in model.named_parameters() if not ('clip_model' in n or 'encoder' in n)]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': 2e-5, 'weight_decay': 1e-4},\n",
    "    {'params': other_params, 'lr': 1e-4, 'weight_decay': 1e-3}\n",
    "])\n",
    "\n",
    "# Cosine annealing with warmup\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4000, eta_min=1e-6)\n",
    "\n",
    "# Label smoothing loss\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.05)\n",
    "\n",
    "# Training configuration\n",
    "n_way = 4\n",
    "k_shot_original = 1\n",
    "k_shot_enhanced = 1\n",
    "q_query = 15\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 0\n",
    "max_patience = 400\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting ENHANCED 1+1 shot training\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Architecture: Multi-stage projection with BatchNorm\")\n",
    "print(f\"  - Loss: Label smoothing (0.05)\")\n",
    "print(f\"  - Augmentation: Dual-strength with multi-view\")\n",
    "print(f\"  - Temperature: Learnable with constraints [5, 20]\")\n",
    "print(f\"  - Episodes: 4000 with cosine annealing\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for episode in tqdm(range(4000), desc=\"Training\"):\n",
    "    model.train()\n",
    "    \n",
    "    # Alternate between strong and weak augmentation\n",
    "    use_strong_aug = episode % 2 == 0\n",
    "    current_transform = transform_train_strong if use_strong_aug else transform_train_weak\n",
    "    \n",
    "    episode_data, _ = create_episode_1plus1(\n",
    "        train_data, enhanced_by_label, n_way, \n",
    "        k_shot_original, k_shot_enhanced, q_query\n",
    "    )\n",
    "    \n",
    "    support_set = [s for s in episode_data if s['is_support']]\n",
    "    query_set = [q for q in episode_data if not q['is_support']]\n",
    "    \n",
    "    # Prepare with multi-view augmentation\n",
    "    support_images, support_images2, support_labels = prepare_batch(support_set, transform_eval, multi_view=False)\n",
    "    query_images, query_images2, query_labels = prepare_batch(query_set, current_transform, multi_view=True)\n",
    "    \n",
    "    # Forward pass - support\n",
    "    support_embeddings = model(support_images)\n",
    "    \n",
    "    # Forward pass - query (with optional second view)\n",
    "    query_embeddings = model(query_images)\n",
    "    if query_images2 is not None:\n",
    "        query_embeddings2 = model(query_images2)\n",
    "        query_embeddings = (query_embeddings + query_embeddings2) / 2  # Average views\n",
    "    \n",
    "    # Compute prototypes\n",
    "    prototypes = compute_prototypes_robust(support_embeddings, support_labels, n_way)\n",
    "    \n",
    "    # Compute distances with learnable temperature\n",
    "    dists = torch.cdist(query_embeddings, prototypes)\n",
    "    logits = -dists * model.temperature\n",
    "    \n",
    "    # Label smoothing loss\n",
    "    loss = criterion(logits, query_labels)\n",
    "    \n",
    "    # Regularization: encourage diverse prototypes\n",
    "    proto_sim = torch.mm(prototypes, prototypes.t())\n",
    "    proto_mask = 1 - torch.eye(n_way).to(device)\n",
    "    proto_diversity = 0.02 * (proto_sim * proto_mask).sum() / (n_way * (n_way - 1))\n",
    "    \n",
    "    # Regularization: prevent temperature from becoming too extreme\n",
    "    temp_reg = 0.001 * (model.temperature - 10.0).abs()\n",
    "    \n",
    "    total_loss = loss + proto_diversity + temp_reg\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_accuracies = []\n",
    "            \n",
    "            for _ in range(25):  # More validation episodes\n",
    "                class_samples_val = {label: [] for label in all_labels}\n",
    "                for item in val_data:\n",
    "                    class_samples_val[item['label']].append(item)\n",
    "                \n",
    "                selected_classes_val = random.sample(all_labels, n_way)\n",
    "                val_episode_data = []\n",
    "                \n",
    "                for cls_idx, cls in enumerate(selected_classes_val):\n",
    "                    samples = class_samples_val[cls]\n",
    "                    if len(samples) < 1 + q_query:\n",
    "                        selected = random.choices(samples, k=1 + q_query)\n",
    "                    else:\n",
    "                        selected = random.sample(samples, 1 + q_query)\n",
    "                    \n",
    "                    val_episode_data.append({\n",
    "                        'path': selected[0]['path'],\n",
    "                        'label': cls,\n",
    "                        'episode_label': cls_idx,\n",
    "                        'is_support': True\n",
    "                    })\n",
    "                    \n",
    "                    for i in range(1, 1 + q_query):\n",
    "                        val_episode_data.append({\n",
    "                            'path': selected[i]['path'],\n",
    "                            'label': cls,\n",
    "                            'episode_label': cls_idx,\n",
    "                            'is_support': False\n",
    "                        })\n",
    "                \n",
    "                val_support = [s for s in val_episode_data if s['is_support']]\n",
    "                val_query = [q for q in val_episode_data if not q['is_support']]\n",
    "                \n",
    "                val_support_images, _, val_support_labels = prepare_batch(val_support, transform_eval)\n",
    "                val_query_images, _, val_query_labels = prepare_batch(val_query, transform_eval)\n",
    "                \n",
    "                val_support_embeddings = model(val_support_images)\n",
    "                val_prototypes = compute_prototypes_robust(val_support_embeddings, val_support_labels, n_way)\n",
    "                \n",
    "                val_query_embeddings = model(val_query_images)\n",
    "                val_dists = torch.cdist(val_query_embeddings, val_prototypes)\n",
    "                val_logits = -val_dists * model.temperature\n",
    "                val_preds = torch.argmax(val_logits, dim=1)\n",
    "                \n",
    "                val_acc = (val_preds == val_query_labels).float().mean().item()\n",
    "                val_accuracies.append(val_acc)\n",
    "            \n",
    "            val_acc_avg = np.mean(val_accuracies)\n",
    "            val_acc_std = np.std(val_accuracies)\n",
    "            train_acc = (torch.argmax(logits, dim=1) == query_labels).float().mean().item()\n",
    "            current_lr = optimizer.param_groups[1]['lr']\n",
    "            current_temp = model.temperature.item()\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | Loss: {loss.item():.4f} | Train: {train_acc:.3f} | \"\n",
    "                  f\"Val: {val_acc_avg:.3f}Â±{val_acc_std:.3f} | Temp: {current_temp:.2f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            if val_acc_avg > best_val_acc:\n",
    "                best_val_acc = val_acc_avg\n",
    "                torch.save(model.state_dict(), 'improved_protonet_1plus1_best.pth')\n",
    "                patience = 0\n",
    "                print(f\"âœ… New best: {best_val_acc:.3f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "model.load_state_dict(torch.load('improved_protonet_1plus1_best.pth'))\n",
    "print(\"Best model loaded for evaluation.\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST-TIME EVALUATION CODE (Compatible with the provided test code)\n",
    "# ============================================================================\n",
    "\n",
    "def create_fixed_support_set(data, n_way=4, k_shot=1):\n",
    "    \"\"\"Create fixed support set for testing\"\"\"\n",
    "    class_samples = {label: [] for label in all_labels}\n",
    "    for item in data:\n",
    "        class_samples[item['label']].append(item)\n",
    "    \n",
    "    support_set = []\n",
    "    for cls_idx, cls in enumerate(all_labels):\n",
    "        samples = class_samples[cls]\n",
    "        # Use first k_shot samples for consistency\n",
    "        selected = samples[:k_shot] if len(samples) >= k_shot else samples\n",
    "        \n",
    "        for sample in selected:\n",
    "            support_set.append({\n",
    "                'path': sample['path'],\n",
    "                'label': cls,\n",
    "                'episode_label': cls_idx,\n",
    "                'is_support': True\n",
    "            })\n",
    "    \n",
    "    return support_set\n",
    "\n",
    "def predict_all_with_confidence(model, test_data):\n",
    "    \"\"\"Predict all test images with confidence scores using TTA\"\"\"\n",
    "    support_set = create_fixed_support_set(train_data, n_way=4, k_shot=1)\n",
    "    \n",
    "    model.eval()\n",
    "    support_images, support_labels = prepare_batch(support_set, transform_eval)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        support_embeddings = model(support_images)\n",
    "        \n",
    "        # Compute class prototypes\n",
    "        prototypes = []\n",
    "        for cls_idx in range(len(all_labels)):\n",
    "            cls_indices = torch.where(support_labels == cls_idx)[0]\n",
    "            if len(cls_indices) > 0:\n",
    "                cls_proto = support_embeddings[cls_indices].mean(0)\n",
    "                prototypes.append(cls_proto)\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(support_embeddings.shape[1]).to(device))\n",
    "        \n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        # Test-time augmentation transforms\n",
    "        tta_transforms = [\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        ]\n",
    "        \n",
    "        # Get temperature value (handle both property and parameter)\n",
    "        temp_value = model.temperature if isinstance(model.temperature, (int, float)) else model.temperature.item()\n",
    "        \n",
    "        for i, sample in enumerate(tqdm(test_data, desc=\"Predicting all test images\")):\n",
    "            img = Image.open(sample['path']).convert('RGB')\n",
    "            true_label = label_to_idx[sample['label']]\n",
    "            \n",
    "            # Apply TTA\n",
    "            embeddings_list = []\n",
    "            for transform in tta_transforms:\n",
    "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                embeddings = model(img_tensor)\n",
    "                embeddings_list.append(embeddings)\n",
    "            \n",
    "            avg_embedding = torch.mean(torch.cat(embeddings_list, dim=0), dim=0, keepdim=True)\n",
    "            \n",
    "            # Calculate distances with temperature scaling\n",
    "            dists = torch.cdist(avg_embedding, prototypes)\n",
    "            logits = -dists * temp_value\n",
    "            \n",
    "            # Convert to probabilities using softmax for confidence scores\n",
    "            confidence_scores = torch.softmax(logits, dim=1).squeeze()\n",
    "            \n",
    "            # Get predicted class\n",
    "            pred_idx = torch.argmax(logits, dim=1).item()\n",
    "            pred_label = all_labels[pred_idx]\n",
    "            max_confidence = confidence_scores[pred_idx].item()\n",
    "            \n",
    "            predictions.append({\n",
    "                'image_index': i,\n",
    "                'image_path': sample['path'],\n",
    "                'true_label': sample['label'],\n",
    "                'predicted_label': pred_label,\n",
    "                'confidence': max_confidence,\n",
    "                'is_correct': pred_idx == true_label,\n",
    "                'all_confidences': {all_labels[j]: confidence_scores[j].item() for j in range(len(all_labels))}\n",
    "            })\n",
    "    \n",
    "    # Sort by confidence (highest first)\n",
    "    predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Get predictions for all test images\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTING ALL TEST IMAGES AND GETTING TOP 15 BY CONFIDENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_predictions = predict_all_with_confidence(model, test_data)\n",
    "\n",
    "# Display top 15 most confident predictions\n",
    "print(\"\\nTOP 15 MOST CONFIDENT PREDICTIONS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, pred in enumerate(all_predictions[:15]):\n",
    "    print(f\"\\nRank {i+1}:\")\n",
    "    print(f\"Image: {os.path.basename(pred['image_path'])}\")\n",
    "    print(f\"True Label: {pred['true_label']}\")\n",
    "    print(f\"Predicted Label: {pred['predicted_label']}\")\n",
    "    print(f\"Confidence: {pred['confidence']:.4f}\")\n",
    "    print(f\"Correct: {'âœ…' if pred['is_correct'] else 'âŒ'}\")\n",
    "    print(\"All class confidences:\")\n",
    "    for label, conf in pred['all_confidences'].items():\n",
    "        print(f\"  {label}: {conf:.4f}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "correct_predictions = sum(1 for pred in all_predictions if pred['is_correct'])\n",
    "total_predictions = len(all_predictions)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL RESULTS\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Total test images: {total_predictions}\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Overall accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Save results to JSON\n",
    "results_data = {\n",
    "    'overall_accuracy': accuracy,\n",
    "    'total_images': total_predictions,\n",
    "    'correct_predictions': correct_predictions,\n",
    "    'top_15_predictions': all_predictions[:15],\n",
    "    'all_predictions': all_predictions\n",
    "}\n",
    "\n",
    "with open('test_predictions_with_confidence_seagrass_1shot_filtered.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to 'test_predictions_with_confidence_seagrass_1shot_filtered.json'\")\n",
    "\n",
    "# Get predictions for all test images - Per Class Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTING ALL TEST IMAGES AND GETTING TOP 15 BY CONFIDENCE PER CLASS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group predictions by predicted class and get top 15 for each\n",
    "predictions_by_class = {}\n",
    "for pred in all_predictions:\n",
    "    predicted_class = pred['predicted_label']\n",
    "    if predicted_class not in predictions_by_class:\n",
    "        predictions_by_class[predicted_class] = []\n",
    "    predictions_by_class[predicted_class].append(pred)\n",
    "\n",
    "# Sort each class by confidence and get top 15\n",
    "top15_per_class = {}\n",
    "for class_name, class_predictions in predictions_by_class.items():\n",
    "    # Sort by confidence (highest first)\n",
    "    class_predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    top15_per_class[class_name] = class_predictions[:15]\n",
    "\n",
    "# Display top 15 for each class\n",
    "print(\"\\nTOP 15 MOST CONFIDENT PREDICTIONS PER CLASS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for class_name, top_predictions in top15_per_class.items():\n",
    "    print(f\"\\nðŸ·ï¸  CLASS: {class_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, pred in enumerate(top_predictions):\n",
    "        print(f\"  Rank {i+1}:\")\n",
    "        print(f\"    Image: {os.path.basename(pred['image_path'])}\")\n",
    "        print(f\"    True Label: {pred['true_label']}\")\n",
    "        print(f\"    Confidence: {pred['confidence']:.4f}\")\n",
    "        print(f\"    Correct: {'âœ…' if pred['is_correct'] else 'âŒ'}\")\n",
    "        \n",
    "        # Show top 3 confidence scores for this prediction\n",
    "        sorted_confidences = sorted(pred['all_confidences'].items(), \n",
    "                                  key=lambda x: x[1], reverse=True)\n",
    "        print(f\"    Top confidences: {', '.join([f'{label}:{conf:.3f}' for label, conf in sorted_confidences[:3]])}\")\n",
    "        print()\n",
    "\n",
    "# Calculate per-class statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_stats = {}\n",
    "for class_name in all_labels:\n",
    "    # Get all predictions for this class (both correct and incorrect)\n",
    "    class_preds = [p for p in all_predictions if p['predicted_label'] == class_name]\n",
    "    correct_preds = [p for p in class_preds if p['is_correct']]\n",
    "    \n",
    "    # Also get true instances of this class\n",
    "    true_instances = [p for p in all_predictions if p['true_label'] == class_name]\n",
    "    correctly_identified = [p for p in true_instances if p['is_correct']]\n",
    "    \n",
    "    class_stats[class_name] = {\n",
    "        'predicted_as_this_class': len(class_preds),\n",
    "        'correctly_predicted_as_this_class': len(correct_preds),\n",
    "        'true_instances': len(true_instances),\n",
    "        'correctly_identified_instances': len(correctly_identified),\n",
    "        'precision': len(correct_preds) / len(class_preds) if class_preds else 0,\n",
    "        'recall': len(correctly_identified) / len(true_instances) if true_instances else 0\n",
    "    }\n",
    "\n",
    "for class_name, stats in class_stats.items():\n",
    "    precision = stats['precision']\n",
    "    recall = stats['recall']\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {class_name.upper()}:\")\n",
    "    print(f\"   Predicted as this class: {stats['predicted_as_this_class']}\")\n",
    "    print(f\"   Correctly predicted: {stats['correctly_predicted_as_this_class']}\")\n",
    "    print(f\"   True instances: {stats['true_instances']}\")\n",
    "    print(f\"   Correctly identified: {stats['correctly_identified_instances']}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"OVERALL RESULTS\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Total test images: {total_predictions}\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Overall accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Save enhanced results to JSON\n",
    "results_data = {\n",
    "    'overall_accuracy': accuracy,\n",
    "    'total_images': total_predictions,\n",
    "    'correct_predictions': correct_predictions,\n",
    "    'top_15_per_class': top15_per_class,\n",
    "    'class_statistics': class_stats,\n",
    "    'all_predictions': all_predictions\n",
    "}\n",
    "\n",
    "with open('test_predictions_top15_per_class_seagrass_1shot_filtered.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=4, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to 'test_predictions_top15_per_class_seagrass_1shot_filtered.json'\")\n",
    "\n",
    "# Optional: Create a summary table\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY TABLE - TOP 15 COUNT PER CLASS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Class':<15} {'Top 15 Count':<12} {'Avg Confidence':<15} {'Correct in Top 15':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for class_name, predictions in top15_per_class.items():\n",
    "    avg_conf = sum(p['confidence'] for p in predictions) / len(predictions)\n",
    "    correct_count = sum(1 for p in predictions if p['is_correct'])\n",
    "    print(f\"{class_name:<15} {len(predictions):<12} {avg_conf:.4f}{'':>7} {correct_count:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e15bb-3009-451e-89a9-f6a671009511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
